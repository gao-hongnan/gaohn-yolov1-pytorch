
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>YOLOv1 &#8212; YOLOv1 Explained</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Introduction" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">YOLOv1 Explained</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   YOLOv1
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/reigHns92/ml_blog/main?urlpath=tree/docs/yolov1.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/reigHns92/ml_blog/blob/main/docs/yolov1.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/reigHns92/ml_blog"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/reigHns92/ml_blog/issues/new?title=Issue%20on%20page%20%2Fyolov1.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/yolov1.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="_sources/yolov1.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unified-detection">
   Unified Detection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-architecture">
   Model Architecture
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#python-implementation">
     Python Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-summary">
     Model Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backbone">
     Backbone
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#head">
     Head
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anchors-and-prior-boxes">
   Anchors and Prior Boxes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bounding-box-parametrization">
   Bounding Box Parametrization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#yolov1-encoding-setup">
   YOLOv1 Encoding Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations-and-definitions">
   Notations and Definitions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sample-image">
     Sample Image
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#yolov1-md-bounding-box-parametrization">
     Bounding Box Parametrization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-function">
     Loss Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-important-notations">
     Other Important Notations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-3d-tensor-to-2d-matrix">
   From 3D Tensor to 2D Matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#construction-of-ground-truth-matrix">
   Construction of Ground Truth Matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#construction-of-prediction-matrix">
   Construction of Prediction Matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   Loss Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bipartite-matching">
     Bipartite Matching
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#total-loss-for-a-single-image">
     Total Loss for a Single Image
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-for-a-single-grid-cell-in-a-single-image">
     Loss for a Single Grid Cell in a Single Image
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-formula">
       The Formula
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-with-numbers-part-i">
       Example with Numbers Part I
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-formula-modified">
       The Formula Modified
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#recap-on-the-modified-loss-function">
       Recap on the Modified Loss Function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examples-with-numbers-part-ii">
       Examples with Numbers Part II
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sphinx">
   Sphinx
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#citations">
   Citations
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>YOLOv1</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unified-detection">
   Unified Detection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-architecture">
   Model Architecture
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#python-implementation">
     Python Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-summary">
     Model Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backbone">
     Backbone
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#head">
     Head
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anchors-and-prior-boxes">
   Anchors and Prior Boxes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bounding-box-parametrization">
   Bounding Box Parametrization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#yolov1-encoding-setup">
   YOLOv1 Encoding Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations-and-definitions">
   Notations and Definitions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sample-image">
     Sample Image
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#yolov1-md-bounding-box-parametrization">
     Bounding Box Parametrization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-function">
     Loss Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-important-notations">
     Other Important Notations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-3d-tensor-to-2d-matrix">
   From 3D Tensor to 2D Matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#construction-of-ground-truth-matrix">
   Construction of Ground Truth Matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#construction-of-prediction-matrix">
   Construction of Prediction Matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   Loss Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bipartite-matching">
     Bipartite Matching
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#total-loss-for-a-single-image">
     Total Loss for a Single Image
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-for-a-single-grid-cell-in-a-single-image">
     Loss for a Single Grid Cell in a Single Image
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-formula">
       The Formula
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-with-numbers-part-i">
       Example with Numbers Part I
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-formula-modified">
       The Formula Modified
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#recap-on-the-modified-loss-function">
       Recap on the Modified Loss Function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examples-with-numbers-part-ii">
       Examples with Numbers Part II
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sphinx">
   Sphinx
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#citations">
   Citations
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="math notranslate nohighlight">
\[
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\newcommand{\1}{\mathbb{1}}
\newcommand{\Pobj}{\mathbb{P}(\text{obj})}
\newcommand{\yhat}{\symbf{\hat{y}}}
\newcommand{\xhat}{\symbf{\hat{x}}}
\newcommand{\x}{\symbf{x}}
\newcommand{\y}{\symbf{y}}
\newcommand{\w}{\symbf{w}}
\newcommand{\h}{\symbf{h}}
\newcommand{\bs}{\textbf{bs}}
\newcommand{\byolo}{\mathrm{b}_{\text{yolo}}}
\newcommand{\bgrid}{\mathrm{b}_{\text{grid}}}
\newcommand{\cc}{\mathrm{c}}
\newcommand{\iou}{\textbf{IOU}_{\symbf{\hat{b}}}^{\symbf{b}}}
\newcommand{\conf}{\textbf{conf}}
\newcommand{\confhat}{\hat{\textbf{conf}}}
\newcommand{\X}{\symbf{X}}
\newcommand{\xx}{\mathrm{x}}
\newcommand{\yy}{\mathrm{y}}
\newcommand{\ww}{\mathrm{w}}
\newcommand{\hh}{\mathrm{h}}
\newcommand{\xxhat}{\hat{\mathrm{x}}}
\newcommand{\yyhat}{\hat{\mathrm{y}}}
\newcommand{\wwhat}{\hat{\mathrm{w}}}
\newcommand{\hhhat}{\hat{\mathrm{h}}}
\newcommand{\gx}{\mathrm{g_x}}
\newcommand{\gy}{\mathrm{g_y}}
\newcommand{\b}{\symbf{b}}
\newcommand{\bhat}{\symbf{\hat{b}}}
\newcommand{\p}{\symbf{p}}
\newcommand{\phat}{\symbf{\hat{p}}}
\newcommand{\y}{\symbf{y}}
\newcommand{\L}{\mathcal{L}}
\newcommand{\lsq}{\left[}
\newcommand{\rsq}{\right]}
\newcommand{\lpar}{\left(}
\newcommand{\rpar}{\right)}
\newcommand{\jmax}{j_{\max}}
\newcommand{\obji}{\mathbb{1}_{i}^{\text{obj}}}
\newcommand{\nobji}{\mathbb{1}_{i}^{\text{noobj}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\abs}{\text{abs}}
\]</div>
<section class="tex2jax_ignore mathjax_ignore" id="yolov1">
<h1>YOLOv1<a class="headerlink" href="#yolov1" title="Permalink to this headline">#</a></h1>
<p>YOLO (You Only Look Once) is a single-stage object detector that frames object detection as a single
regression problem to predict bounding box coordinates and class probabilities of objects in an image.
The model is called YOLO because you only look once at an image to predict what objects are present
and where they are in the image.
There are several versions of YOLO models, with each one having a slightly different architecture
from the others. In this article, we will focus on the very first model called YOLOv1.</p>
<p>YOLOv1 comprises of a single convolutional neural network that simultaneously predicts
multiple bounding boxes and class probabilities for these boxes.
Compared to other traditional methods of object detection such as DPM and R-CNN,
the YOLO model has several benefits such as being extremely fast,
being able to reason globally about an image when making predictions and being able to learn
generalizable representations of objects in an iamge.</p>
<p>The YOLOv1 model uses an anchor-free architecture with parameterised bounding boxes.
It takes in an RGB image (448×448×3) as its input and returns a tensor (7×7×30) as its output.
The parameterisation of bounding boxes means that the the bounding box coordinates are defined
relative to a particular grid in the 7×7 grid space (rather than being defined on an absolute scale).
More information on the model architecture will be detailed in the Model Architecture section below.</p>
<section id="unified-detection">
<h2>Unified Detection<a class="headerlink" href="#unified-detection" title="Permalink to this headline">#</a></h2>
<p>YOLOv1 is a unified detection model that simultaneously predicts multiple bounding boxes and class.</p>
<p>What this means is that given an input image <span class="math notranslate nohighlight">\(\X\)</span> of size <span class="math notranslate nohighlight">\(448\times 448\times 3\)</span>, the model network
can accurately <em><strong>locate</strong></em> and <em><strong>classify</strong></em> multiple objects in the image with just one forward pass.</p>
<p>This is in contrast to other object detection models such as R-CNN which require multiple forward passes
since they use a two-stage pipeline.</p>
<p>What is so smart about this architecture is that the author managed to design a network such that it can
reason globally about the image when making predictions. The model’s feature map is
so powerful such that it can do both <strong>regression (locate)</strong> and <strong>classification (classify)</strong> at the same time.
Regression being the task of predicting/localizing bounding box coordinates and
classification being the task of predicting the class of the object in the bounding box.
In the first version of YOLO, the model is treated as a regression problem, simply because the loss function
is mean squared error, but in later versions
of YOLO, the model is revised such that the loss function is a combination of regression and classification.</p>
<p>Before we dive into more details, we define the model architecture first.</p>
</section>
<section id="model-architecture">
<h2>Model Architecture<a class="headerlink" href="#model-architecture" title="Permalink to this headline">#</a></h2>
<p>The model architecture from the YOLOv1 paper is presented below in <a class="reference internal" href="#yolov1-model"><span class="std std-numref">Fig. 1</span></a>.</p>
<figure class="align-default" id="yolov1-model">
<a class="reference internal image-reference" href="https://storage.googleapis.com/reighns/images/yolov1_model.png"><img alt="https://storage.googleapis.com/reighns/images/yolov1_model.png" src="https://storage.googleapis.com/reighns/images/yolov1_model.png" style="width: 600px; height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">YoloV1 Model Architecture</span><a class="headerlink" href="#yolov1-model" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The YOLOv1 model is made up of 24 convolutional layers and 2 fully connected layers, a surprisingly
simple architecture that resembles a image classification model. The authors also mentioned that
the model was inspired by GoogLeNet.</p>
<p>We are more interested in the last layer of the network, as that is where the novelty lies.
<a class="reference internal" href="#label-matrix"><span class="std std-numref">Fig. 2</span></a> is a zoomed in version of the last layer, a cuboid of shape <span class="math notranslate nohighlight">\(7 \times 7 \times 30\)</span>.
This cuboid is extremely important to understand, which we will mention more later.</p>
<figure class="align-default" id="label-matrix">
<img alt="https://storage.googleapis.com/reighns/images/label_matrix.png" src="https://storage.googleapis.com/reighns/images/label_matrix.png" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">The output tensor from YOLOv1’s last layer.</span><a class="headerlink" href="#label-matrix" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Permalink to this headline">#</a></h3>
<p>We present a python implementation of the model in PyTorch. The implementation is modified
from Aladdin Persson’s <a class="reference external" href="https://github.com/aladdinpersson/Machine-Learning-Collection">repository</a>.
In the implementation, there are some small changes such as adding
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html"><strong>batch norm</strong></a> layers.
However, the overall architecture remains similar to what was proposed in the paper.</p>
<p>The model architecture in code is defined below:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="linenos">  2</span>
<span class="linenos">  3</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="linenos">  4</span><span class="kn">import</span> <span class="nn">torchinfo</span>
<span class="linenos">  5</span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="linenos">  6</span>
<span class="linenos">  7</span><span class="k">class</span> <span class="nc">CNNBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="linenos">  8</span>    <span class="sd">&quot;&quot;&quot;Creates CNNBlock similar to YOLOv1 Darknet architecture</span>
<span class="linenos">  9</span>
<span class="linenos"> 10</span><span class="sd">    Note:</span>
<span class="linenos"> 11</span><span class="sd">        1. On top of `nn.Conv2d` we add `nn.BatchNorm2d` and `nn.LeakyReLU`.</span>
<span class="linenos"> 12</span><span class="sd">        2. We set `track_running_stats=False` in `nn.BatchNorm2d` because we want</span>
<span class="linenos"> 13</span><span class="sd">           to avoid updating running mean and variance during training.</span>
<span class="linenos"> 14</span><span class="sd">           ref: https://tinyurl.com/ap22f8nf</span>
<span class="linenos"> 15</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos"> 16</span>
<span class="linenos"> 17</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 18</span>        <span class="sd">&quot;&quot;&quot;Initialize CNNBlock.</span>
<span class="linenos"> 19</span>
<span class="linenos"> 20</span><span class="sd">        Args:</span>
<span class="linenos"> 21</span><span class="sd">            in_channels (int): The number of input channels.</span>
<span class="linenos"> 22</span><span class="sd">            out_channels (int): The number of output channels.</span>
<span class="linenos"> 23</span><span class="sd">            **kwargs (Dict[Any]): Keyword arguments for `nn.Conv2d` such as `kernel_size`,</span>
<span class="linenos"> 24</span><span class="sd">                     `stride` and `padding`.</span>
<span class="linenos"> 25</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos"> 26</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="linenos"> 27</span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="linenos"> 28</span>        <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
<span class="linenos"> 29</span>            <span class="n">num_features</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">False</span>
<span class="linenos"> 30</span>        <span class="p">)</span>
<span class="linenos"> 31</span>        <span class="bp">self</span><span class="o">.</span><span class="n">leakyrelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="linenos"> 32</span>
<span class="linenos"> 33</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos"> 34</span>        <span class="sd">&quot;&quot;&quot;Forward pass.&quot;&quot;&quot;</span>
<span class="linenos"> 35</span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">leakyrelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="linenos"> 36</span>
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="k">class</span> <span class="nc">Yolov1Darknet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="linenos"> 39</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<span class="linenos"> 40</span>        <span class="bp">self</span><span class="p">,</span>
<span class="linenos"> 41</span>        <span class="n">architecture</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
<span class="linenos"> 42</span>        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span class="linenos"> 43</span>        <span class="n">S</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span>
<span class="linenos"> 44</span>        <span class="n">B</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
<span class="linenos"> 45</span>        <span class="n">C</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="linenos"> 46</span>        <span class="n">init_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="linenos"> 47</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 48</span>        <span class="sd">&quot;&quot;&quot;Initialize Yolov1Darknet.</span>
<span class="linenos"> 49</span>
<span class="linenos"> 50</span><span class="sd">        Note:</span>
<span class="linenos"> 51</span><span class="sd">            1. `self.backbone` is the backbone of Darknet.</span>
<span class="linenos"> 52</span><span class="sd">            2. `self.head` is the head of Darknet.</span>
<span class="linenos"> 53</span><span class="sd">            3. Currently the head is hardcoded to have 1024 neurons and if you change</span>
<span class="linenos"> 54</span><span class="sd">               the image size from the default 448, then you will have to change the</span>
<span class="linenos"> 55</span><span class="sd">               neurons in the head.</span>
<span class="linenos"> 56</span>
<span class="linenos"> 57</span><span class="sd">        Args:</span>
<span class="linenos"> 58</span><span class="sd">            architecture (List): The architecture of Darknet. See config.py for more details.</span>
<span class="linenos"> 59</span><span class="sd">            in_channels (int): The in_channels. Defaults to 3 as we expect RGB images.</span>
<span class="linenos"> 60</span><span class="sd">            S (int): Grid Size. Defaults to 7.</span>
<span class="linenos"> 61</span><span class="sd">            B (int): Number of Bounding Boxes to predict. Defaults to 2.</span>
<span class="linenos"> 62</span><span class="sd">            C (int): Number of Classes. Defaults to 20.</span>
<span class="linenos"> 63</span><span class="sd">            init_weights (bool): Whether to init weights. Defaults to False.</span>
<span class="linenos"> 64</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos"> 65</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="linenos"> 66</span>
<span class="linenos"> 67</span>        <span class="bp">self</span><span class="o">.</span><span class="n">architecture</span> <span class="o">=</span> <span class="n">architecture</span>
<span class="linenos"> 68</span>        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
<span class="linenos"> 69</span>        <span class="bp">self</span><span class="o">.</span><span class="n">S</span> <span class="o">=</span> <span class="n">S</span>
<span class="linenos"> 70</span>        <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span>
<span class="linenos"> 71</span>        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
<span class="linenos"> 72</span>
<span class="linenos"> 73</span>        <span class="c1"># backbone is darknet</span>
<span class="linenos"> 74</span>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_darknet_backbone</span><span class="p">()</span>
<span class="linenos"> 75</span>        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_darknet_head</span><span class="p">()</span>
<span class="linenos"> 76</span>
<span class="linenos"> 77</span>        <span class="k">if</span> <span class="n">init_weights</span><span class="p">:</span>
<span class="linenos"> 78</span>            <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span>
<span class="linenos"> 79</span>
<span class="linenos"> 80</span>    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 81</span>        <span class="sd">&quot;&quot;&quot;Initialize weights for Conv2d, BatchNorm2d, and Linear layers.&quot;&quot;&quot;</span>
<span class="linenos"> 82</span>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
<span class="linenos"> 83</span>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
<span class="linenos"> 84</span>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span>
<span class="linenos"> 85</span>                    <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;fan_in&quot;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s2">&quot;leaky_relu&quot;</span>
<span class="linenos"> 86</span>                <span class="p">)</span>
<span class="linenos"> 87</span>                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 88</span>                    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="linenos"> 89</span>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
<span class="linenos"> 90</span>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 91</span>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="linenos"> 92</span>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
<span class="linenos"> 93</span>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="linenos"> 94</span>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="linenos"> 95</span>
<span class="linenos"> 96</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos"> 97</span>        <span class="sd">&quot;&quot;&quot;Forward pass.&quot;&quot;&quot;</span>
<span class="linenos"> 98</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="linenos"> 99</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="linenos">100</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
<span class="linenos">101</span>        <span class="c1"># if self.squash_type == &quot;flatten&quot;:</span>
<span class="linenos">102</span>        <span class="c1">#     x = torch.flatten(x, start_dim=1)</span>
<span class="linenos">103</span>        <span class="c1"># elif self.squash_type == &quot;3D&quot;:</span>
<span class="linenos">104</span>        <span class="c1">#     x = x.reshape(-1, self.S, self.S, self.C + self.B * 5)</span>
<span class="linenos">105</span>        <span class="c1"># elif self.squash_type == &quot;2D&quot;:</span>
<span class="linenos">106</span>        <span class="c1">#     x = x.reshape(-1, self.S * self.S, self.C + self.B * 5)</span>
<span class="linenos">107</span>        <span class="k">return</span> <span class="n">x</span>
<span class="linenos">108</span>
<span class="linenos">109</span>    <span class="k">def</span> <span class="nf">_create_darknet_backbone</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
<span class="linenos">110</span>        <span class="sd">&quot;&quot;&quot;Create Darknet backbone.&quot;&quot;&quot;</span>
<span class="linenos">111</span>        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="linenos">112</span>        <span class="n">in_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
<span class="linenos">113</span>
<span class="linenos">114</span>        <span class="k">for</span> <span class="n">layer_config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">architecture</span><span class="p">:</span>
<span class="linenos">115</span>            <span class="c1"># convolutional layer</span>
<span class="linenos">116</span>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_config</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
<span class="linenos">117</span>                <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="n">layer_config</span>
<span class="linenos">118</span>                <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span>
<span class="linenos">119</span>                    <span class="n">CNNBlock</span><span class="p">(</span>
<span class="linenos">120</span>                        <span class="n">in_channels</span><span class="p">,</span>
<span class="linenos">121</span>                        <span class="n">out_channels</span><span class="p">,</span>
<span class="linenos">122</span>                        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
<span class="linenos">123</span>                        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
<span class="linenos">124</span>                        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
<span class="linenos">125</span>                    <span class="p">)</span>
<span class="linenos">126</span>                <span class="p">]</span>
<span class="linenos">127</span>                <span class="c1"># update next layer&#39;s in_channels to be current layer&#39;s out_channels</span>
<span class="linenos">128</span>                <span class="n">in_channels</span> <span class="o">=</span> <span class="n">layer_config</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="linenos">129</span>
<span class="linenos">130</span>            <span class="c1"># max pooling</span>
<span class="linenos">131</span>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_config</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">layer_config</span> <span class="o">==</span> <span class="s2">&quot;M&quot;</span><span class="p">:</span>
<span class="linenos">132</span>                <span class="c1"># hardcode maxpooling layer</span>
<span class="linenos">133</span>                <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))]</span>
<span class="linenos">134</span>
<span class="linenos">135</span>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_config</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<span class="linenos">136</span>                <span class="n">conv1</span> <span class="o">=</span> <span class="n">layer_config</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="linenos">137</span>                <span class="n">conv2</span> <span class="o">=</span> <span class="n">layer_config</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="linenos">138</span>                <span class="n">num_repeats</span> <span class="o">=</span> <span class="n">layer_config</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="linenos">139</span>
<span class="linenos">140</span>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_repeats</span><span class="p">):</span>
<span class="linenos">141</span>                    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span>
<span class="linenos">142</span>                        <span class="n">CNNBlock</span><span class="p">(</span>
<span class="linenos">143</span>                            <span class="n">in_channels</span><span class="p">,</span>
<span class="linenos">144</span>                            <span class="n">out_channels</span><span class="o">=</span><span class="n">conv1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="linenos">145</span>                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<span class="linenos">146</span>                            <span class="n">stride</span><span class="o">=</span><span class="n">conv1</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
<span class="linenos">147</span>                            <span class="n">padding</span><span class="o">=</span><span class="n">conv1</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
<span class="linenos">148</span>                        <span class="p">)</span>
<span class="linenos">149</span>                    <span class="p">]</span>
<span class="linenos">150</span>                    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span>
<span class="linenos">151</span>                        <span class="n">CNNBlock</span><span class="p">(</span>
<span class="linenos">152</span>                            <span class="n">in_channels</span><span class="o">=</span><span class="n">conv1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="linenos">153</span>                            <span class="n">out_channels</span><span class="o">=</span><span class="n">conv2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="linenos">154</span>                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<span class="linenos">155</span>                            <span class="n">stride</span><span class="o">=</span><span class="n">conv2</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
<span class="linenos">156</span>                            <span class="n">padding</span><span class="o">=</span><span class="n">conv2</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
<span class="linenos">157</span>                        <span class="p">)</span>
<span class="linenos">158</span>                    <span class="p">]</span>
<span class="linenos">159</span>                    <span class="n">in_channels</span> <span class="o">=</span> <span class="n">conv2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="linenos">160</span>
<span class="linenos">161</span>        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
<span class="linenos">162</span>
<span class="linenos">163</span>    <span class="k">def</span> <span class="nf">_create_darknet_head</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
<span class="linenos">164</span>        <span class="sd">&quot;&quot;&quot;Create the fully connected layers of Darknet head.</span>
<span class="linenos">165</span>
<span class="linenos">166</span><span class="sd">        Note:</span>
<span class="linenos">167</span><span class="sd">            1. In original paper this should be</span>
<span class="linenos">168</span><span class="sd">                nn.Sequential(</span>
<span class="linenos">169</span><span class="sd">                    nn.Linear(1024*S*S, 4096),</span>
<span class="linenos">170</span><span class="sd">                    nn.LeakyReLU(0.1),</span>
<span class="linenos">171</span><span class="sd">                    nn.Linear(4096, S*S*(B*5+C))</span>
<span class="linenos">172</span><span class="sd">                    )</span>
<span class="linenos">173</span><span class="sd">            2. You can add `nn.Sigmoid` to the last layer to stabilize training</span>
<span class="linenos">174</span><span class="sd">               and avoid exploding gradients with high loss since sigmoid will</span>
<span class="linenos">175</span><span class="sd">               force your values to be between 0 and 1. Remember if you do not put</span>
<span class="linenos">176</span><span class="sd">               this your predictions can be unbounded and contain negative numbers even.</span>
<span class="linenos">177</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">178</span>
<span class="linenos">179</span>        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<span class="linenos">180</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
<span class="linenos">181</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span>
<span class="linenos">182</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
<span class="linenos">183</span>            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span>
<span class="linenos">184</span>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)),</span>
<span class="linenos">185</span>            <span class="c1"># nn.Sigmoid(),</span>
<span class="linenos">186</span>        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We then run a forward pass of the model as a sanity check.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="linenos"> 2</span><span class="n">image_size</span> <span class="o">=</span> <span class="mi">448</span>
<span class="linenos"> 3</span><span class="n">in_channels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="linenos"> 4</span><span class="n">S</span> <span class="o">=</span> <span class="mi">7</span>
<span class="linenos"> 5</span><span class="n">B</span> <span class="o">=</span> <span class="mi">2</span>
<span class="linenos"> 6</span><span class="n">C</span> <span class="o">=</span> <span class="mi">20</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="n">DARKNET_ARCHITECTURE</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos"> 9</span>    <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span class="linenos">10</span>    <span class="s2">&quot;M&quot;</span><span class="p">,</span>
<span class="linenos">11</span>    <span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="linenos">12</span>    <span class="s2">&quot;M&quot;</span><span class="p">,</span>
<span class="linenos">13</span>    <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="linenos">14</span>    <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="linenos">15</span>    <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="linenos">16</span>    <span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="linenos">17</span>    <span class="s2">&quot;M&quot;</span><span class="p">,</span>
<span class="linenos">18</span>    <span class="p">[(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">4</span><span class="p">],</span>
<span class="linenos">19</span>    <span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="linenos">20</span>    <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="linenos">21</span>    <span class="s2">&quot;M&quot;</span><span class="p">,</span>
<span class="linenos">22</span>    <span class="p">[(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">],</span>
<span class="linenos">23</span>    <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="linenos">24</span>    <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="linenos">25</span>    <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="linenos">26</span>    <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="linenos">27</span><span class="p">]</span>
<span class="linenos">28</span>
<span class="linenos">29</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">)</span>
<span class="linenos">30</span><span class="n">y_trues</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">B</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">C</span><span class="p">)</span>
<span class="linenos">31</span>
<span class="linenos">32</span><span class="n">yolov1</span> <span class="o">=</span> <span class="n">Yolov1Darknet</span><span class="p">(</span>
<span class="linenos">33</span>    <span class="n">architecture</span><span class="o">=</span><span class="n">DARKNET_ARCHITECTURE</span><span class="p">,</span>
<span class="linenos">34</span>    <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
<span class="linenos">35</span>    <span class="n">S</span><span class="o">=</span><span class="n">S</span><span class="p">,</span>
<span class="linenos">36</span>    <span class="n">B</span><span class="o">=</span><span class="n">B</span><span class="p">,</span>
<span class="linenos">37</span>    <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span>
<span class="linenos">38</span><span class="p">)</span>
<span class="linenos">39</span>
<span class="linenos">40</span><span class="n">y_preds</span> <span class="o">=</span> <span class="n">yolov1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="linenos">41</span>
<span class="linenos">42</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x.shape: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">43</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y_trues.shape: </span><span class="si">{</span><span class="n">y_trues</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">44</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y_preds.shape: </span><span class="si">{</span><span class="n">y_preds</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x.shape: torch.Size([4, 3, 448, 448])
y_trues.shape: torch.Size([4, 7, 7, 30])
y_preds.shape: torch.Size([4, 7, 7, 30])
</pre></div>
</div>
</div>
</div>
<p>The input label <code class="docutils literal notranslate"><span class="pre">y_trues</span></code> and <code class="docutils literal notranslate"><span class="pre">y_preds</span></code> are of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">S,</span> <span class="pre">S,</span> <span class="pre">B</span> <span class="pre">*</span> <span class="pre">5</span> <span class="pre">+</span> <span class="pre">C)</span></code>,
in our case is <code class="docutils literal notranslate"><span class="pre">(4,</span> <span class="pre">7,</span> <span class="pre">7,</span> <span class="pre">30)</span></code> and indeed the shape that we expected in <a class="reference internal" href="#label-matrix"><span class="std std-numref">Fig. 2</span></a>.
The additional first dimension is the batch size.
We will talk more in the next few sections.</p>
</section>
<section id="model-summary">
<span id="yolov1-md-model-summary"></span><h3>Model Summary<a class="headerlink" href="#model-summary" title="Permalink to this headline">#</a></h3>
<p>We use <code class="docutils literal notranslate"><span class="pre">torchinfo</span></code> package to print out the model summary. This is a useful tool that
is similar to <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> in Keras.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">torchinfo</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span>
<span class="linenos">2</span>    <span class="n">yolov1</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">)</span>
<span class="linenos">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Yolov1Darknet                            [4, 7, 7, 30]             --
├─Sequential: 1-1                        [4, 1024, 7, 7]           --
│    └─CNNBlock: 2-1                     [4, 64, 224, 224]         --
│    │    └─Conv2d: 3-1                  [4, 64, 224, 224]         9,408
│    │    └─BatchNorm2d: 3-2             [4, 64, 224, 224]         128
│    │    └─LeakyReLU: 3-3               [4, 64, 224, 224]         --
│    └─MaxPool2d: 2-2                    [4, 64, 112, 112]         --
│    └─CNNBlock: 2-3                     [4, 192, 112, 112]        --
│    │    └─Conv2d: 3-4                  [4, 192, 112, 112]        110,592
│    │    └─BatchNorm2d: 3-5             [4, 192, 112, 112]        384
│    │    └─LeakyReLU: 3-6               [4, 192, 112, 112]        --
│    └─MaxPool2d: 2-4                    [4, 192, 56, 56]          --
│    └─CNNBlock: 2-5                     [4, 128, 56, 56]          --
│    │    └─Conv2d: 3-7                  [4, 128, 56, 56]          24,576
│    │    └─BatchNorm2d: 3-8             [4, 128, 56, 56]          256
│    │    └─LeakyReLU: 3-9               [4, 128, 56, 56]          --
│    └─CNNBlock: 2-6                     [4, 256, 56, 56]          --
│    │    └─Conv2d: 3-10                 [4, 256, 56, 56]          294,912
│    │    └─BatchNorm2d: 3-11            [4, 256, 56, 56]          512
│    │    └─LeakyReLU: 3-12              [4, 256, 56, 56]          --
│    └─CNNBlock: 2-7                     [4, 256, 56, 56]          --
│    │    └─Conv2d: 3-13                 [4, 256, 56, 56]          65,536
│    │    └─BatchNorm2d: 3-14            [4, 256, 56, 56]          512
│    │    └─LeakyReLU: 3-15              [4, 256, 56, 56]          --
│    └─CNNBlock: 2-8                     [4, 512, 56, 56]          --
│    │    └─Conv2d: 3-16                 [4, 512, 56, 56]          1,179,648
│    │    └─BatchNorm2d: 3-17            [4, 512, 56, 56]          1,024
│    │    └─LeakyReLU: 3-18              [4, 512, 56, 56]          --
│    └─MaxPool2d: 2-9                    [4, 512, 28, 28]          --
│    └─CNNBlock: 2-10                    [4, 256, 28, 28]          --
│    │    └─Conv2d: 3-19                 [4, 256, 28, 28]          131,072
│    │    └─BatchNorm2d: 3-20            [4, 256, 28, 28]          512
│    │    └─LeakyReLU: 3-21              [4, 256, 28, 28]          --
│    └─CNNBlock: 2-11                    [4, 512, 28, 28]          --
│    │    └─Conv2d: 3-22                 [4, 512, 28, 28]          1,179,648
│    │    └─BatchNorm2d: 3-23            [4, 512, 28, 28]          1,024
│    │    └─LeakyReLU: 3-24              [4, 512, 28, 28]          --
│    └─CNNBlock: 2-12                    [4, 256, 28, 28]          --
│    │    └─Conv2d: 3-25                 [4, 256, 28, 28]          131,072
│    │    └─BatchNorm2d: 3-26            [4, 256, 28, 28]          512
│    │    └─LeakyReLU: 3-27              [4, 256, 28, 28]          --
│    └─CNNBlock: 2-13                    [4, 512, 28, 28]          --
│    │    └─Conv2d: 3-28                 [4, 512, 28, 28]          1,179,648
│    │    └─BatchNorm2d: 3-29            [4, 512, 28, 28]          1,024
│    │    └─LeakyReLU: 3-30              [4, 512, 28, 28]          --
│    └─CNNBlock: 2-14                    [4, 256, 28, 28]          --
│    │    └─Conv2d: 3-31                 [4, 256, 28, 28]          131,072
│    │    └─BatchNorm2d: 3-32            [4, 256, 28, 28]          512
│    │    └─LeakyReLU: 3-33              [4, 256, 28, 28]          --
│    └─CNNBlock: 2-15                    [4, 512, 28, 28]          --
│    │    └─Conv2d: 3-34                 [4, 512, 28, 28]          1,179,648
│    │    └─BatchNorm2d: 3-35            [4, 512, 28, 28]          1,024
│    │    └─LeakyReLU: 3-36              [4, 512, 28, 28]          --
│    └─CNNBlock: 2-16                    [4, 256, 28, 28]          --
│    │    └─Conv2d: 3-37                 [4, 256, 28, 28]          131,072
│    │    └─BatchNorm2d: 3-38            [4, 256, 28, 28]          512
│    │    └─LeakyReLU: 3-39              [4, 256, 28, 28]          --
│    └─CNNBlock: 2-17                    [4, 512, 28, 28]          --
│    │    └─Conv2d: 3-40                 [4, 512, 28, 28]          1,179,648
│    │    └─BatchNorm2d: 3-41            [4, 512, 28, 28]          1,024
│    │    └─LeakyReLU: 3-42              [4, 512, 28, 28]          --
│    └─CNNBlock: 2-18                    [4, 512, 28, 28]          --
│    │    └─Conv2d: 3-43                 [4, 512, 28, 28]          262,144
│    │    └─BatchNorm2d: 3-44            [4, 512, 28, 28]          1,024
│    │    └─LeakyReLU: 3-45              [4, 512, 28, 28]          --
│    └─CNNBlock: 2-19                    [4, 1024, 28, 28]         --
│    │    └─Conv2d: 3-46                 [4, 1024, 28, 28]         4,718,592
│    │    └─BatchNorm2d: 3-47            [4, 1024, 28, 28]         2,048
│    │    └─LeakyReLU: 3-48              [4, 1024, 28, 28]         --
│    └─MaxPool2d: 2-20                   [4, 1024, 14, 14]         --
│    └─CNNBlock: 2-21                    [4, 512, 14, 14]          --
│    │    └─Conv2d: 3-49                 [4, 512, 14, 14]          524,288
│    │    └─BatchNorm2d: 3-50            [4, 512, 14, 14]          1,024
│    │    └─LeakyReLU: 3-51              [4, 512, 14, 14]          --
│    └─CNNBlock: 2-22                    [4, 1024, 14, 14]         --
│    │    └─Conv2d: 3-52                 [4, 1024, 14, 14]         4,718,592
│    │    └─BatchNorm2d: 3-53            [4, 1024, 14, 14]         2,048
│    │    └─LeakyReLU: 3-54              [4, 1024, 14, 14]         --
│    └─CNNBlock: 2-23                    [4, 512, 14, 14]          --
│    │    └─Conv2d: 3-55                 [4, 512, 14, 14]          524,288
│    │    └─BatchNorm2d: 3-56            [4, 512, 14, 14]          1,024
│    │    └─LeakyReLU: 3-57              [4, 512, 14, 14]          --
│    └─CNNBlock: 2-24                    [4, 1024, 14, 14]         --
│    │    └─Conv2d: 3-58                 [4, 1024, 14, 14]         4,718,592
│    │    └─BatchNorm2d: 3-59            [4, 1024, 14, 14]         2,048
│    │    └─LeakyReLU: 3-60              [4, 1024, 14, 14]         --
│    └─CNNBlock: 2-25                    [4, 1024, 14, 14]         --
│    │    └─Conv2d: 3-61                 [4, 1024, 14, 14]         9,437,184
│    │    └─BatchNorm2d: 3-62            [4, 1024, 14, 14]         2,048
│    │    └─LeakyReLU: 3-63              [4, 1024, 14, 14]         --
│    └─CNNBlock: 2-26                    [4, 1024, 7, 7]           --
│    │    └─Conv2d: 3-64                 [4, 1024, 7, 7]           9,437,184
│    │    └─BatchNorm2d: 3-65            [4, 1024, 7, 7]           2,048
│    │    └─LeakyReLU: 3-66              [4, 1024, 7, 7]           --
│    └─CNNBlock: 2-27                    [4, 1024, 7, 7]           --
│    │    └─Conv2d: 3-67                 [4, 1024, 7, 7]           9,437,184
│    │    └─BatchNorm2d: 3-68            [4, 1024, 7, 7]           2,048
│    │    └─LeakyReLU: 3-69              [4, 1024, 7, 7]           --
│    └─CNNBlock: 2-28                    [4, 1024, 7, 7]           --
│    │    └─Conv2d: 3-70                 [4, 1024, 7, 7]           9,437,184
│    │    └─BatchNorm2d: 3-71            [4, 1024, 7, 7]           2,048
│    │    └─LeakyReLU: 3-72              [4, 1024, 7, 7]           --
├─Sequential: 1-2                        [4, 1470]                 --
│    └─Flatten: 2-29                     [4, 50176]                --
│    └─Linear: 2-30                      [4, 4096]                 205,524,992
│    └─Dropout: 2-31                     [4, 4096]                 --
│    └─LeakyReLU: 2-32                   [4, 4096]                 --
│    └─Linear: 2-33                      [4, 1470]                 6,022,590
==========================================================================================
Total params: 271,716,734
Trainable params: 271,716,734
Non-trainable params: 0
Total mult-adds (G): 81.14
==========================================================================================
Input size (MB): 9.63
Forward/backward pass size (MB): 883.28
Params size (MB): 1086.87
Estimated Total Size (MB): 1979.78
==========================================================================================
</pre></div>
</div>
</div>
</div>
</section>
<section id="backbone">
<h3>Backbone<a class="headerlink" href="#backbone" title="Permalink to this headline">#</a></h3>
<p>We use Darknet as our backbone. The backbone serves as a feature extractor.
This means that we can replace the backbone with any other feature extractor.</p>
<p>For example, we can replace the Darknet backbone with ResNet50, which is a 50-layer Convoluational
Neural Network. You only need to make sure that the output of the backbone can match the shape
of the input of the YOLO head. We often overcome the shape mismatch issue with
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html"><strong>Global Average Pooling</strong></a>.</p>
</section>
<section id="head">
<span id="yolov1-md-head"></span><h3>Head<a class="headerlink" href="#head" title="Permalink to this headline">#</a></h3>
<p>Head is the part of the model that takes the output of the backbone and
transforms it into the output of the model. In this case, we need to transform
whatever shape we get from the backbone into the shape of the label matrix, which is
<code class="docutils literal notranslate"><span class="pre">(7,</span> <span class="pre">7,</span> <span class="pre">30)</span></code>.</p>
<p>Let’s print out the last layer(s) of the YOLOv1 model:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;yolov1 last layer: </span><span class="si">{</span><span class="n">yolov1</span><span class="o">.</span><span class="n">head</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>yolov1 last layer: Linear(in_features=4096, out_features=1470, bias=True)
</pre></div>
</div>
</div>
</div>
<p>Unfortunately, the info is not very helpful. We will refer back to the model summary
in section <a class="reference internal" href="#yolov1-md-model-summary"><span class="std std-ref">model summary</span></a> to understand the output shape of the last layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Linear</span><span class="p">:</span> <span class="mi">2</span><span class="o">-</span><span class="mi">33</span>                      <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1470</span><span class="p">]</span>                 <span class="mi">6</span><span class="p">,</span><span class="mi">022</span><span class="p">,</span><span class="mi">590</span>
</pre></div>
</div>
<p>Notice that the last layer is actually a linear (fc) layer with shape <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">1470]</span></code>. This is in
contrast of the output shape of <code class="docutils literal notranslate"><span class="pre">y_preds</span></code> which is <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">7,</span> <span class="pre">7,</span> <span class="pre">30]</span></code>. This is because in the
<code class="docutils literal notranslate"><span class="pre">forward</span></code> method, we reshaped the output of the last layer to be <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">7,</span> <span class="pre">7,</span> <span class="pre">30]</span></code>.</p>
<div class="important admonition">
<p class="admonition-title">Why do we need to reshape the output of the last layer?</p>
<p>The reason of the reshape is because of better interpretation.
More concretely, the paper said that the core idea is to divide the input image into an <span class="math notranslate nohighlight">\(S \times S\)</span>
grid, where each grid cell has a shape of <span class="math notranslate nohighlight">\(5B + C\)</span>. See <a class="reference internal" href="#label-matrix"><span class="std std-numref">Fig. 2</span></a> for a visual example.</p>
</div>
<div class="admonition-d-matrix-vs-3d-tensor admonition">
<p class="admonition-title">2D matrix vs 3D tensor</p>
<p>If we remove the batch size dimension, then the output tensor of <code class="docutils literal notranslate"><span class="pre">y_trues</span></code> and <code class="docutils literal notranslate"><span class="pre">y_preds</span></code> will be
a 3D tensor of shape <code class="docutils literal notranslate"><span class="pre">(S,</span> <span class="pre">S,</span> <span class="pre">B</span> <span class="pre">*</span> <span class="pre">5</span> <span class="pre">+</span> <span class="pre">C)</span> <span class="pre">=</span> <span class="pre">(7,</span> <span class="pre">7,</span> <span class="pre">30)</span></code>.</p>
<p>We can also think of it as a 2D matrix, where we flatten the first and second dimension, essentially
collapsing the <span class="math notranslate nohighlight">\(7 \times 7\)</span> grid into a single dimension of <span class="math notranslate nohighlight">\(49\)</span>. The reason why I did this is for
easier interpretation, as a 2D matrix is easier to visualize than a 3D tensor.</p>
<p>We will carry this idea forward to the next few sections.</p>
</div>
</section>
</section>
<section id="anchors-and-prior-boxes">
<h2>Anchors and Prior Boxes<a class="headerlink" href="#anchors-and-prior-boxes" title="Permalink to this headline">#</a></h2>
<p>Before we move on, it is beneficial to read on what
<a class="reference external" href="https://www.harrysprojects.com/articles/fasterrcnn.html">anchors and prior boxes are</a>.
This will give you a better idea on why the author divide the input image into an <span class="math notranslate nohighlight">\(S \times S\)</span> grid.</p>
</section>
<section id="bounding-box-parametrization">
<h2>Bounding Box Parametrization<a class="headerlink" href="#bounding-box-parametrization" title="Permalink to this headline">#</a></h2>
<p>Before we move on, it is beneficial to read on what
<a class="reference external" href="https://www.harrysprojects.com/articles/fastrcnn.html">bounding box parametrization is</a>.
This will give you a better idea on why the author wants to transform the bounding box
into offsets pertaining to the grid cell.</p>
</section>
<section id="yolov1-encoding-setup">
<h2>YOLOv1 Encoding Setup<a class="headerlink" href="#yolov1-encoding-setup" title="Permalink to this headline">#</a></h2>
<p>As a continuation of the previous section on <a class="reference internal" href="#yolov1-md-head"><span class="std std-ref">head</span></a>, we will now answer the question
on why we reshape the output of the last layer to be <code class="docutils literal notranslate"><span class="pre">[7,</span> <span class="pre">7,</span> <span class="pre">30]</span></code>.</p>
<p>The definitions of some keywords are defined in section on
<a class="reference internal" href="#yolov1-md-other-important-notations"><span class="std std-ref">definitions</span></a>.</p>
<p>Quoting from the paper:</p>
<blockquote>
<div><p>Our system divides the input image into an S × S grid.
If the center of an object falls into a grid cell, that grid cell
is responsible for detecting that object <span id="id1">[<a class="reference internal" href="#id13" title="Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: unified, real-time object detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. doi:10.1109/cvpr.2016.91.">Redmon <em>et al.</em>, 2016</a>]</span>.</p>
</div></blockquote>
<p>Let’s visualize this idea with the help of the diagram below.</p>
<figure class="align-default" id="image-1-grids">
<img alt="_images/yolov1_image_grids.jpg" src="_images/yolov1_image_grids.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Image 1 with grids.</span><a class="headerlink" href="#image-1-grids" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Given an input image <span class="math notranslate nohighlight">\(\X\)</span> at part 1 of <a class="reference internal" href="#image-1-grids"><span class="std std-numref">Fig. 3</span></a>, we divide the image into an <span class="math notranslate nohighlight">\(S \times S\)</span> grid.
We see there’s a <strong>person</strong> and a <strong>dog</strong> in the image.</p>
<p>Part 2 of <a class="reference internal" href="#image-1-grids"><span class="std std-numref">Fig. 3</span></a> shows the <strong>ground truth</strong> bounding box of the object.</p>
<p>Part 3 of <a class="reference internal" href="#image-1-grids"><span class="std std-numref">Fig. 3</span></a> adds on the center of the bounding boxes as dots.</p>
<p>In the case of the paper, <span class="math notranslate nohighlight">\(S=7\)</span> implies YOLOv1 breaks the image up into a grid of size <span class="math notranslate nohighlight">\(7 \times 7\)</span>,
as shown in part 4 of <a class="reference internal" href="#image-1-grids"><span class="std std-numref">Fig. 3</span></a>. The grids are drawn in white. There are a total
of <span class="math notranslate nohighlight">\(7 \times 7 = 49\)</span> grid cells. Note the distinction between the size of the grid <span class="math notranslate nohighlight">\(S\)</span> and the grid cell.</p>
<p>These grid cells represent prior boxes so that when the network predicts box coordinates,
it has something to reference them from. Remember that earlier I said whenever you predict boxes,
you have to say with respect to what? Well it’s with respect to these grid cells. More concretely,
the network can detect objects by predicting scales and offsets from those prior boxes.</p>
<p>As an illustrative example, take the prior box on the 4th row and 4th column.
It’s centered on the <strong>person</strong>, so it seems reasonable that this prior box should be responsible
for predicting the person in this image.
The 7x7 grid isn’t actually drawn on the image, it’s just implied,
and the thing that implies it is the 7x7 grid in the output tensor.
You can imagine overlaying the output tensor on the image, and each cell corresponds to a
part of the image. If you understand anchors, this idea should feel famililar to you
<span id="id2">[<a class="reference internal" href="#id14" title="Harry Turner. Yolo v1. 2021. URL: https://www.harrysprojects.com/articles/yolov1.html.">Turner, 2021</a>]</span>.</p>
<p>Consequently, part 5 of <a class="reference internal" href="#image-1-grids"><span class="std std-numref">Fig. 3</span></a> highlighted the “responsible” grid cell for each object in red.</p>
<p>So we have understood the first quote on why we divide the input image into an <span class="math notranslate nohighlight">\(S \times S\)</span> grid.
Since the person’s center falls into the 4th row and 4th column, the grid cell at that position is then
our <strong>ground truth</strong> bounding box for the person in this image. Similarly, the dog’s center falls into the
5th row and 3rd column, the grid cell at that position is then our <strong>ground truth</strong> bounding box for the dog
in this image. As an aside, all other grid cells are <strong>background</strong> and we will ignore them by assigning
them all zeros, more on that later.</p>
<p>We have answered the reason for why we need to divide the input image into an <span class="math notranslate nohighlight">\(S \times S\)</span> grid. Next is
why there the output tensor’s shape has a 3rd dimension of depth <span class="math notranslate nohighlight">\(B * 5 + C = 30\)</span>?</p>
<blockquote>
<div><p>Each grid cell predicts <span class="math notranslate nohighlight">\(B\)</span> bounding boxes and confidence
scores for those boxes as well as <span class="math notranslate nohighlight">\(C\)</span> conditional class probabilities <span id="id3">[<a class="reference internal" href="#id13" title="Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: unified, real-time object detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. doi:10.1109/cvpr.2016.91.">Redmon <em>et al.</em>, 2016</a>]</span>.</p>
</div></blockquote>
<p>For each grid cell <span class="math notranslate nohighlight">\(i\)</span>, we a 30-d vector, 30 is derived from <span class="math notranslate nohighlight">\(5 \times B + C\)</span> elements.</p>
<p>So each cell is responsible for predicting boxes from a single part of the image. More specifically, each cell is responsible for predicting precisely two boxes for each part of the image. Note that there are 49 cells, and each cell is predicting two boxes, so the whole network is only going to predict 98 boxes. That number is fixed.</p>
<p>In order to predict a single box, the network must output a number of things. Firstly it must encode the coordinates of the box which YOLO encodes as (x, y, w, h), where x and y are the center of the box. Earlier I suggested you familiarise yourself with box parameterisation, this is because YOLO does not output the actual coordinates of the box, but parameterised coordinates instead. Firstly, the width and height are normalised with respect to the image width, so if the network outputs a value of 1.0 for the width, it’s saying the box should span the entire image, likewise 0.5 means it’s half the width of the image. Note that the width and height have nothing to do with the actual grid cell itself. The x and y values are parameterised with respect to the grid cell, they represent offsets from the grid cell position. The grid cell has a width and height which is equal to 1/S (we’ve normalised the image to have width and height 1.0). If the network outputs a value of 1.0 for x, then it’s saying that the x value of the box is the x position of the grid cell plus the width of the grid cell.</p>
<p>Secondly, YOLO also predicts a confidence score for each box which represents the probability that the box contains an object. Lastly, YOLO predicts a class, which is represented by a vector of C values, and the predicted class is the one with the highest value. Now, here’s the catch. YOLO does not predict a class for every box, it predicts a class for each cell. But each cell is associated with two boxes, so those boxes will have the same predicted class, even though they may have different shapes and positions. Let’s tie all that together visually, let me copy down my diagram again.</p>
<figure class="align-default" id="label-matrix-copied">
<img alt="https://storage.googleapis.com/reighns/images/label_matrix.png" src="https://storage.googleapis.com/reighns/images/label_matrix.png" />
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">The output tensor from YOLOv1’s last layer.</span><a class="headerlink" href="#label-matrix-copied" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The first five values encode the location and confidence of the first box, the next five encode the location and confidence of the next box, and the final 20 encode the 20 classes (because Pascal VOC has 20 classes). In total, the size of the vector is 5xB + C where B is the number of boxes, and C is the number of classes.</p>
<p>The way that YOLO actually predicts boxes, is by predicting target scale and offset values for each prior, these are parameterised by normalising by the width and height of the image. For example, take the highlighted top right cell in the output tensor, this particular cell corresponds to the far top right cell in the input image (which looks like the branch of a tree). That cell represents a prior box, which will have a width and height equal to the image width divided by 7 and image height divided by 7 respectively, and the location being the top right. The outputs from this single cell will therefore shift and stretch that prior box into new positions that hopefully contain the object.</p>
<p>Because the cell predicts two boxes, it will shift and stretch the prior box in two different ways, possibly to cover two different objects (but both are constrained to have the same class). You might wonder why it’s trying to do two boxes. The answer is probably because 49 boxes isn’t enough, especially when there are lots of objects close together, although what tends to happen during training is that the predicted boxes become specialised. So one box might learn to find big things, the other might learn to find small things, this may help the network generalise to other domains.</p>
<p>To wrap this section up, I want to point out one difference between the approach that YOLO has taken, and the anchor boxes in the Region Proposal Network. Anchors in the RPN actually refer to the nine different aspect ratios and scales from a single location. In other words, each position in the RPN predicts nine different boxes from nine different prior widths and heights. In contrast, it’s as if YOLO has two anchors at each position, but they have the same width and height. YOLO does not introduce variations in aspect ratio or size into the anchor boxes.</p>
<p>As a final note to help your intuition, it’s reasonable to wonder why they didn’t predict a class for each box. What would the output look like? You’d still have 7x7 cells, but instead of each cell being of size 5xB + C, you’d have (5+C) x B. So for two boxes, you’d have 50 outputs, not 30. That doesn’t seem unreasonable, and it gives the network the flexibility to predict two different classes from the same location.</p>
</section>
<section id="notations-and-definitions">
<h2>Notations and Definitions<a class="headerlink" href="#notations-and-definitions" title="Permalink to this headline">#</a></h2>
<section id="sample-image">
<h3>Sample Image<a class="headerlink" href="#sample-image" title="Permalink to this headline">#</a></h3>
<figure class="align-default" id="yolov1-sample-image">
<img alt="https://storage.googleapis.com/reighns/images/grid_on_image.PNG" src="https://storage.googleapis.com/reighns/images/grid_on_image.PNG" />
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Sample Image with Grids.</span><a class="headerlink" href="#yolov1-sample-image" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="yolov1-md-bounding-box-parametrization">
<span id="id4"></span><h3>Bounding Box Parametrization<a class="headerlink" href="#yolov1-md-bounding-box-parametrization" title="Permalink to this headline">#</a></h3>
<p>Given a yolo format bounding box, we will perform parametrization to transform the
coordinates of the bounding box to a more convenient form. Before that, let us
define some notations.</p>
<div class="proof definition admonition" id="yolo-bbox">
<p class="admonition-title"><span class="caption-number">Definition 1 </span> (YOLO Format Bounding Box)</p>
<section class="definition-content" id="proof-content">
<p>The YOLO format bounding box is a 4-tuple vector consisting of the coordinates of
the bounding box in the following order:</p>
<div class="math notranslate nohighlight" id="equation-yolo-bbox">
<span class="eqno">(1)<a class="headerlink" href="#equation-yolo-bbox" title="Permalink to this equation">#</a></span>\[
\byolo = \begin{bmatrix} \xx_c &amp; \yy_c &amp; \ww_n &amp; \hh_n \end{bmatrix} \in \R^{1 \times 4}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\xx_c\)</span> and <span class="math notranslate nohighlight">\(\yy_c\)</span> are the coordinates of the center of the bounding box,
normalized with respect to the image width and height;</p></li>
<li><p><span class="math notranslate nohighlight">\(\ww_n\)</span> and <span class="math notranslate nohighlight">\(\hh_n\)</span> are the width and height of the bounding box,
normalized with respect to the image width and height.</p></li>
</ul>
<p>Consequently, all coordinates are in the range <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
</section>
</div><p>We could be done at this step and ask the model to predict the bounding box in
YOLO format. However, the author proposes a more convenient parametrization for
the model to learn better:</p>
<ol class="arabic simple">
<li><p>The center of the bounding box is parametrized as the offset from the top-left
corner of the grid cell to the center of the bounding box. We will go through an
an example later.</p></li>
<li><p>The width and height of the bounding box are parametrized to the square root
of the width and height of the bounding box.</p></li>
</ol>
<div class="admonition-intuition-parametrization-of-bounding-box admonition">
<p class="admonition-title">Intuition: Parametrization of Bounding Box</p>
<p>The loss function of YOLOv1 is using mean squared errror.</p>
<p>The square root is present so that errors in small bounding boxes are more penalizing
than errors in big bounding boxes.
Recall that square root mapping expands the range of small values for values between
<span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>For example, if the normalized width and height of a bounding box is <span class="math notranslate nohighlight">\([0.05, 0.8]\)</span> respectively,
it means that the bounding box’s width is 5% of the image width and height is 80% of the image height.
We can scale it back since absolute numbers are easier to visualize.</p>
<p>Given an image of size <span class="math notranslate nohighlight">\(100 \times 100\)</span>, the bounding box’s width and height unnormalized are <span class="math notranslate nohighlight">\(5\)</span> and <span class="math notranslate nohighlight">\(80\)</span> respectively.
Then let’s say the model predicts the bounding box’s width and height to be <span class="math notranslate nohighlight">\([0.2, 0.95]\)</span>.
The mean squared error is <span class="math notranslate nohighlight">\((0.2 - 0.05)^2 + (0.95 - 0.8)^2 = 0.0225 + 0.0225 = 0.045\)</span>. We see that
both errors are penalized equally. But if you scale the predicted bounding box’s width and height back
to the original image size, you will get <span class="math notranslate nohighlight">\(20\)</span> and <span class="math notranslate nohighlight">\(95\)</span> respectively, then the relative error is
much worse for the width than the height (i.e both deviates 15 pixels but the width deviates much more
percentage wise).</p>
<p>Consequently, the square root mapping is used to penalize errors in small bounding boxes more than
the errors in big bounding boxes. If we use square root mapping, our original width and height
becomes <span class="math notranslate nohighlight">\([0.22, 0.89]\)</span> and the predicted width and height becomes <span class="math notranslate nohighlight">\([0.45, 0.97]\)</span>. The mean squared error
is then <span class="math notranslate nohighlight">\((0.45 - 0.22)^2 + (0.97 - 0.89)^2 = 0.0529 + 0.0064 = 0.0593\)</span>. We see that the error in the
width is penalized more than the error in the height. This helps the model to learn better by
assigning more importance to small bounding boxes errors.</p>
</div>
<div class="proof definition admonition" id="param-bbox">
<p class="admonition-title"><span class="caption-number">Definition 2 </span> (Parametrized Bounding Box)</p>
<section class="definition-content" id="proof-content">
<p>The parametrized bounding box is a 4-tuple vector consisting of the coordinates of
bounding box in the following order:</p>
<div class="math notranslate nohighlight" id="equation-param-bbox">
<span class="eqno">(2)<a class="headerlink" href="#equation-param-bbox" title="Permalink to this equation">#</a></span>\[
\b = \begin{bmatrix} f(\xx_c, \gx) &amp; f(\yy_c, \gy) &amp; \sqrt{\ww_n} &amp; \sqrt{\hh_n} \end{bmatrix} \in \R^{1 \times 4}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\gx = \lfloor S \cdot \xx_c \rfloor\)</span> is the grid cell column (row) index;</p></li>
<li><p><span class="math notranslate nohighlight">\(\gy = \lfloor S \cdot \yy_c \rfloor\)</span> is the grid cell row (column) index;</p></li>
<li><p><span class="math notranslate nohighlight">\(f(\xx_c, \gx) = S \cdot \xx_c - \gx\)</span> and;</p></li>
<li><p><span class="math notranslate nohighlight">\(f(\yy_c, \gy) = S \cdot \yy_c - \gy\)</span></p></li>
</ul>
<p>Take note that during construction, the square root is omitted because it is included
in the loss function later. You will see in our code later that our <span class="math notranslate nohighlight">\(\b\)</span> is actually</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\b &amp;= \begin{bmatrix} f(\xx_c, \gx) &amp; f(\yy_c, \gy) &amp; \ww_n &amp; \hh_n \end{bmatrix} \\
   &amp;= \begin{bmatrix} \xx &amp; \yy &amp; \ww &amp; \hh \end{bmatrix}
\end{align}
\end{split}\]</div>
<p>We will be using the notation <span class="math notranslate nohighlight">\([\xx, \yy, \ww, \hh]\)</span> in the rest of the sections.</p>
<p>As a side note, it is often the case that a single image has multiple bounding boxes. Therefore,
you will need to convert all of them to the parametrized form.</p>
</section>
</div><div class="proof example admonition" id="param-bbox-example">
<p class="admonition-title"><span class="caption-number">Example 1 </span> (Example of Parametrization)</p>
<section class="example-content" id="proof-content">
<p>Consider the <strong>TODO insert image</strong> image. The bounding box is in the YOLO format at first.</p>
<div class="math notranslate nohighlight">
\[
\byolo = \begin{bmatrix} 11 &amp; 0.3442 &amp; 0.611 &amp; 0.4164 &amp; 0.262
         \end{bmatrix}   
\]</div>
<p>Then since <span class="math notranslate nohighlight">\(S = 7\)</span>, we can recover <span class="math notranslate nohighlight">\(f(\xx_c, \gx)\)</span> and <span class="math notranslate nohighlight">\(f(\yy_c, \gy)\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\gx &amp;= \lfloor 7 \cdot 0.3442  \rfloor &amp;= 2 \\
\gy &amp;= \lfloor 7 \cdot 0.611   \rfloor &amp;= 4 \\
f(\xx_c, \gx) &amp;= 7 \cdot 0.3442 - 2 &amp;= 0.4093 \\
f(\yy_c, \gy) &amp;= 7 \cdot 0.611  - 4 &amp;= 0.2770 \\
\end{aligned}
\end{split}\]</div>
<p>Visually, the bounding box of the dog actually lies in the 3rd column and 5th row <span class="math notranslate nohighlight">\((3, 5)\)</span> of the grid.
But we compute it as if it lies in the 2nd column and 4th row <span class="math notranslate nohighlight">\((2, 4)\)</span> of the grid because in python
the index starts from 0 and the top-left corner of the image is considered grid cell <span class="math notranslate nohighlight">\((0, 0)\)</span>.</p>
<p>Then the parametrized bounding box is:</p>
<div class="math notranslate nohighlight">
\[
\b = \begin{bmatrix} 0.4093 &amp; 0.2770 &amp; \sqrt{0.4164} &amp; \sqrt{0.262} \end{bmatrix} \in \R^{1 \times 4}
\]</div>
</section>
</div><p>For more details, have a read at <a class="reference external" href="https://www.harrysprojects.com/articles/fastrcnn.html">this article</a>
to understand the parametrization.</p>
</section>
<section id="loss-function">
<h3>Loss Function<a class="headerlink" href="#loss-function" title="Permalink to this headline">#</a></h3>
<p>See below section.</p>
</section>
<section id="other-important-notations">
<span id="yolov1-md-other-important-notations"></span><h3>Other Important Notations<a class="headerlink" href="#other-important-notations" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="s-b-c">
<p class="admonition-title"><span class="caption-number">Definition 3 </span> (S, B and C)</p>
<section class="definition-content" id="proof-content">
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S\)</span>: We divide an image into an <span class="math notranslate nohighlight">\(S \times S\)</span> grid, so <span class="math notranslate nohighlight">\(S\)</span> is the <strong>grid size</strong>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\gx\)</span> denotes <span class="math notranslate nohighlight">\(x\)</span>-coordinate grid cell and <span class="math notranslate nohighlight">\(\gy\)</span> denotes the <span class="math notranslate nohighlight">\(y\)</span>-coordinate grid cell and so the first grid cell can be denoted <span class="math notranslate nohighlight">\((\gx, \gy) = (0, 0)\)</span> or <span class="math notranslate nohighlight">\((1, 1)\)</span> if using python;</p></li>
<li><p><span class="math notranslate nohighlight">\(B\)</span>: In each grid cell <span class="math notranslate nohighlight">\((\gx, \gy)\)</span>, we can predict <span class="math notranslate nohighlight">\(B\)</span> number of bounding boxes;</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span>: This is the number of classes;</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(\cc \in \{1, 2, \ldots, 20\}\)</span> be a <strong>scalar</strong>, which is the class index (id) where</p>
<ul>
<li><p>20 is the number of classes;</p></li>
<li><p>in Pascal VOC: <code class="docutils literal notranslate"><span class="pre">[aeroplane,</span> <span class="pre">bicycle,</span> <span class="pre">bird,</span> <span class="pre">boat,</span> <span class="pre">bottle,</span> <span class="pre">bus,</span> <span class="pre">car,</span> <span class="pre">cat,</span> <span class="pre">chair,</span> <span class="pre">cow,</span> <span class="pre">diningtable,</span> <span class="pre">dog,</span> <span class="pre">horse,</span> <span class="pre">motorbike,</span> <span class="pre">person,</span> <span class="pre">pottedplant,</span> <span class="pre">sheep,</span> <span class="pre">sofa,</span> <span class="pre">train,</span> <span class="pre">tvmonitor]</span></code></p></li>
<li><p>So if the object is class bicycle, then <span class="math notranslate nohighlight">\(\cc = 2\)</span>;</p></li>
<li><p>Note in python notation, <span class="math notranslate nohighlight">\(\cc\)</span> starts from <span class="math notranslate nohighlight">\(0\)</span> and ends at <span class="math notranslate nohighlight">\(19\)</span> so need to shift accordingly.</p></li>
</ul>
</li>
</ul>
</section>
</div><div class="proof definition admonition" id="prob-object">
<p class="admonition-title"><span class="caption-number">Definition 4 </span> (Probability Object)</p>
<section class="definition-content" id="proof-content">
<p>The author defines <span class="math notranslate nohighlight">\(\Pobj\)</span> to be the probability that an object is present in a grid cell.
This is constructed <strong>deterministically</strong> to be either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>To make the notation more compact, we will add a subscript <span class="math notranslate nohighlight">\(i\)</span> to denote the grid cell.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Pobj_i = 
\begin{cases}
    1     &amp; \textbf{if grid cell } i \textbf{ has an object}\\
    0     &amp; \textbf{otherwise}
\end{cases}
\end{split}\]</div>
<p>By definition, if a ground truth bounding box’s center coordinates <span class="math notranslate nohighlight">\((\xx_c, \yy_c)\)</span>
falls in grid cell <span class="math notranslate nohighlight">\(i\)</span>, then <span class="math notranslate nohighlight">\(\Pobj_i = 1\)</span> for that grid cell.</p>
</section>
</div><div class="proof definition admonition" id="gt-confidence">
<p class="admonition-title"><span class="caption-number">Definition 5 </span> (Ground Truth Confidence Score)</p>
<section class="definition-content" id="proof-content">
<p>The author defines the confidence score of the ground truth matrix
to be</p>
<div class="math notranslate nohighlight">
\[
\conf_i = \Pobj_i \times \iou
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\iou = \underset{\bhat_i \in \{\bhat_i^1, \bhat_i^2\}}{\max}\textbf{IOU}(\b_i, \bhat_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(\bhat_i^1\)</span> and <span class="math notranslate nohighlight">\(\bhat_i^2\)</span> are the two bounding boxes that are predicted by the model.</p>
<p>It is worth noting to the readers that <span class="math notranslate nohighlight">\(\conf_i\)</span> is also an indicator function, since
<span class="math notranslate nohighlight">\(\Pobj_i\)</span> from <a class="reference internal" href="#prob-object">Definition 4</a> is an indicator function.</p>
<p>More concretely,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\conf_i =
\begin{cases}
    \textbf{IOU}(\b_i, \bhat_i)     &amp; \textbf{if grid cell } i \textbf{ has an object}\\
    0                               &amp; \textbf{otherwise}
\end{cases}
\end{split}\]</div>
<p>since <span class="math notranslate nohighlight">\(\Pobj_i = 1\)</span> if the grid cell has an object and <span class="math notranslate nohighlight">\(\Pobj_i = 0\)</span> otherwise.</p>
<p>Therefore, the author is using the IOU as a proxy for the confidence score in the ground truth matrix.</p>
</section>
</div></section>
</section>
<section id="from-3d-tensor-to-2d-matrix">
<h2>From 3D Tensor to 2D Matrix<a class="headerlink" href="#from-3d-tensor-to-2d-matrix" title="Permalink to this headline">#</a></h2>
<p>We will now discuss how to convert the 3D tensor output of the YOLOv1 model to a 2D matrix.</p>
<figure class="align-default" id="dto2d">
<img alt="_images/3dto2d.jpg" src="_images/3dto2d.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">Convert 3D tensor to 2D matrix</span><a class="headerlink" href="#dto2d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Recall that the output of the YOLOv1 model is a 3D tensor of shape <span class="math notranslate nohighlight">\((7, 7, 30)\)</span> for a single image
(not including batch size). Visually, <a class="reference internal" href="#image-1-grids"><span class="std std-numref">Fig. 3</span></a> shows the <span class="math notranslate nohighlight">\(7\)</span> by <span class="math notranslate nohighlight">\(7\)</span> grid overlayed
on the image, each grid will have a depth of <span class="math notranslate nohighlight">\(30\)</span>. However, when computing the loss function,
I took the liberty to squash the <span class="math notranslate nohighlight">\(7 \times 7\)</span> grid into a single dimension, so instead of a cuboid,
we now have a 2d rectangular matrix of shape <span class="math notranslate nohighlight">\(49 \times 30\)</span>.</p>
</section>
<section id="construction-of-ground-truth-matrix">
<h2>Construction of Ground Truth Matrix<a class="headerlink" href="#construction-of-ground-truth-matrix" title="Permalink to this headline">#</a></h2>
<div class="admonition-abuse-of-notation admonition">
<p class="admonition-title">Abuse of Notation</p>
<p>When I say grid cell <span class="math notranslate nohighlight">\(i\)</span>, it also means the <span class="math notranslate nohighlight">\(i\)</span>-th row of the ground truth and prediction matrix.</p>
</div>
<p>The below shows an image alongside its bounding boxes, in YOLO format as per <a class="reference internal" href="#yolo-bbox">Definition 1</a>.</p>
<figure class="align-default" id="image-1-and-label">
<img alt="_images/image_1_and_bbox_and_labels.jpg" src="_images/image_1_and_bbox_and_labels.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">Image 1 and its yolo format label.</span><a class="headerlink" href="#image-1-and-label" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Here we see this image has 2 bounding boxes, and each bounding box has 5 values, which corresponds
to the 5 values in the YOLO format.</p>
<p>Our goal here is to convert the YOLO style labels into a <span class="math notranslate nohighlight">\(49 \times 30\)</span> matrix
(equivalent to a 3D tensor of shape <span class="math notranslate nohighlight">\(7 \times 7 \times 30\)</span>).</p>
<p>Recall that in section <a class="reference internal" href="#yolov1-md-bounding-box-parametrization"><span class="std std-ref">bounding box parametrization</span></a>, we
mentioned that YOLOv1 predicts the offset for its bounding box center, and the square root
of width and height. And recall that the ground truth bounding box’s center determines
which grid cell it belongs to, this is particularly important to remember.</p>
<p>More formally, we denote the subscript <span class="math notranslate nohighlight">\(i\)</span> to be the <span class="math notranslate nohighlight">\(i\)</span>-th grid cell where
<span class="math notranslate nohighlight">\(i \in \{1, 2, \ldots 49\}\)</span> as seen in <a class="reference internal" href="#dto2d"><span class="std std-numref">Fig. 6</span></a>.</p>
<p>We will assume <span class="math notranslate nohighlight">\(S=7\)</span>, <span class="math notranslate nohighlight">\(B=2\)</span>, and <span class="math notranslate nohighlight">\(C=20\)</span>, where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S\)</span> is the grid size;</p></li>
<li><p><span class="math notranslate nohighlight">\(B\)</span> is the number of bounding boxes to be predicted;</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span> is the number of classes.</p></li>
</ul>
<p>We will also assume that our batch size is <span class="math notranslate nohighlight">\(1\)</span>, and hence we are only looking
at one single image. This simplifies the explanation. Just note that if we have
a batch size of <span class="math notranslate nohighlight">\(N\)</span>, then we will have <span class="math notranslate nohighlight">\(N\)</span> ground truth matrices.</p>
<p>Consequently, each row of the ground truth matrix will have <span class="math notranslate nohighlight">\(2B + C = 30\)</span> elements.</p>
<p>Remember that each row <span class="math notranslate nohighlight">\(i\)</span> of the ground truth matrix corresponds to the grid cell <span class="math notranslate nohighlight">\(i\)</span>
as seen in figure <a class="reference internal" href="#dto2d"><span class="std std-numref">Fig. 6</span></a>.</p>
<div class="proof definition admonition" id="yolo_gt_matrix">
<p class="admonition-title"><span class="caption-number">Definition 6 </span> (YOLOv1 Ground Truth Matrix)</p>
<section class="definition-content" id="proof-content">
<p>Define <span class="math notranslate nohighlight">\(\y_i \in \R^{1 \times 30}\)</span> to be the <span class="math notranslate nohighlight">\(i\)</span>-th row of the
ground truth matrix <span class="math notranslate nohighlight">\(\y \in \R^{49 \times 30}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-gt-yi">
<span class="eqno">(3)<a class="headerlink" href="#equation-eq-gt-yi" title="Permalink to this equation">#</a></span>\[
\y_i
= \begin{bmatrix}
\b_i &amp; \conf_i &amp; \b_i &amp; \conf_i &amp; \p_i 
\end{bmatrix} \in \R^{1 \times 30}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\b_i = \begin{bmatrix}\xx_i &amp; \yy_i &amp; \ww_i &amp; \hh_i \end{bmatrix} \in \R^{1 \times 4}\)</span>
as per <a class="reference internal" href="#param-bbox">Definition 2</a>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\conf_i = \Pobj_i \cdot \iou \in \R\)</span> as per <a class="reference internal" href="#gt-confidence">Definition 5</a>, note very carefully
how <span class="math notranslate nohighlight">\(\conf_i\)</span> is defined if the grid cell has an object, and how it is <span class="math notranslate nohighlight">\(0\)</span> if there are no objects in
that grid cell <span class="math notranslate nohighlight">\(i\)</span>.</p>
<ul>
<li><p>We will keep the formal definition off the tables for now and set <span class="math notranslate nohighlight">\(\conf_i\)</span> to be equals to <span class="math notranslate nohighlight">\(\Pobj_i\)</span>
such that <span class="math notranslate nohighlight">\(\conf = 1\)</span> if <span class="math notranslate nohighlight">\(\Pobj_i = 1\)</span> and <span class="math notranslate nohighlight">\(0\)</span> if <span class="math notranslate nohighlight">\(\Pobj_i = 0\)</span>.</p></li>
<li><p>The reason is non-trivial because we have no way of knowing the IOU of the ground truth bounding
box with the predicted bounding box before training. You can think of it as a proxy for the
calculation later during the loss function construction.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\p_i = \begin{bmatrix}0 &amp; 0 &amp; 1 &amp; \cdots &amp;0\end{bmatrix} \in \R^{1 \times 20}\)</span> where we use the
class id <span class="math notranslate nohighlight">\(\cc\)</span> to construct our class probability ground truth vector such that <span class="math notranslate nohighlight">\(\p\)</span> is everywhere
<span class="math notranslate nohighlight">\(0\)</span> encoded except at the <span class="math notranslate nohighlight">\(\cc\)</span>-th index (one hot encoding). In the paper, <span class="math notranslate nohighlight">\(\p_i\)</span> is defined as
<span class="math notranslate nohighlight">\(\P(\text{Class}_i \mid \text{Obj})\)</span> which means that <span class="math notranslate nohighlight">\(\p_i\)</span> is conditioned on the grid cell given
there exists an object, which means for grid cells <span class="math notranslate nohighlight">\(i\)</span> without any objects, <span class="math notranslate nohighlight">\(\p_i\)</span> is a zero vector.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\y_i\)</span> will be initiated as a zero vector, and will remain a zero vector if there are no objects in
grid cell <span class="math notranslate nohighlight">\(i\)</span>. Otherwise, we will update the elements of <span class="math notranslate nohighlight">\(\y_i\)</span> as per the above definitions.</p>
<p>Then the ground truth matrix <span class="math notranslate nohighlight">\(\y\)</span> is constructed as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\y = \begin{bmatrix}
\y_1 \\
\y_2 \\
\vdots \\
\y_{49}
\end{bmatrix} \in \R^{49 \times 30}
\end{split}\]</div>
<p>Note that this is often reshaped to be <span class="math notranslate nohighlight">\(\y \in \R^{7 \times 7 \times 30}\)</span> in many implementations.</p>
</section>
</div><div class="proof remark admonition" id="yolo_gt_matrix_remark">
<p class="admonition-title"><span class="caption-number">Remark 1 </span> (Remark: Ground Truth Matrix Construction)</p>
<section class="remark-content" id="proof-content">
<p><strong>TODO insert encode here to show why the 1st 5 and next 5 elements are the same</strong></p>
<p>It is also worth noting to everyone that we set the first 5 elements and the next 5 elements the same,
therefore, we don’t make a conscious effort to differentiate between <span class="math notranslate nohighlight">\(\b_i\)</span>,
as we will see later in the prediction matrix.
This is because we only have one set of ground truth and our choice of encoding is simply to repeat
the ground truth coordinates twice in the first 10 elements.</p>
<p>The next thing to note is that what if the same image has 2 bounding boxes having the same center coordinates?
Then <strong>by design</strong>, one of them will be dropped by this construction, this kind of “flawed design”
will be fixed in future yolo iterations.</p>
<p>One can read how it is implemented in python under the <code class="docutils literal notranslate"><span class="pre">encode</span></code> function. The logic should follow through.</p>
<p>One more note is for example the dog/human image, there are two bounding boxes in that image, and one can see their center lie in different grid cells, which means the final <span class="math notranslate nohighlight">\(7 \times 7 \times 30\)</span> matrix will have grid cell <span class="math notranslate nohighlight">\((3, 5)\)</span> and <span class="math notranslate nohighlight">\((4, 4)\)</span> filled with values of these two bounding boxes and rest are initiated with zeros since there does not exist any objects in the other grid cells. If you are using <span class="math notranslate nohighlight">\(49 \times 30\)</span> method, then they instead like in grid cell <span class="math notranslate nohighlight">\(3 \times 7 + 5 = 26\)</span> grid cell and <span class="math notranslate nohighlight">\(4 \times 7 + 4 = 32\)</span> grid cell (note it is not just 3 x 5 or 4 x 4 !)</p>
<p>Lastly, the idea of having 2 bounding boxes in the encoding construction will be more apparent in the next section.</p>
</section>
</div></section>
<section id="construction-of-prediction-matrix">
<h2>Construction of Prediction Matrix<a class="headerlink" href="#construction-of-prediction-matrix" title="Permalink to this headline">#</a></h2>
<div class="admonition-abuse-of-notation admonition">
<p class="admonition-title">Abuse of Notation</p>
<p>When I say grid cell <span class="math notranslate nohighlight">\(i\)</span>, it also means the <span class="math notranslate nohighlight">\(i\)</span>-th row of the ground truth and prediction matrix.</p>
</div>
<p>The construction of the prediction matrix <span class="math notranslate nohighlight">\(\hat{\y}\)</span> follows the last layer of the neural network,
shown earlier in diagram <a class="reference internal" href="#yolov1-model"><span class="std std-ref">YoloV1 Model Architecture</span></a>.</p>
<p>To stay consistent with the shape defined in <a class="reference internal" href="#yolo_gt_matrix">Definition 6</a>, we will reshape
the last layer from <span class="math notranslate nohighlight">\(7 \times 7 \times 30\)</span> to <span class="math notranslate nohighlight">\(49 \times 30\)</span>. As mentioned in the section on
<a class="reference internal" href="#yolov1-md-head"><span class="std std-ref">model’s head</span></a> the last layer is not really a 3d-tensor,
it is in fact a linear/dense layer of shape <span class="math notranslate nohighlight">\([-1, 1470]\)</span>.
The <span class="math notranslate nohighlight">\(1470\)</span> neurons were reshaped to be <span class="math notranslate nohighlight">\(7 \times 7 \times 30\)</span> so that readers like us can
interpret it better with the injection of grid cell idea.</p>
<div class="proof definition admonition" id="yolo_pred_matrix">
<p class="admonition-title"><span class="caption-number">Definition 7 </span> (YOLOv1 Prediction Matrix)</p>
<section class="definition-content" id="proof-content">
<p>Define <span class="math notranslate nohighlight">\(\hat{\y}_i \in \R^{1 \times 30}\)</span> to be the <span class="math notranslate nohighlight">\(i\)</span>-th row of the prediction matrix
<span class="math notranslate nohighlight">\(\hat{\y} \in \R^{49 \times 30}\)</span>, output from the last layer of the neural network.</p>
<div class="math notranslate nohighlight" id="equation-eq-gt-yhati">
<span class="eqno">(4)<a class="headerlink" href="#equation-eq-gt-yhati" title="Permalink to this equation">#</a></span>\[
\yhat_i
= \begin{bmatrix}
\bhat_i^1 &amp; \confhat_i^1 &amp; \bhat_i^2 &amp; \confhat_i^2 &amp; \phat_i 
\end{bmatrix} \in \R^{1 \times 30}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bhat_i^1 = \begin{bmatrix}\xxhat_i^1 &amp; \yyhat_i^1 &amp; \wwhat_i^1 &amp; \hhhat_i^1 \end{bmatrix} 
\in \R^{1 \times 4}\)</span> is the predictions of the 4 coordinates made by bounding box 1;</p></li>
<li><p><span class="math notranslate nohighlight">\(\bhat_i^2\)</span> is then the predictions of the 4 coordinates made by bounding box 2;</p></li>
<li><p><span class="math notranslate nohighlight">\(\confhat_i^1 \in \R\)</span> is the object/bounding box confidence score (a scalar) of the first bounding
box made by the model. As a reminder, this value will be compared during loss function with the
<span class="math notranslate nohighlight">\(\conf\)</span> constructed in the ground truth;</p></li>
<li><p><span class="math notranslate nohighlight">\(\confhat_i^2 \in \R\)</span> is the object/bounding box confidence score of the second bounding box made by the model;</p></li>
<li><p><span class="math notranslate nohighlight">\(\phat_i \in \R^{1 \times 20}\)</span> where the model predicts a class probability vector indicating
which class is the most likely. By construction of loss function, this probability vector does not
sum to 1 since the author uses MSELoss to penalize, this is slightly counter intuitive as
cross-entropy loss does a better job at forcing classification loss - this is remedied in later yolo versions!</p>
<ul>
<li><p>Notice that there is no superscript for <span class="math notranslate nohighlight">\(\phat_i\)</span>, that is because the model only predicts one set of class probabilities for each grid cell <span class="math notranslate nohighlight">\(i\)</span>, even though you can predict <span class="math notranslate nohighlight">\(B\)</span> number of bounding boxes.</p></li>
</ul>
</li>
</ul>
<p>Consequently, the final form of the prediction matrix <span class="math notranslate nohighlight">\(\yhat\)</span> can be denoted as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\yhat =
\begin{bmatrix}
\yhat_1 \\
\yhat_2 \\
\vdots \\
\yhat_{49}
\end{bmatrix} \in \R^{49 \times 30}
\end{split}\]</div>
<p>and of course they must be the same shape as <span class="math notranslate nohighlight">\(\y\)</span>.</p>
</section>
</div><div class="warning admonition">
<p class="admonition-title">Some Remarks</p>
<ol class="arabic simple">
<li><p>Note that in our <code class="docutils literal notranslate"><span class="pre">head</span></code> layer, we did not choose to add <code class="docutils literal notranslate"><span class="pre">nn.Sigmoid()</span></code> after the last layer. This
will cause the output of the last layer to be in the range of <span class="math notranslate nohighlight">\([-\infty, \infty]\)</span>, which means
it is unbounded. Therefore, non-negative values like the width and height <code class="docutils literal notranslate"><span class="pre">what_i</span></code> and <code class="docutils literal notranslate"><span class="pre">hhat_i</span></code>
can be negative!</p></li>
<li><p>Each grid cell predicts two bound boxes, it will shift and stretch the prior box in two different ways,
possibly to cover two different objects (but both are constrained to have the same class).
You might wonder why it’s trying to do two boxes. The answer is probably because 49 boxes isn’t
enough, especially when there are lots of objects close together, although what tends to happen
during training is that the predicted boxes become specialised. So one box might learn to find
big things, the other might learn to find small things, this may help the network
generalise to other domains<a class="footnote-reference brackets" href="#id17" id="id5">1</a>.</p></li>
</ol>
</div>
</section>
<section id="id6">
<h2>Loss Function<a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h2>
<p>Possibly the most important part of the YOLOv1 paper is the loss function, it is also
the most confusing if you are not familiar with the notation.</p>
<p>We will use the first batch of images to illustrate the loss function, the batch size is 4.</p>
<figure class="align-default" id="first-batch">
<img alt="_images/batch_image.png" src="_images/batch_image.png" />
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">The first 4 images.</span><a class="headerlink" href="#first-batch" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We will encode the 4 ground truth images in the first batch into the ground truth matrix <span class="math notranslate nohighlight">\(\y\)</span>.
Subsequently, we will pass the first batch of images to our model defined in the model section
and obtain the prediction matrix <span class="math notranslate nohighlight">\(\hat{\y}\)</span>.</p>
<div class="admonition-abuse-of-notation admonition">
<p class="admonition-title">Abuse of Notation</p>
<p>In <a class="reference internal" href="#yolo_gt_matrix">Definition 6</a> and <a class="reference internal" href="#yolo_pred_matrix">Definition 7</a>, <span class="math notranslate nohighlight">\(\y\)</span> and <span class="math notranslate nohighlight">\(\hat{\y}\)</span> are 2 dimensional matrices
with shape <span class="math notranslate nohighlight">\(49 \times 30\)</span>. Here, we are loading 4 images, so the shape of <span class="math notranslate nohighlight">\(\y\)</span> and <span class="math notranslate nohighlight">\(\hat{\y}\)</span> will be
<span class="math notranslate nohighlight">\(4 \times 49 \times 30\)</span>.</p>
</div>
<p>I directly saved the ground truth matrix <span class="math notranslate nohighlight">\(\y\)</span> and prediction matrix <span class="math notranslate nohighlight">\(\yhat\)</span> as <code class="docutils literal notranslate"><span class="pre">y_trues.pt</span></code> and <code class="docutils literal notranslate"><span class="pre">y_preds.pt</span></code>
respectively, you can load them with <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> and they are in the shape of <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">7,</span> <span class="pre">7,</span> <span class="pre">30]</span></code>.
This means that there are 4 images in the batch, each image has a ground truth matrix of shape
<code class="docutils literal notranslate"><span class="pre">[7,</span> <span class="pre">7,</span> <span class="pre">30]</span></code> and a prediction matrix of shape <code class="docutils literal notranslate"><span class="pre">[7,</span> <span class="pre">7,</span> <span class="pre">30]</span></code>. The 4 images are the first 4 images in our
first batch of the train loader, as illustrated in <a class="reference internal" href="#first-batch"><span class="std std-numref">Fig. 8</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1"># load directly the first batch of the train loader</span>
<span class="linenos">2</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="linenos">3</span><span class="n">y_trues</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;./assets/y_trues.pt&quot;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">4</span><span class="n">y_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;./assets/y_preds.pt&quot;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">5</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y_trues.shape: </span><span class="si">{</span><span class="n">y_trues</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">6</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y_preds.shape: </span><span class="si">{</span><span class="n">y_preds</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y_trues.shape: torch.Size([4, 7, 7, 30])
y_preds.shape: torch.Size([4, 7, 7, 30])
</pre></div>
</div>
</div>
</div>
<p>To be <strong>consistent</strong> with our notation and definition in <a class="reference internal" href="#yolo_gt_matrix">Definition 6</a> and <a class="reference internal" href="#yolo_pred_matrix">Definition 7</a>,
we will only use the first image in the batch, <code class="docutils literal notranslate"><span class="pre">y_true</span> <span class="pre">=</span> <span class="pre">y_trues[0]</span></code> and <code class="docutils literal notranslate"><span class="pre">y_pred</span> <span class="pre">=</span> <span class="pre">y_preds[0]</span></code>
and reshape them to be <code class="docutils literal notranslate"><span class="pre">[49,</span> <span class="pre">30]</span></code> and <code class="docutils literal notranslate"><span class="pre">[49,</span> <span class="pre">30]</span></code> respectively. Thus, <code class="docutils literal notranslate"><span class="pre">y_true</span></code>
corresponds to the ground truth matrix
<span class="math notranslate nohighlight">\(\y\)</span> and <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> corresponds to the prediction matrix <span class="math notranslate nohighlight">\(\hat{\y}\)</span>.</p>
<p>Both of these matrix are reshaped to <span class="math notranslate nohighlight">\(49 \times 30\)</span> and visualized as a pandas dataframe:</p>
<div class="cell tag_remove-input docutils container">
</div>
<div class="cell tag_output_scroll tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>x_i^1</th>
      <th>y_i^1</th>
      <th>w_i^1</th>
      <th>h_i^1</th>
      <th>conf_i^1</th>
      <th>x_i^2</th>
      <th>y_i^2</th>
      <th>w_i^2</th>
      <th>h_i^2</th>
      <th>conf_i^2</th>
      <th>p_1</th>
      <th>p_2</th>
      <th>p_3</th>
      <th>p_4</th>
      <th>p_5</th>
      <th>p_6</th>
      <th>p_7</th>
      <th>p_8</th>
      <th>p_9</th>
      <th>p_10</th>
      <th>p_11</th>
      <th>p_12</th>
      <th>p_13</th>
      <th>p_14</th>
      <th>p_15</th>
      <th>p_16</th>
      <th>p_17</th>
      <th>p_18</th>
      <th>p_19</th>
      <th>p_20</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.569</td>
      <td>0.570</td>
      <td>0.975</td>
      <td>0.972</td>
      <td>1</td>
      <td>0.569</td>
      <td>0.570</td>
      <td>0.975</td>
      <td>0.972</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.409</td>
      <td>0.277</td>
      <td>0.416</td>
      <td>0.262</td>
      <td>1</td>
      <td>0.409</td>
      <td>0.277</td>
      <td>0.416</td>
      <td>0.262</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>47</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell tag_output_scroll tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>xhat_i^1</th>
      <th>yhat_i^1</th>
      <th>what_i^1</th>
      <th>hhat_i^1</th>
      <th>confhat_i^1</th>
      <th>xhat_i^2</th>
      <th>yhat_i^2</th>
      <th>what_i^2</th>
      <th>hhat_i^2</th>
      <th>confhat_i^2</th>
      <th>phat_1</th>
      <th>phat_2</th>
      <th>phat_3</th>
      <th>phat_4</th>
      <th>phat_5</th>
      <th>phat_6</th>
      <th>phat_7</th>
      <th>phat_8</th>
      <th>phat_9</th>
      <th>phat_10</th>
      <th>phat_11</th>
      <th>phat_12</th>
      <th>phat_13</th>
      <th>phat_14</th>
      <th>phat_15</th>
      <th>phat_16</th>
      <th>phat_17</th>
      <th>phat_18</th>
      <th>phat_19</th>
      <th>phat_20</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.925</td>
      <td>-0.167</td>
      <td>0.191</td>
      <td>0.723</td>
      <td>2.136e-01</td>
      <td>1.001</td>
      <td>0.123</td>
      <td>0.058</td>
      <td>0.562</td>
      <td>-0.119</td>
      <td>-0.586</td>
      <td>-0.023</td>
      <td>-0.102</td>
      <td>-0.160</td>
      <td>-0.408</td>
      <td>0.564</td>
      <td>-0.163</td>
      <td>0.294</td>
      <td>-0.206</td>
      <td>-0.684</td>
      <td>-0.215</td>
      <td>0.340</td>
      <td>0.425</td>
      <td>0.139</td>
      <td>-0.416</td>
      <td>-0.233</td>
      <td>0.282</td>
      <td>0.517</td>
      <td>0.394</td>
      <td>0.067</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.333</td>
      <td>0.190</td>
      <td>-0.084</td>
      <td>0.428</td>
      <td>-2.902e-01</td>
      <td>-0.190</td>
      <td>0.359</td>
      <td>0.036</td>
      <td>-0.130</td>
      <td>-0.684</td>
      <td>0.415</td>
      <td>-0.072</td>
      <td>-0.182</td>
      <td>0.325</td>
      <td>0.034</td>
      <td>-0.096</td>
      <td>-0.258</td>
      <td>-0.180</td>
      <td>-0.489</td>
      <td>-0.192</td>
      <td>0.040</td>
      <td>0.550</td>
      <td>0.492</td>
      <td>0.279</td>
      <td>-0.157</td>
      <td>-0.329</td>
      <td>-0.106</td>
      <td>0.201</td>
      <td>-0.394</td>
      <td>-0.536</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.357</td>
      <td>0.445</td>
      <td>0.691</td>
      <td>-0.301</td>
      <td>-3.701e-01</td>
      <td>0.430</td>
      <td>0.238</td>
      <td>0.456</td>
      <td>-0.005</td>
      <td>0.113</td>
      <td>0.245</td>
      <td>0.247</td>
      <td>0.007</td>
      <td>-0.348</td>
      <td>0.301</td>
      <td>-0.747</td>
      <td>-0.056</td>
      <td>-0.194</td>
      <td>0.124</td>
      <td>-0.437</td>
      <td>0.172</td>
      <td>0.239</td>
      <td>-0.426</td>
      <td>0.208</td>
      <td>0.255</td>
      <td>0.335</td>
      <td>-0.152</td>
      <td>-0.526</td>
      <td>-0.571</td>
      <td>0.732</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.120</td>
      <td>-0.016</td>
      <td>0.732</td>
      <td>-0.233</td>
      <td>-4.154e-02</td>
      <td>0.428</td>
      <td>-0.482</td>
      <td>0.158</td>
      <td>-0.246</td>
      <td>0.008</td>
      <td>0.007</td>
      <td>-0.134</td>
      <td>-0.639</td>
      <td>-0.528</td>
      <td>-0.545</td>
      <td>-0.689</td>
      <td>0.001</td>
      <td>0.037</td>
      <td>0.306</td>
      <td>-0.073</td>
      <td>0.491</td>
      <td>-1.053</td>
      <td>-0.516</td>
      <td>-0.011</td>
      <td>-0.237</td>
      <td>0.480</td>
      <td>-0.823</td>
      <td>0.418</td>
      <td>-0.284</td>
      <td>0.166</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.530</td>
      <td>-0.381</td>
      <td>0.081</td>
      <td>-0.241</td>
      <td>-3.752e-03</td>
      <td>0.144</td>
      <td>0.989</td>
      <td>-0.177</td>
      <td>0.247</td>
      <td>0.254</td>
      <td>0.082</td>
      <td>0.023</td>
      <td>-0.102</td>
      <td>0.011</td>
      <td>0.217</td>
      <td>0.497</td>
      <td>0.224</td>
      <td>0.241</td>
      <td>-0.125</td>
      <td>0.143</td>
      <td>0.318</td>
      <td>0.057</td>
      <td>-0.001</td>
      <td>-0.278</td>
      <td>-0.100</td>
      <td>-0.022</td>
      <td>-0.320</td>
      <td>-0.012</td>
      <td>-0.473</td>
      <td>-0.214</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.367</td>
      <td>0.217</td>
      <td>0.163</td>
      <td>0.474</td>
      <td>3.846e-02</td>
      <td>0.819</td>
      <td>0.911</td>
      <td>-0.328</td>
      <td>-0.047</td>
      <td>-0.169</td>
      <td>0.276</td>
      <td>0.370</td>
      <td>-0.223</td>
      <td>0.452</td>
      <td>-0.117</td>
      <td>-0.101</td>
      <td>0.521</td>
      <td>0.543</td>
      <td>0.128</td>
      <td>0.347</td>
      <td>-0.242</td>
      <td>-0.280</td>
      <td>-0.414</td>
      <td>-0.296</td>
      <td>0.662</td>
      <td>-0.016</td>
      <td>-0.429</td>
      <td>0.006</td>
      <td>-0.033</td>
      <td>0.192</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.056</td>
      <td>-0.261</td>
      <td>-0.041</td>
      <td>-0.109</td>
      <td>-4.398e-01</td>
      <td>0.329</td>
      <td>0.178</td>
      <td>-0.114</td>
      <td>0.438</td>
      <td>-0.139</td>
      <td>-0.394</td>
      <td>0.267</td>
      <td>0.391</td>
      <td>-0.533</td>
      <td>0.146</td>
      <td>0.357</td>
      <td>0.239</td>
      <td>-0.165</td>
      <td>-0.399</td>
      <td>0.138</td>
      <td>0.357</td>
      <td>0.368</td>
      <td>-0.390</td>
      <td>-0.063</td>
      <td>0.536</td>
      <td>0.388</td>
      <td>0.494</td>
      <td>-0.719</td>
      <td>0.554</td>
      <td>-0.684</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.323</td>
      <td>-0.115</td>
      <td>-0.155</td>
      <td>-0.017</td>
      <td>-5.969e-02</td>
      <td>-0.324</td>
      <td>0.163</td>
      <td>0.098</td>
      <td>0.368</td>
      <td>-0.294</td>
      <td>0.195</td>
      <td>0.213</td>
      <td>-0.606</td>
      <td>0.530</td>
      <td>0.527</td>
      <td>0.623</td>
      <td>-0.519</td>
      <td>-0.168</td>
      <td>0.006</td>
      <td>-0.098</td>
      <td>0.550</td>
      <td>-0.658</td>
      <td>0.092</td>
      <td>-0.250</td>
      <td>0.283</td>
      <td>-0.299</td>
      <td>0.028</td>
      <td>0.308</td>
      <td>0.108</td>
      <td>-0.388</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.058</td>
      <td>-0.061</td>
      <td>0.219</td>
      <td>-0.263</td>
      <td>-1.409e-01</td>
      <td>-0.413</td>
      <td>1.218</td>
      <td>0.328</td>
      <td>-0.635</td>
      <td>0.053</td>
      <td>0.021</td>
      <td>0.107</td>
      <td>-0.358</td>
      <td>-0.160</td>
      <td>-0.074</td>
      <td>0.215</td>
      <td>0.065</td>
      <td>0.347</td>
      <td>-0.394</td>
      <td>0.225</td>
      <td>-0.056</td>
      <td>0.299</td>
      <td>0.251</td>
      <td>-0.041</td>
      <td>0.250</td>
      <td>0.526</td>
      <td>-0.049</td>
      <td>-0.105</td>
      <td>-0.047</td>
      <td>0.449</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.203</td>
      <td>0.044</td>
      <td>-0.260</td>
      <td>-0.105</td>
      <td>-1.989e-01</td>
      <td>0.513</td>
      <td>0.673</td>
      <td>-0.081</td>
      <td>-0.436</td>
      <td>0.098</td>
      <td>-0.065</td>
      <td>0.443</td>
      <td>0.228</td>
      <td>-0.377</td>
      <td>-0.392</td>
      <td>0.062</td>
      <td>0.130</td>
      <td>-0.444</td>
      <td>-0.184</td>
      <td>-0.444</td>
      <td>0.390</td>
      <td>-0.070</td>
      <td>0.167</td>
      <td>-0.247</td>
      <td>0.537</td>
      <td>0.262</td>
      <td>0.460</td>
      <td>-0.112</td>
      <td>0.332</td>
      <td>0.022</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.238</td>
      <td>0.411</td>
      <td>-0.264</td>
      <td>-0.063</td>
      <td>-3.446e-01</td>
      <td>-0.468</td>
      <td>-0.379</td>
      <td>-0.382</td>
      <td>0.066</td>
      <td>-0.019</td>
      <td>0.330</td>
      <td>-0.013</td>
      <td>-0.436</td>
      <td>0.677</td>
      <td>-0.064</td>
      <td>-0.539</td>
      <td>0.522</td>
      <td>0.236</td>
      <td>-0.458</td>
      <td>0.315</td>
      <td>0.715</td>
      <td>-0.603</td>
      <td>-0.656</td>
      <td>0.243</td>
      <td>-0.438</td>
      <td>0.360</td>
      <td>-0.120</td>
      <td>-0.199</td>
      <td>0.668</td>
      <td>-0.379</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.088</td>
      <td>0.015</td>
      <td>-0.039</td>
      <td>0.089</td>
      <td>8.807e-04</td>
      <td>0.047</td>
      <td>0.180</td>
      <td>0.015</td>
      <td>0.551</td>
      <td>-0.589</td>
      <td>0.366</td>
      <td>0.779</td>
      <td>0.066</td>
      <td>0.002</td>
      <td>-0.246</td>
      <td>-0.187</td>
      <td>-0.101</td>
      <td>-0.064</td>
      <td>-0.061</td>
      <td>-0.185</td>
      <td>0.833</td>
      <td>-0.086</td>
      <td>0.148</td>
      <td>-0.872</td>
      <td>-0.028</td>
      <td>-0.332</td>
      <td>0.629</td>
      <td>-0.334</td>
      <td>0.074</td>
      <td>-0.261</td>
    </tr>
    <tr>
      <th>12</th>
      <td>-0.696</td>
      <td>0.406</td>
      <td>0.131</td>
      <td>0.442</td>
      <td>2.473e-01</td>
      <td>0.346</td>
      <td>0.212</td>
      <td>0.511</td>
      <td>0.261</td>
      <td>-0.268</td>
      <td>0.843</td>
      <td>0.509</td>
      <td>0.369</td>
      <td>0.157</td>
      <td>0.678</td>
      <td>0.062</td>
      <td>0.338</td>
      <td>0.233</td>
      <td>0.043</td>
      <td>-0.484</td>
      <td>0.741</td>
      <td>0.894</td>
      <td>0.248</td>
      <td>-0.087</td>
      <td>0.459</td>
      <td>0.017</td>
      <td>0.316</td>
      <td>0.255</td>
      <td>-0.019</td>
      <td>0.087</td>
    </tr>
    <tr>
      <th>13</th>
      <td>-0.929</td>
      <td>0.255</td>
      <td>0.358</td>
      <td>0.456</td>
      <td>2.613e-01</td>
      <td>0.114</td>
      <td>-0.744</td>
      <td>0.038</td>
      <td>-0.050</td>
      <td>-0.090</td>
      <td>-0.079</td>
      <td>0.846</td>
      <td>-0.015</td>
      <td>-0.195</td>
      <td>-0.542</td>
      <td>-0.793</td>
      <td>0.091</td>
      <td>0.340</td>
      <td>0.454</td>
      <td>-0.159</td>
      <td>-0.206</td>
      <td>-0.405</td>
      <td>-0.869</td>
      <td>0.209</td>
      <td>-0.464</td>
      <td>-0.476</td>
      <td>-0.498</td>
      <td>0.077</td>
      <td>0.125</td>
      <td>0.794</td>
    </tr>
    <tr>
      <th>14</th>
      <td>-0.256</td>
      <td>-0.506</td>
      <td>0.134</td>
      <td>-0.352</td>
      <td>1.512e-01</td>
      <td>-0.156</td>
      <td>0.459</td>
      <td>0.003</td>
      <td>0.396</td>
      <td>0.045</td>
      <td>0.563</td>
      <td>-0.141</td>
      <td>0.418</td>
      <td>-0.167</td>
      <td>0.270</td>
      <td>-0.238</td>
      <td>0.416</td>
      <td>0.160</td>
      <td>0.144</td>
      <td>0.170</td>
      <td>-0.024</td>
      <td>-0.285</td>
      <td>0.368</td>
      <td>0.608</td>
      <td>0.626</td>
      <td>0.444</td>
      <td>0.312</td>
      <td>-0.195</td>
      <td>0.312</td>
      <td>-0.177</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.627</td>
      <td>-0.240</td>
      <td>-0.348</td>
      <td>-0.289</td>
      <td>-1.770e-01</td>
      <td>0.407</td>
      <td>0.771</td>
      <td>0.890</td>
      <td>0.773</td>
      <td>0.266</td>
      <td>0.277</td>
      <td>-0.003</td>
      <td>0.392</td>
      <td>0.296</td>
      <td>-0.704</td>
      <td>-0.073</td>
      <td>0.464</td>
      <td>-0.375</td>
      <td>-0.123</td>
      <td>0.335</td>
      <td>0.818</td>
      <td>0.108</td>
      <td>0.300</td>
      <td>0.042</td>
      <td>0.247</td>
      <td>-0.042</td>
      <td>0.103</td>
      <td>0.063</td>
      <td>-0.433</td>
      <td>-0.014</td>
    </tr>
    <tr>
      <th>16</th>
      <td>-0.128</td>
      <td>0.017</td>
      <td>-0.486</td>
      <td>-0.198</td>
      <td>1.439e-01</td>
      <td>0.089</td>
      <td>-0.336</td>
      <td>0.091</td>
      <td>0.735</td>
      <td>0.257</td>
      <td>-0.166</td>
      <td>0.179</td>
      <td>-0.143</td>
      <td>-0.146</td>
      <td>-0.147</td>
      <td>-0.181</td>
      <td>0.783</td>
      <td>-0.271</td>
      <td>-0.275</td>
      <td>-0.407</td>
      <td>-0.366</td>
      <td>-0.667</td>
      <td>-0.326</td>
      <td>0.114</td>
      <td>0.101</td>
      <td>-0.164</td>
      <td>-1.082</td>
      <td>0.694</td>
      <td>0.254</td>
      <td>0.340</td>
    </tr>
    <tr>
      <th>17</th>
      <td>-0.156</td>
      <td>-0.408</td>
      <td>-0.223</td>
      <td>-0.094</td>
      <td>1.400e-01</td>
      <td>0.829</td>
      <td>1.379</td>
      <td>-0.280</td>
      <td>-1.207</td>
      <td>0.340</td>
      <td>-0.236</td>
      <td>-0.458</td>
      <td>-0.165</td>
      <td>0.690</td>
      <td>0.432</td>
      <td>-0.300</td>
      <td>0.104</td>
      <td>0.311</td>
      <td>-0.302</td>
      <td>0.112</td>
      <td>0.182</td>
      <td>-0.267</td>
      <td>-0.387</td>
      <td>-0.137</td>
      <td>0.476</td>
      <td>-0.040</td>
      <td>-0.070</td>
      <td>0.232</td>
      <td>-0.208</td>
      <td>0.206</td>
    </tr>
    <tr>
      <th>18</th>
      <td>-0.208</td>
      <td>-0.629</td>
      <td>0.139</td>
      <td>-0.178</td>
      <td>1.545e-01</td>
      <td>0.104</td>
      <td>-0.012</td>
      <td>-0.480</td>
      <td>-0.259</td>
      <td>-0.010</td>
      <td>-0.117</td>
      <td>0.114</td>
      <td>-0.057</td>
      <td>-0.348</td>
      <td>-0.097</td>
      <td>0.329</td>
      <td>0.122</td>
      <td>0.006</td>
      <td>-0.043</td>
      <td>0.091</td>
      <td>-0.165</td>
      <td>0.039</td>
      <td>-0.523</td>
      <td>0.317</td>
      <td>0.061</td>
      <td>0.349</td>
      <td>-0.630</td>
      <td>0.397</td>
      <td>0.087</td>
      <td>0.022</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.516</td>
      <td>-0.315</td>
      <td>0.357</td>
      <td>0.173</td>
      <td>-1.141e-01</td>
      <td>-0.242</td>
      <td>-0.058</td>
      <td>-0.178</td>
      <td>-0.078</td>
      <td>-0.209</td>
      <td>-0.403</td>
      <td>0.590</td>
      <td>-0.189</td>
      <td>-0.262</td>
      <td>0.242</td>
      <td>-0.006</td>
      <td>-0.010</td>
      <td>-0.319</td>
      <td>0.180</td>
      <td>-0.260</td>
      <td>-0.295</td>
      <td>0.196</td>
      <td>-0.749</td>
      <td>0.095</td>
      <td>-0.191</td>
      <td>-0.689</td>
      <td>0.130</td>
      <td>-0.059</td>
      <td>0.621</td>
      <td>0.088</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.075</td>
      <td>0.442</td>
      <td>-0.116</td>
      <td>-0.698</td>
      <td>6.595e-02</td>
      <td>0.181</td>
      <td>0.416</td>
      <td>0.036</td>
      <td>0.198</td>
      <td>-0.068</td>
      <td>-0.370</td>
      <td>-0.099</td>
      <td>-0.246</td>
      <td>-0.526</td>
      <td>0.410</td>
      <td>0.160</td>
      <td>0.308</td>
      <td>-0.390</td>
      <td>0.231</td>
      <td>-0.502</td>
      <td>0.010</td>
      <td>-0.239</td>
      <td>-0.090</td>
      <td>-0.288</td>
      <td>-0.193</td>
      <td>-0.338</td>
      <td>0.602</td>
      <td>-0.227</td>
      <td>1.143</td>
      <td>0.518</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.625</td>
      <td>-0.004</td>
      <td>-0.461</td>
      <td>-0.328</td>
      <td>-1.156e-01</td>
      <td>0.363</td>
      <td>0.091</td>
      <td>0.826</td>
      <td>-0.377</td>
      <td>0.265</td>
      <td>-0.144</td>
      <td>0.283</td>
      <td>-0.274</td>
      <td>0.619</td>
      <td>0.521</td>
      <td>0.352</td>
      <td>-0.183</td>
      <td>0.418</td>
      <td>-0.023</td>
      <td>-0.051</td>
      <td>-0.356</td>
      <td>-0.173</td>
      <td>0.314</td>
      <td>0.120</td>
      <td>-0.319</td>
      <td>-0.106</td>
      <td>0.071</td>
      <td>-0.332</td>
      <td>0.554</td>
      <td>0.650</td>
    </tr>
    <tr>
      <th>22</th>
      <td>-0.101</td>
      <td>-0.386</td>
      <td>0.327</td>
      <td>0.211</td>
      <td>5.427e-01</td>
      <td>0.987</td>
      <td>0.666</td>
      <td>0.807</td>
      <td>-0.560</td>
      <td>-0.260</td>
      <td>-0.205</td>
      <td>0.005</td>
      <td>0.339</td>
      <td>-0.447</td>
      <td>0.104</td>
      <td>-0.263</td>
      <td>-0.693</td>
      <td>0.747</td>
      <td>0.684</td>
      <td>-0.277</td>
      <td>0.024</td>
      <td>-0.388</td>
      <td>0.067</td>
      <td>-0.445</td>
      <td>0.472</td>
      <td>0.182</td>
      <td>-0.503</td>
      <td>0.262</td>
      <td>-0.286</td>
      <td>0.196</td>
    </tr>
    <tr>
      <th>23</th>
      <td>-0.318</td>
      <td>-0.309</td>
      <td>0.280</td>
      <td>-0.485</td>
      <td>-2.033e-02</td>
      <td>0.575</td>
      <td>0.736</td>
      <td>-0.528</td>
      <td>0.378</td>
      <td>0.803</td>
      <td>0.240</td>
      <td>-0.358</td>
      <td>-0.281</td>
      <td>0.106</td>
      <td>0.386</td>
      <td>0.070</td>
      <td>-0.379</td>
      <td>0.506</td>
      <td>0.043</td>
      <td>-0.266</td>
      <td>0.247</td>
      <td>0.202</td>
      <td>-0.310</td>
      <td>0.593</td>
      <td>0.443</td>
      <td>0.054</td>
      <td>0.148</td>
      <td>-0.057</td>
      <td>-0.048</td>
      <td>-0.165</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.031</td>
      <td>0.674</td>
      <td>0.309</td>
      <td>0.422</td>
      <td>1.777e-01</td>
      <td>2.276</td>
      <td>0.814</td>
      <td>6.221</td>
      <td>5.785</td>
      <td>1.956</td>
      <td>0.037</td>
      <td>-0.025</td>
      <td>0.183</td>
      <td>-0.214</td>
      <td>-0.333</td>
      <td>0.207</td>
      <td>0.274</td>
      <td>0.112</td>
      <td>-0.262</td>
      <td>-0.221</td>
      <td>0.200</td>
      <td>-0.123</td>
      <td>0.432</td>
      <td>-0.165</td>
      <td>0.630</td>
      <td>-0.053</td>
      <td>0.105</td>
      <td>-0.056</td>
      <td>0.131</td>
      <td>0.515</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.112</td>
      <td>0.055</td>
      <td>0.704</td>
      <td>-0.505</td>
      <td>-3.833e-01</td>
      <td>-0.666</td>
      <td>0.021</td>
      <td>1.070</td>
      <td>-1.507</td>
      <td>0.301</td>
      <td>0.108</td>
      <td>-0.215</td>
      <td>-0.404</td>
      <td>0.033</td>
      <td>-0.145</td>
      <td>-0.339</td>
      <td>0.210</td>
      <td>-0.316</td>
      <td>0.029</td>
      <td>0.130</td>
      <td>0.483</td>
      <td>-0.533</td>
      <td>-0.450</td>
      <td>0.264</td>
      <td>-0.004</td>
      <td>0.066</td>
      <td>0.005</td>
      <td>0.196</td>
      <td>0.095</td>
      <td>-0.115</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.084</td>
      <td>0.758</td>
      <td>0.304</td>
      <td>0.382</td>
      <td>1.690e-02</td>
      <td>0.662</td>
      <td>0.258</td>
      <td>4.950</td>
      <td>-0.876</td>
      <td>-0.106</td>
      <td>-0.523</td>
      <td>-0.220</td>
      <td>-0.109</td>
      <td>-0.043</td>
      <td>-0.013</td>
      <td>-0.274</td>
      <td>0.166</td>
      <td>-0.090</td>
      <td>0.430</td>
      <td>-0.062</td>
      <td>0.079</td>
      <td>0.501</td>
      <td>-0.005</td>
      <td>0.051</td>
      <td>0.738</td>
      <td>-0.253</td>
      <td>0.020</td>
      <td>0.513</td>
      <td>0.019</td>
      <td>-0.048</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.010</td>
      <td>-0.309</td>
      <td>0.020</td>
      <td>-0.540</td>
      <td>-3.044e-01</td>
      <td>0.052</td>
      <td>-0.082</td>
      <td>-0.102</td>
      <td>-1.079</td>
      <td>0.305</td>
      <td>0.508</td>
      <td>0.436</td>
      <td>0.133</td>
      <td>-0.735</td>
      <td>0.416</td>
      <td>0.387</td>
      <td>0.350</td>
      <td>0.752</td>
      <td>0.375</td>
      <td>0.213</td>
      <td>0.159</td>
      <td>0.528</td>
      <td>-0.177</td>
      <td>0.134</td>
      <td>0.217</td>
      <td>0.316</td>
      <td>0.596</td>
      <td>-0.629</td>
      <td>-0.189</td>
      <td>0.037</td>
    </tr>
    <tr>
      <th>28</th>
      <td>-0.290</td>
      <td>-1.145</td>
      <td>0.114</td>
      <td>0.138</td>
      <td>4.577e-01</td>
      <td>-0.334</td>
      <td>-0.080</td>
      <td>-0.171</td>
      <td>0.339</td>
      <td>-0.057</td>
      <td>0.113</td>
      <td>-0.859</td>
      <td>-0.185</td>
      <td>0.362</td>
      <td>0.334</td>
      <td>0.529</td>
      <td>0.081</td>
      <td>0.137</td>
      <td>-0.038</td>
      <td>0.065</td>
      <td>-0.062</td>
      <td>-0.236</td>
      <td>0.135</td>
      <td>-0.199</td>
      <td>-0.166</td>
      <td>-0.440</td>
      <td>0.536</td>
      <td>0.291</td>
      <td>-0.158</td>
      <td>0.359</td>
    </tr>
    <tr>
      <th>29</th>
      <td>-0.232</td>
      <td>-0.138</td>
      <td>-0.229</td>
      <td>0.584</td>
      <td>-4.025e-02</td>
      <td>0.744</td>
      <td>0.117</td>
      <td>0.739</td>
      <td>-0.579</td>
      <td>0.088</td>
      <td>0.049</td>
      <td>-0.057</td>
      <td>0.081</td>
      <td>0.311</td>
      <td>-0.083</td>
      <td>0.468</td>
      <td>0.575</td>
      <td>0.107</td>
      <td>-0.345</td>
      <td>-0.360</td>
      <td>-1.048</td>
      <td>0.240</td>
      <td>0.573</td>
      <td>0.296</td>
      <td>0.066</td>
      <td>-0.124</td>
      <td>-0.789</td>
      <td>-0.052</td>
      <td>-0.213</td>
      <td>0.272</td>
    </tr>
    <tr>
      <th>30</th>
      <td>-0.483</td>
      <td>-0.351</td>
      <td>-0.595</td>
      <td>0.018</td>
      <td>1.622e-01</td>
      <td>2.688</td>
      <td>2.002</td>
      <td>-1.169</td>
      <td>-4.592</td>
      <td>0.758</td>
      <td>-0.232</td>
      <td>0.527</td>
      <td>-0.256</td>
      <td>-0.283</td>
      <td>0.382</td>
      <td>-0.166</td>
      <td>-0.071</td>
      <td>-0.175</td>
      <td>0.147</td>
      <td>0.017</td>
      <td>-0.415</td>
      <td>1.163</td>
      <td>-0.043</td>
      <td>-0.095</td>
      <td>0.132</td>
      <td>-0.106</td>
      <td>0.269</td>
      <td>0.092</td>
      <td>-0.085</td>
      <td>0.584</td>
    </tr>
    <tr>
      <th>31</th>
      <td>-0.043</td>
      <td>-0.228</td>
      <td>0.005</td>
      <td>0.294</td>
      <td>1.484e-02</td>
      <td>0.528</td>
      <td>1.305</td>
      <td>-1.325</td>
      <td>-0.599</td>
      <td>0.679</td>
      <td>-0.370</td>
      <td>0.313</td>
      <td>-0.204</td>
      <td>-0.030</td>
      <td>0.068</td>
      <td>0.374</td>
      <td>0.116</td>
      <td>-0.045</td>
      <td>0.269</td>
      <td>0.022</td>
      <td>-0.111</td>
      <td>0.308</td>
      <td>-0.293</td>
      <td>0.095</td>
      <td>0.169</td>
      <td>0.391</td>
      <td>-0.277</td>
      <td>0.197</td>
      <td>-0.056</td>
      <td>0.120</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.380</td>
      <td>-0.099</td>
      <td>-0.117</td>
      <td>0.635</td>
      <td>1.412e-02</td>
      <td>1.199</td>
      <td>0.737</td>
      <td>1.064</td>
      <td>-0.284</td>
      <td>0.241</td>
      <td>-0.261</td>
      <td>0.317</td>
      <td>0.112</td>
      <td>-0.012</td>
      <td>0.474</td>
      <td>-0.274</td>
      <td>-0.170</td>
      <td>0.042</td>
      <td>0.186</td>
      <td>-0.043</td>
      <td>0.219</td>
      <td>-0.278</td>
      <td>0.171</td>
      <td>0.226</td>
      <td>0.166</td>
      <td>-0.015</td>
      <td>0.160</td>
      <td>0.050</td>
      <td>0.914</td>
      <td>-0.187</td>
    </tr>
    <tr>
      <th>33</th>
      <td>-0.074</td>
      <td>-0.186</td>
      <td>-0.815</td>
      <td>0.013</td>
      <td>-1.385e-01</td>
      <td>-0.446</td>
      <td>-0.041</td>
      <td>-0.260</td>
      <td>0.365</td>
      <td>0.066</td>
      <td>-0.081</td>
      <td>0.040</td>
      <td>-0.323</td>
      <td>-0.252</td>
      <td>0.216</td>
      <td>0.442</td>
      <td>-0.403</td>
      <td>0.147</td>
      <td>0.812</td>
      <td>-0.169</td>
      <td>-0.296</td>
      <td>-0.652</td>
      <td>-0.351</td>
      <td>-0.173</td>
      <td>0.257</td>
      <td>-0.094</td>
      <td>-0.460</td>
      <td>-0.587</td>
      <td>0.069</td>
      <td>0.291</td>
    </tr>
    <tr>
      <th>34</th>
      <td>-0.264</td>
      <td>0.157</td>
      <td>-0.141</td>
      <td>-0.298</td>
      <td>1.590e-01</td>
      <td>0.173</td>
      <td>0.292</td>
      <td>0.196</td>
      <td>0.171</td>
      <td>0.125</td>
      <td>0.335</td>
      <td>0.310</td>
      <td>-0.096</td>
      <td>-0.256</td>
      <td>-0.547</td>
      <td>0.563</td>
      <td>-0.379</td>
      <td>0.368</td>
      <td>0.563</td>
      <td>-0.211</td>
      <td>0.119</td>
      <td>0.737</td>
      <td>0.035</td>
      <td>-0.611</td>
      <td>-0.418</td>
      <td>-0.289</td>
      <td>-0.358</td>
      <td>-0.282</td>
      <td>-0.033</td>
      <td>-0.180</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.023</td>
      <td>0.560</td>
      <td>0.045</td>
      <td>0.104</td>
      <td>4.954e-02</td>
      <td>-0.089</td>
      <td>-0.058</td>
      <td>0.548</td>
      <td>-0.386</td>
      <td>-0.023</td>
      <td>0.242</td>
      <td>-0.063</td>
      <td>-0.587</td>
      <td>0.214</td>
      <td>0.067</td>
      <td>-0.028</td>
      <td>1.047</td>
      <td>0.086</td>
      <td>0.401</td>
      <td>0.406</td>
      <td>-0.036</td>
      <td>0.218</td>
      <td>-0.133</td>
      <td>0.110</td>
      <td>0.785</td>
      <td>0.277</td>
      <td>-0.226</td>
      <td>0.051</td>
      <td>0.201</td>
      <td>0.022</td>
    </tr>
    <tr>
      <th>36</th>
      <td>-0.038</td>
      <td>-0.069</td>
      <td>0.223</td>
      <td>0.378</td>
      <td>-1.722e-01</td>
      <td>-0.231</td>
      <td>-0.247</td>
      <td>0.507</td>
      <td>0.426</td>
      <td>0.216</td>
      <td>0.325</td>
      <td>0.273</td>
      <td>0.050</td>
      <td>-0.431</td>
      <td>-0.280</td>
      <td>-0.227</td>
      <td>0.028</td>
      <td>-0.457</td>
      <td>-0.297</td>
      <td>0.201</td>
      <td>0.077</td>
      <td>0.285</td>
      <td>-0.141</td>
      <td>-0.438</td>
      <td>-0.240</td>
      <td>0.078</td>
      <td>0.137</td>
      <td>-0.226</td>
      <td>0.151</td>
      <td>0.191</td>
    </tr>
    <tr>
      <th>37</th>
      <td>-0.256</td>
      <td>-0.322</td>
      <td>-0.064</td>
      <td>-0.569</td>
      <td>3.160e-01</td>
      <td>1.093</td>
      <td>0.600</td>
      <td>-0.157</td>
      <td>-0.419</td>
      <td>0.471</td>
      <td>-0.133</td>
      <td>-0.361</td>
      <td>0.115</td>
      <td>-0.133</td>
      <td>0.211</td>
      <td>0.220</td>
      <td>0.058</td>
      <td>-0.307</td>
      <td>0.959</td>
      <td>0.586</td>
      <td>-0.157</td>
      <td>0.073</td>
      <td>-0.866</td>
      <td>-0.055</td>
      <td>0.460</td>
      <td>0.559</td>
      <td>0.272</td>
      <td>-0.189</td>
      <td>0.344</td>
      <td>0.097</td>
    </tr>
    <tr>
      <th>38</th>
      <td>-0.161</td>
      <td>-0.275</td>
      <td>-0.108</td>
      <td>0.397</td>
      <td>7.066e-02</td>
      <td>0.264</td>
      <td>0.407</td>
      <td>2.354</td>
      <td>0.681</td>
      <td>0.441</td>
      <td>0.512</td>
      <td>0.170</td>
      <td>-0.161</td>
      <td>-0.680</td>
      <td>-0.133</td>
      <td>0.115</td>
      <td>0.258</td>
      <td>-0.391</td>
      <td>0.005</td>
      <td>0.327</td>
      <td>-0.149</td>
      <td>-0.028</td>
      <td>-0.196</td>
      <td>-0.357</td>
      <td>0.136</td>
      <td>-0.408</td>
      <td>0.051</td>
      <td>-0.043</td>
      <td>0.280</td>
      <td>-0.103</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.233</td>
      <td>-0.394</td>
      <td>-0.066</td>
      <td>-0.022</td>
      <td>-4.802e-02</td>
      <td>1.303</td>
      <td>0.367</td>
      <td>-1.006</td>
      <td>-1.285</td>
      <td>0.188</td>
      <td>0.169</td>
      <td>0.279</td>
      <td>0.387</td>
      <td>0.117</td>
      <td>0.061</td>
      <td>-0.204</td>
      <td>0.198</td>
      <td>0.600</td>
      <td>0.584</td>
      <td>0.395</td>
      <td>0.114</td>
      <td>0.707</td>
      <td>-0.492</td>
      <td>-0.170</td>
      <td>-0.406</td>
      <td>-0.353</td>
      <td>0.061</td>
      <td>0.258</td>
      <td>0.726</td>
      <td>0.371</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.067</td>
      <td>-0.016</td>
      <td>-0.281</td>
      <td>0.049</td>
      <td>-2.214e-01</td>
      <td>0.900</td>
      <td>0.064</td>
      <td>0.447</td>
      <td>0.288</td>
      <td>0.325</td>
      <td>-0.316</td>
      <td>-0.313</td>
      <td>0.165</td>
      <td>-0.379</td>
      <td>0.236</td>
      <td>-0.118</td>
      <td>0.768</td>
      <td>-0.003</td>
      <td>0.395</td>
      <td>0.139</td>
      <td>-0.133</td>
      <td>0.294</td>
      <td>-0.252</td>
      <td>0.744</td>
      <td>0.378</td>
      <td>0.529</td>
      <td>0.110</td>
      <td>-0.271</td>
      <td>0.268</td>
      <td>-0.639</td>
    </tr>
    <tr>
      <th>41</th>
      <td>-0.258</td>
      <td>-0.132</td>
      <td>-0.131</td>
      <td>0.567</td>
      <td>-2.100e-02</td>
      <td>-0.102</td>
      <td>-0.303</td>
      <td>0.484</td>
      <td>0.287</td>
      <td>0.103</td>
      <td>0.013</td>
      <td>0.586</td>
      <td>-0.384</td>
      <td>-0.518</td>
      <td>-0.048</td>
      <td>-0.115</td>
      <td>0.391</td>
      <td>0.318</td>
      <td>0.168</td>
      <td>-0.354</td>
      <td>-0.136</td>
      <td>-0.501</td>
      <td>0.166</td>
      <td>-0.209</td>
      <td>-0.268</td>
      <td>0.378</td>
      <td>-0.781</td>
      <td>-0.212</td>
      <td>0.371</td>
      <td>0.454</td>
    </tr>
    <tr>
      <th>42</th>
      <td>-0.157</td>
      <td>-0.136</td>
      <td>0.543</td>
      <td>0.018</td>
      <td>1.420e-01</td>
      <td>-0.482</td>
      <td>0.225</td>
      <td>-0.901</td>
      <td>-0.172</td>
      <td>0.134</td>
      <td>0.217</td>
      <td>0.489</td>
      <td>-0.264</td>
      <td>0.168</td>
      <td>0.573</td>
      <td>-0.813</td>
      <td>0.301</td>
      <td>0.660</td>
      <td>0.308</td>
      <td>-0.033</td>
      <td>-0.167</td>
      <td>-0.180</td>
      <td>-0.382</td>
      <td>-0.109</td>
      <td>0.381</td>
      <td>0.244</td>
      <td>-0.338</td>
      <td>0.556</td>
      <td>0.116</td>
      <td>-0.841</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.158</td>
      <td>0.103</td>
      <td>0.370</td>
      <td>0.025</td>
      <td>-1.047e-01</td>
      <td>-0.226</td>
      <td>0.104</td>
      <td>0.029</td>
      <td>0.274</td>
      <td>-0.110</td>
      <td>-0.524</td>
      <td>0.316</td>
      <td>0.202</td>
      <td>-0.159</td>
      <td>0.017</td>
      <td>0.241</td>
      <td>0.223</td>
      <td>0.086</td>
      <td>-0.115</td>
      <td>0.052</td>
      <td>0.911</td>
      <td>0.342</td>
      <td>0.166</td>
      <td>0.288</td>
      <td>0.232</td>
      <td>-0.450</td>
      <td>0.068</td>
      <td>-0.958</td>
      <td>0.740</td>
      <td>-0.203</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.181</td>
      <td>-0.002</td>
      <td>-0.045</td>
      <td>0.212</td>
      <td>-1.935e-01</td>
      <td>0.486</td>
      <td>-0.113</td>
      <td>-0.438</td>
      <td>-0.140</td>
      <td>-0.173</td>
      <td>0.277</td>
      <td>-0.417</td>
      <td>-0.045</td>
      <td>-0.393</td>
      <td>-0.307</td>
      <td>0.563</td>
      <td>-0.399</td>
      <td>0.184</td>
      <td>0.373</td>
      <td>0.509</td>
      <td>-0.224</td>
      <td>-0.348</td>
      <td>-0.201</td>
      <td>-0.623</td>
      <td>-0.025</td>
      <td>-0.463</td>
      <td>0.455</td>
      <td>0.421</td>
      <td>0.628</td>
      <td>0.174</td>
    </tr>
    <tr>
      <th>45</th>
      <td>-0.180</td>
      <td>0.126</td>
      <td>0.313</td>
      <td>0.186</td>
      <td>-6.769e-02</td>
      <td>-0.009</td>
      <td>0.774</td>
      <td>-0.556</td>
      <td>0.296</td>
      <td>0.269</td>
      <td>0.271</td>
      <td>-0.069</td>
      <td>-0.554</td>
      <td>0.032</td>
      <td>0.030</td>
      <td>0.095</td>
      <td>0.026</td>
      <td>-0.448</td>
      <td>-0.114</td>
      <td>0.397</td>
      <td>0.265</td>
      <td>1.088</td>
      <td>-0.015</td>
      <td>0.211</td>
      <td>0.155</td>
      <td>-0.449</td>
      <td>-0.025</td>
      <td>-0.025</td>
      <td>0.331</td>
      <td>-0.460</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0.058</td>
      <td>-0.294</td>
      <td>-0.381</td>
      <td>0.612</td>
      <td>-5.014e-01</td>
      <td>0.021</td>
      <td>-0.630</td>
      <td>0.295</td>
      <td>0.212</td>
      <td>-0.230</td>
      <td>-0.372</td>
      <td>-0.687</td>
      <td>0.647</td>
      <td>0.118</td>
      <td>0.434</td>
      <td>0.732</td>
      <td>-0.097</td>
      <td>-0.159</td>
      <td>-0.340</td>
      <td>-0.229</td>
      <td>-0.081</td>
      <td>0.407</td>
      <td>-0.173</td>
      <td>0.660</td>
      <td>0.883</td>
      <td>-0.492</td>
      <td>-0.142</td>
      <td>-0.162</td>
      <td>0.672</td>
      <td>-0.297</td>
    </tr>
    <tr>
      <th>47</th>
      <td>-0.154</td>
      <td>0.357</td>
      <td>-0.121</td>
      <td>0.054</td>
      <td>-5.688e-01</td>
      <td>0.250</td>
      <td>0.541</td>
      <td>0.546</td>
      <td>0.167</td>
      <td>0.010</td>
      <td>0.374</td>
      <td>0.129</td>
      <td>-0.448</td>
      <td>0.288</td>
      <td>-0.173</td>
      <td>0.332</td>
      <td>-0.243</td>
      <td>-0.657</td>
      <td>0.427</td>
      <td>-0.328</td>
      <td>-0.369</td>
      <td>-0.069</td>
      <td>-0.203</td>
      <td>0.036</td>
      <td>-0.555</td>
      <td>0.120</td>
      <td>0.669</td>
      <td>-0.172</td>
      <td>0.295</td>
      <td>0.019</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.620</td>
      <td>0.495</td>
      <td>-0.367</td>
      <td>-0.208</td>
      <td>1.661e-01</td>
      <td>0.076</td>
      <td>-0.032</td>
      <td>-0.078</td>
      <td>-0.066</td>
      <td>0.106</td>
      <td>-0.684</td>
      <td>0.022</td>
      <td>0.161</td>
      <td>0.151</td>
      <td>0.547</td>
      <td>1.098</td>
      <td>0.040</td>
      <td>0.096</td>
      <td>0.026</td>
      <td>-0.444</td>
      <td>-0.571</td>
      <td>0.388</td>
      <td>0.031</td>
      <td>0.032</td>
      <td>-0.096</td>
      <td>0.594</td>
      <td>-0.439</td>
      <td>0.036</td>
      <td>0.178</td>
      <td>-0.153</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="admonition-some-remarks admonition">
<p class="admonition-title">Some Remarks</p>
<p>It should not come as a surprise that almost all rows in the ground truth matrix <span class="math notranslate nohighlight">\(\y\)</span> (<code class="docutils literal notranslate"><span class="pre">y_true_df</span></code>)
are zeros, by construction, the ground truth matrix <span class="math notranslate nohighlight">\(\y\)</span> only has non-zero values in the rows
where there is an object. Neither should you be worried that the prediction matrix <span class="math notranslate nohighlight">\(\hat{\y}\)</span>
(<code class="docutils literal notranslate"><span class="pre">y_pred_df</span></code>) has “jibberish” values, this is because this is the first pass of the model for the
first batch of images, the model has not been trained properly yet.</p>
</div>
<section id="bipartite-matching">
<h3>Bipartite Matching<a class="headerlink" href="#bipartite-matching" title="Permalink to this headline">#</a></h3>
<p><strong>TODO put more image</strong></p>
<p>In <a class="reference internal" href="#image-1-grids"><span class="std std-numref">Fig. 3</span></a>, there are 2 ground truth bounding box dog and human in that image.
Let us take the dog for an example, the dog lies in grid cell <span class="math notranslate nohighlight">\(i=30\)</span> as the groundtruth, and
is encoded in row 30 of the ground truth matrix <span class="math notranslate nohighlight">\(\y\)</span>.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">display</span><span class="p">(</span><span class="n">y_true_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">30</span><span class="p">]</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>x_i^1</th>
      <th>y_i^1</th>
      <th>w_i^1</th>
      <th>h_i^1</th>
      <th>conf_i^1</th>
      <th>x_i^2</th>
      <th>y_i^2</th>
      <th>w_i^2</th>
      <th>h_i^2</th>
      <th>conf_i^2</th>
      <th>p_1</th>
      <th>p_2</th>
      <th>p_3</th>
      <th>p_4</th>
      <th>p_5</th>
      <th>p_6</th>
      <th>p_7</th>
      <th>p_8</th>
      <th>p_9</th>
      <th>p_10</th>
      <th>p_11</th>
      <th>p_12</th>
      <th>p_13</th>
      <th>p_14</th>
      <th>p_15</th>
      <th>p_16</th>
      <th>p_17</th>
      <th>p_18</th>
      <th>p_19</th>
      <th>p_20</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>0.409</td>
      <td>0.277</td>
      <td>0.416</td>
      <td>0.262</td>
      <td>1.0</td>
      <td>0.409</td>
      <td>0.277</td>
      <td>0.416</td>
      <td>0.262</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>A brief glance confirms that our encoding is sound, the first four elements are the coordinates
of the bounding box, the fifth element is the object confidence score which is constructed as 1
since there exists a ground truth inside. These 5 elements
are repeated for the next 5 elements by design as we allow <span class="math notranslate nohighlight">\(B=2\)</span> bounding boxes per grid cell.
The last 20 elements are the class probabilities, one-hot encoded at the 12th index (11 if index starts
from 0) because the dog class is the 12th class in the dataset.</p>
<p>We now look at the corresponding grid cell in the prediction matrix <span class="math notranslate nohighlight">\(\hat{\y}\)</span>, note that this
row is entirely predicted by the model during the first iteration, and numbers can be very
different from the ground truth. The reason you see negative coordinates is because our outputs
are not constrained, it ranges from <span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span>. A reasonable choice is to add a <code class="docutils literal notranslate"><span class="pre">nn.Sigmoid()</span></code>
layer after the last layer of the <code class="docutils literal notranslate"><span class="pre">head</span></code> module, but we did not do that.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">display</span><span class="p">(</span><span class="n">y_pred_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">30</span><span class="p">]</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>xhat_i^1</th>
      <th>yhat_i^1</th>
      <th>what_i^1</th>
      <th>hhat_i^1</th>
      <th>confhat_i^1</th>
      <th>xhat_i^2</th>
      <th>yhat_i^2</th>
      <th>what_i^2</th>
      <th>hhat_i^2</th>
      <th>confhat_i^2</th>
      <th>phat_1</th>
      <th>phat_2</th>
      <th>phat_3</th>
      <th>phat_4</th>
      <th>phat_5</th>
      <th>phat_6</th>
      <th>phat_7</th>
      <th>phat_8</th>
      <th>phat_9</th>
      <th>phat_10</th>
      <th>phat_11</th>
      <th>phat_12</th>
      <th>phat_13</th>
      <th>phat_14</th>
      <th>phat_15</th>
      <th>phat_16</th>
      <th>phat_17</th>
      <th>phat_18</th>
      <th>phat_19</th>
      <th>phat_20</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>-0.483</td>
      <td>-0.351</td>
      <td>-0.595</td>
      <td>0.018</td>
      <td>0.162</td>
      <td>2.688</td>
      <td>2.002</td>
      <td>-1.169</td>
      <td>-4.592</td>
      <td>0.758</td>
      <td>-0.232</td>
      <td>0.527</td>
      <td>-0.256</td>
      <td>-0.283</td>
      <td>0.382</td>
      <td>-0.166</td>
      <td>-0.071</td>
      <td>-0.175</td>
      <td>0.147</td>
      <td>0.017</td>
      <td>-0.415</td>
      <td>1.163</td>
      <td>-0.043</td>
      <td>-0.095</td>
      <td>0.132</td>
      <td>-0.106</td>
      <td>0.269</td>
      <td>0.092</td>
      <td>-0.085</td>
      <td>0.584</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><img alt="" src="https://storage.googleapis.com/reighns/images/flattened_grid_cell.jpg" /></p>
<p>As seen in the figure, for the grid cell 30, the model predicts two bounding boxes, but there’s only
one ground truth bounding box.</p>
<div class="cell tag_output_scroll tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>b_gt: [0.4093485  0.27699995 0.4164306  0.262     ]
b_pred_1: [-0.48271456 -0.3510531  -0.5948943   0.01832254]
b_pred_2: [ 2.6877854  2.0019963 -1.1685951 -4.591512 ]
</pre></div>
</div>
</div>
</div>
<p>It then makes sense to only choose one of the <em><strong>two predicted</strong></em> bounding boxes to <strong>match</strong> with the
ground truth bounding box. This is where the bipartite matching comes in, we will choose the
bounding box with the highest IOU (Intersection over Union) with the ground truth bounding box.
In other words, we use IOU as a proxy to measure the similarity metric of <code class="docutils literal notranslate"><span class="pre">IOU(b_gt,</span> <span class="pre">b_pred_1)</span></code>
and <code class="docutils literal notranslate"><span class="pre">IOU(b_gt,</span> <span class="pre">b_pred_2)</span></code>. The bounding box (<code class="docutils literal notranslate"><span class="pre">b_pred_1</span></code> or <code class="docutils literal notranslate"><span class="pre">b_pred_2</span></code> but never both)
with the highest IOU will be the one that we choose to compute the loss with.</p>
<p>And this is why in the paper <span id="id7">[<a class="reference internal" href="#id13" title="Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: unified, real-time object detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. doi:10.1109/cvpr.2016.91.">Redmon <em>et al.</em>, 2016</a>]</span>, the authors mentioned that the
construction of the <strong>confidence in ground truth</strong> to be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\conf_i =
\begin{cases}
    \textbf{IOU}(\b_i, \bhat_i)     &amp; \textbf{if grid cell } i \textbf{ has an object}\\
    0                               &amp; \textbf{otherwise}
\end{cases}
\end{split}\]</div>
<p>where we define the confidence score of the ground truth to be the IOU between the
ground truth <span class="math notranslate nohighlight">\(\b_{30}\)</span> and the “survivor” <span class="math notranslate nohighlight">\(\bhat_{30}\)</span>, chosen out of the two predictions,
by <span class="math notranslate nohighlight">\(\underset{\bhat_i \in \{\bhat_i^1, \bhat_i^2\}}{\max}\textbf{IOU}(\b_i, \bhat_i)\)</span>.</p>
<div class="admonition-note admonition">
<p class="admonition-title">Note</p>
<p>During our construction of the ground truth matrix in <a class="reference internal" href="#yolo_gt_matrix">Definition 6</a>, we set the confidence score
to be 1 if there is an object in the grid cell, and 0 otherwise. These numbers are a placeholder
and will only be realized during training, as we will only know the IOU between the ground truth
and the predicted bounding box during training.</p>
</div>
<p>What we have described above is a form of matching algorithm. To reiterate, a model like
YOLOv1 can output and predict multiple <span class="math notranslate nohighlight">\(B\)</span> number of bounding boxes (<span class="math notranslate nohighlight">\(B=2\)</span>),
but you need to choose one out of the <span class="math notranslate nohighlight">\(B\)</span> predicted bounding boxes to compute/compare with
the ground truth bounding box. In YOLOv1, they used the same matching algorithm that
two-staged detectors like Faster RCNN use, which use the IOU between the ground truth
and predicted bounding boxes to determine matching, (i.e ground truth in grid cell
<span class="math notranslate nohighlight">\(i\)</span> will match to the predicted bbox in grid cell <span class="math notranslate nohighlight">\(i\)</span> with the highest IOU between them).</p>
<p>It’s also worth pointing out that two-stage architectures also specify a minimum IOU
for defining negative background boxes, and their loss functions explicitly ignore
all predicted boxes that fall between these thresholds.
YOLO doesn’t do this, most likely because it’s producing so few boxes anyway that it isn’t
a problem in practice <span id="id8">[<a class="reference internal" href="#id14" title="Harry Turner. Yolo v1. 2021. URL: https://www.harrysprojects.com/articles/yolov1.html.">Turner, 2021</a>]</span>.</p>
</section>
<section id="total-loss-for-a-single-image">
<h3>Total Loss for a Single Image<a class="headerlink" href="#total-loss-for-a-single-image" title="Permalink to this headline">#</a></h3>
<p>Having the construction of the ground truth and the prediction matrix, it is now time to understand
how the loss function is formulated. I took the liberty to change the notations from the original
paper for simplicity.</p>
<p>Let’s look at the original loss function from the paper <span id="id9">[<a class="reference internal" href="#id13" title="Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: unified, real-time object detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. doi:10.1109/cvpr.2016.91.">Redmon <em>et al.</em>, 2016</a>]</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-original-yolo-loss">
<span class="eqno">(5)<a class="headerlink" href="#equation-eq-original-yolo-loss" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
\mathcal{L}(\y, \yhat) 
&amp;= \color{blue}{\lambda_\textbf{coord}
\sum_{i = 0}^{S^2}
    \sum_{j = 0}^{B}
     {\mathbb{1}}_{ij}^{\text{obj}}
            \left[
            \left(
                x_i - \hat{x}_i
            \right)^2 +
            \left(
                y_i - \hat{y}_i
            \right)^2
            \right]} \\
&amp;= \color{blue}{\lambda_\textbf{coord} 
\sum_{i = 0}^{S^2}
    \sum_{j = 0}^{B}
         {\mathbb{1}}_{ij}^{\text{obj}}
         \left[
        \left(
            \sqrt{w_i} - \sqrt{\hat{w}_i}
        \right)^2 +
        \left(
            \sqrt{h_i} - \sqrt{\hat{h}_i}
        \right)^2
        \right]} \\
&amp;= \color{green}{
 \sum_{i = 0}^{S^2}
    \sum_{j = 0}^{B}
        {\mathbb{1}}_{ij}^{\text{obj}}
        \left(
            C_i - \hat{C}_i
        \right)^2} \\
&amp;= \color{green}{\lambda_\textrm{noobj}
\sum_{i = 0}^{S^2}
    \sum_{j = 0}^{B}
    {\mathbb{1}}_{ij}^{\text{noobj}}
        \left(
            C_i - \hat{C}_i
        \right)^2} \\
&amp;= \color{red}{
 \sum_{i = 0}^{S^2}
{{1}}_i^{\text{obj}}
    \sum_{c \in \textrm{classes}}
        \left(
            p_i(c) - \hat{p}_i(c)
        \right)^2}
\end{align}
\end{split}\]</div>
<p>Before we dive into what each equation means, we first establish the notations that we will be using:</p>
<p>We define the loss function to be <span class="math notranslate nohighlight">\(\L\)</span>, a function of <span class="math notranslate nohighlight">\(\y\)</span> and <span class="math notranslate nohighlight">\(\yhat\)</span> respectively.
Both <span class="math notranslate nohighlight">\(\y\)</span> and <span class="math notranslate nohighlight">\(\yhat\)</span> are of shape <span class="math notranslate nohighlight">\(\R^{49 \times 30}\)</span>, the <strong>outer summation</strong> <span class="math notranslate nohighlight">\(\sum_{i = 0}^{S^2}\)</span>
tells us that we are actually computing
the loss over each grid cell <span class="math notranslate nohighlight">\(i\)</span> and summing them (49 rows) up afterwards, which constitute
to our total loss <span class="math notranslate nohighlight">\(\L(\y, \yhat)\)</span>. We will skip the meaning behind the summation <span class="math notranslate nohighlight">\(\sum_{j = 0}^{B}\)</span> for now.</p>
<p>The <a class="reference internal" href="#yolov1-loss-1"><span class="std std-numref">Fig. 9</span></a> depicts how the loss function is summed over a single image over all the grid cells.</p>
<figure class="align-default" id="yolov1-loss-1">
<img alt="_images/image_1_loss.jpg" src="_images/image_1_loss.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">Loss function for a single image.</span><a class="headerlink" href="#yolov1-loss-1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Consequently, we define <span class="math notranslate nohighlight">\(\L_i\)</span> to be the loss of each grid cell <span class="math notranslate nohighlight">\(i\)</span> and say that the total loss for
a single image is defined as:</p>
<div class="math notranslate nohighlight" id="equation-eq-yolov1-total-loss">
<span class="eqno">(6)<a class="headerlink" href="#equation-eq-yolov1-total-loss" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \L(\y, \yhat) &amp; \overset{(a)}{=}  \sum_{i=1}^{S=7}\sum_{j=1}^{S=7} \L_{ij}(\y_{ij}, \yhat_{ij}) \\
                  &amp; \overset{(b)}{=} \sum_{i=1}^{S^2=49} \L_i(\y_i, \yhat_i)                        \\
                  &amp; \overset{(c)}{=} \sum_{i=0}^{S^2=48} \L_i(\y_i, \yhat_i)                        \\
\end{align}
\end{split}\]</div>
<p>but recall that the equation <span class="math notranslate nohighlight">\((a)\)</span> is not used by us as it is more cumbersome in notations,
but just remember that equation <span class="math notranslate nohighlight">\((a)\)</span> and <span class="math notranslate nohighlight">\((b)\)</span> are equivalent. Lastly, because we are
dealing with python, we start our indexing from 0, hence the equation <span class="math notranslate nohighlight">\((c)\)</span>. Do not get confused!</p>
<p>Equation <a class="reference internal" href="#equation-eq-yolov1-total-loss">(6)</a> <em><strong>merely</strong></em> sums up the loss for 1 single image,
however, in deep learning, we also have the concept of batch size, where an additional batch
size dimension is added. Rest assured it is as simple as summing over the batches and averaging over batch only
and will be shown in code later.</p>
<div class="math notranslate nohighlight" id="equation-eq-yolov1-total-loss-over-batches">
<span class="eqno">(7)<a class="headerlink" href="#equation-eq-yolov1-total-loss-over-batches" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \L(\y, \yhat) &amp; \overset{(d)}{=} \dfrac{1}{\text{Batch Size}} \sum_{k=0}^{\text{Batch Size}}\L(\y^{k}, \yhat^{k})                       \\
\end{align}
\end{split}\]</div>
</section>
<section id="loss-for-a-single-grid-cell-in-a-single-image">
<h3>Loss for a Single Grid Cell in a Single Image<a class="headerlink" href="#loss-for-a-single-grid-cell-in-a-single-image" title="Permalink to this headline">#</a></h3>
<p>The simplications in the previous sections allow us to better appreciate what each equation in the loss
function mean.</p>
<p>We will also make some very rigid assumptions:</p>
<div class="admonition-intuition admonition">
<p class="admonition-title">Intuition</p>
<p>Before we look at the seemingly scary formula, let us first think retrospectively on what the loss
should penalize/maximize.</p>
<ol class="arabic simple">
<li><p>The loss should penalize the model if the predicted bounding box x-y coordinates is far away from the ground truth.</p></li>
<li><p>The loss should penalize the model if the predicted bounding box width and height is far away from the ground truth.</p></li>
<li><p>The loss should penalize the model if the predicted bounding box has low confidence that the grid cell has
an object where in fact there is an object. This means the model is not confident enough to predict that
this grid cell has an object.</p></li>
<li><p>The loss should penalize the model if the predicted bounding box has high confidence that the grid cell
has an object where in fact there is no object.</p></li>
<li><p>The loss should penalize the model if the predicted bounding box is not predicting the correct class.</p></li>
</ol>
</div>
<section id="the-formula">
<h4>The Formula<a class="headerlink" href="#the-formula" title="Permalink to this headline">#</a></h4>
<p>In the previous section, equation <a class="reference internal" href="#equation-eq-yolov1-total-loss">(6)</a> is the loss function for a single image.
We now define <span class="math notranslate nohighlight">\(\L_i(\y_i, \yhat_i)\)</span>, the loss of each grid cell <span class="math notranslate nohighlight">\(i\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-yolov1-loss-for-single-grid-cell">
<span class="eqno">(8)<a class="headerlink" href="#equation-eq-yolov1-loss-for-single-grid-cell" title="Permalink to this equation">#</a></span>\[\begin{split}
    \begin{align}
        \L_i(\y_i, \yhat_i) &amp; \overset{(a)}{=} \color{blue}{\lambda_\textbf{coord} \sum_{j=1}^{B=2} \1_{ij}^{\text{obj}} \lsq \lpar x_i - \hat{x}_i^j \rpar^2 + \lpar y_i - \hat{y}_i^j \rpar^2 \rsq}                             \\
                            &amp; \overset{(b)}{+} \color{blue}{\lambda_\textbf{coord} \sum_{j=1}^{B=2} \1_{ij}^{\text{obj}} \lsq \lpar \sqrt{w_i} - \sqrt{\hat{w}_i^j} \rpar^2 + \lpar \sqrt{h_i} - \sqrt{\hat{h}_i^j} \rpar^2 \rsq} \\
                            &amp; \overset{(c)}{+} \color{green}{\sum_{j=1}^{B=2} \1_{ij}^{\text{obj}} \lpar \conf_i - \confhat_i^j \rpar^2}                                                                                          \\
                            &amp; \overset{(d)}{+} \color{green}{\lambda_\textbf{noobj}\sum_{j=1}^{B=2} \1_{ij}^{\text{noobj}} \lpar \conf_i - \confhat_i^j \rpar^2}                                                                  \\
                            &amp; \overset{(e)}{+} \color{red}{\obji \sum_{c \in \cc} \lpar \p_i(c) - \phat_i(c) \rpar^2}                                                                                               \\
    \end{align}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p>We removed the outer summation <span class="math notranslate nohighlight">\(\sum_{i=1}^{S^2=49}\)</span> as we are only looking at one grid cell <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{i}^{obj}\)</span> is <span class="math notranslate nohighlight">\(1\)</span> when there is a <strong>ground truth object</strong> in cell <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^{obj}\)</span> denotes that the <span class="math notranslate nohighlight">\(j\)</span>th bounding box predictor in cell <span class="math notranslate nohighlight">\(i\)</span> is <strong>matched</strong> to ground truth object.
This is not easy to understand.</p>
<ul>
<li><p>What this means in our context is that for any grid cell <span class="math notranslate nohighlight">\(i\)</span>, there are <span class="math notranslate nohighlight">\(B=2\)</span> bounding box predictors;</p></li>
<li><p>Then, <span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^{obj}\)</span> is <span class="math notranslate nohighlight">\(1\)</span> if it fulfills two conditions:</p></li>
<li><p>Firstly, there is a ground truth object in cell <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p>Secondly, if the first point is true, then out of the <span class="math notranslate nohighlight">\(B=2\)</span> bounding box predictors, only one of them is matched to the ground truth object.
We index the 2 bounding box predictors with <span class="math notranslate nohighlight">\(j\)</span>, and the one that is matched to the ground truth object will have <span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^{obj}=1\)</span>,
and the other one will have <span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^{obj}=0\)</span>, essentially only taking the matched bounding box predictor into account.</p></li>
<li><p>Remember the matching is done by checking which of the <span class="math notranslate nohighlight">\(B=2\)</span> predicted bounding box has the highest IOU score with the ground truth bounding box.</p></li>
<li><p>This is why the notation has a summation <span class="math notranslate nohighlight">\(\sum_{j=1}^{B=2}\)</span>, we are looping over the <span class="math notranslate nohighlight">\(B=2\)</span> bounding box predictors to see which one is matched to the ground truth object.</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\1_{ij}^{\text{obj}} = 
\begin{cases}
    1     &amp; \textbf{if the ith grid cell has a ground truth obj and jth predictor is matched}\\
    0     &amp; \textbf{otherwise}
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^{noobj}\)</span> is the opposite of <span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^{obj}\)</span>, where it is <span class="math notranslate nohighlight">\(1\)</span> when there is no <strong>ground truth object</strong> in cell <span class="math notranslate nohighlight">\(i\)</span>
and the <span class="math notranslate nohighlight">\(j\)</span>th bounding box predictor in cell <span class="math notranslate nohighlight">\(i\)</span> has the highest IOU score among all the predictors of this cell
when compared to the ground truth. <strong>More on this later as there can be a few interpretations.</strong></p></li>
<li><p>Note carefully <span class="math notranslate nohighlight">\(j\)</span> in this context is the indices of the bounding box predictors in each grid cell i.e. in <span class="math notranslate nohighlight">\(\bhat^1\)</span> is the 1st predicted bounding box and the 1 refers to the index <span class="math notranslate nohighlight">\(j=1\)</span>.</p></li>
<li><p>A constant on <strong>what is matched with an object mean? -&gt; it means the bipartite matching algorithm discussed in the previous section</strong>.</p></li>
<li><p>Last but not least, the <span class="math notranslate nohighlight">\(\xx_i\)</span>, <span class="math notranslate nohighlight">\(\yy_i\)</span>, <span class="math notranslate nohighlight">\(\ww_i\)</span>, <span class="math notranslate nohighlight">\(\hh_i\)</span> are the ground truth bounding box coordinates and <span class="math notranslate nohighlight">\(\hat{\xx}_i^j\)</span>, <span class="math notranslate nohighlight">\(\hat{\yy}_i^j\)</span>, <span class="math notranslate nohighlight">\(\hat{\ww}_i^j\)</span>, <span class="math notranslate nohighlight">\(\hat{\hh}_i^j\)</span> are the predicted bounding box coordinates, both
for the <strong>grid cell</strong> <span class="math notranslate nohighlight">\(i\)</span>;</p></li>
<li><p>The <span class="math notranslate nohighlight">\(\conf_i\)</span> and <span class="math notranslate nohighlight">\(\confhat_i^j\)</span> are the ground truth confidence score and the predicted confidence score respectively,
both for the <strong>grid cell</strong> <span class="math notranslate nohighlight">\(i\)</span>;</p></li>
<li><p>The <span class="math notranslate nohighlight">\(\p_i(c)\)</span> and <span class="math notranslate nohighlight">\(\phat_i(c)\)</span> are the ground truth probability of the class <span class="math notranslate nohighlight">\(c\)</span> and the predicted probability of
the class <span class="math notranslate nohighlight">\(c\)</span> respectively,
both for the <strong>grid cell</strong> <span class="math notranslate nohighlight">\(i\)</span>;</p></li>
</ul>
</section>
<section id="example-with-numbers-part-i">
<h4>Example with Numbers Part I<a class="headerlink" href="#example-with-numbers-part-i" title="Permalink to this headline">#</a></h4>
<p>To be honest, I never understood the above equations without the help of numbers. So let us look at some numbers.
Let’s zoom in on how to calculate loss for one grid cell <span class="math notranslate nohighlight">\(i\)</span>. And for continuity, we will use the
<span class="math notranslate nohighlight">\(i=30\)</span> grid cell as an example, the one which the dog is located in.</p>
<p>We will use back the dataframe row which represent the grid cell <span class="math notranslate nohighlight">\(i=30\)</span>, for both
the ground truth and the predicted bounding boxes.</p>
<div class="cell tag_output_scroll tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>x_i^1</th>
      <th>y_i^1</th>
      <th>w_i^1</th>
      <th>h_i^1</th>
      <th>conf_i^1</th>
      <th>x_i^2</th>
      <th>y_i^2</th>
      <th>w_i^2</th>
      <th>h_i^2</th>
      <th>conf_i^2</th>
      <th>p_1</th>
      <th>p_2</th>
      <th>p_3</th>
      <th>p_4</th>
      <th>p_5</th>
      <th>p_6</th>
      <th>p_7</th>
      <th>p_8</th>
      <th>p_9</th>
      <th>p_10</th>
      <th>p_11</th>
      <th>p_12</th>
      <th>p_13</th>
      <th>p_14</th>
      <th>p_15</th>
      <th>p_16</th>
      <th>p_17</th>
      <th>p_18</th>
      <th>p_19</th>
      <th>p_20</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>0.409</td>
      <td>0.277</td>
      <td>0.416</td>
      <td>0.262</td>
      <td>1.0</td>
      <td>0.409</td>
      <td>0.277</td>
      <td>0.416</td>
      <td>0.262</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell tag_output_scroll tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>xhat_i^1</th>
      <th>yhat_i^1</th>
      <th>what_i^1</th>
      <th>hhat_i^1</th>
      <th>confhat_i^1</th>
      <th>xhat_i^2</th>
      <th>yhat_i^2</th>
      <th>what_i^2</th>
      <th>hhat_i^2</th>
      <th>confhat_i^2</th>
      <th>phat_1</th>
      <th>phat_2</th>
      <th>phat_3</th>
      <th>phat_4</th>
      <th>phat_5</th>
      <th>phat_6</th>
      <th>phat_7</th>
      <th>phat_8</th>
      <th>phat_9</th>
      <th>phat_10</th>
      <th>phat_11</th>
      <th>phat_12</th>
      <th>phat_13</th>
      <th>phat_14</th>
      <th>phat_15</th>
      <th>phat_16</th>
      <th>phat_17</th>
      <th>phat_18</th>
      <th>phat_19</th>
      <th>phat_20</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>-0.483</td>
      <td>-0.351</td>
      <td>-0.595</td>
      <td>0.018</td>
      <td>0.162</td>
      <td>2.688</td>
      <td>2.002</td>
      <td>-1.169</td>
      <td>-4.592</td>
      <td>0.758</td>
      <td>-0.232</td>
      <td>0.527</td>
      <td>-0.256</td>
      <td>-0.283</td>
      <td>0.382</td>
      <td>-0.166</td>
      <td>-0.071</td>
      <td>-0.175</td>
      <td>0.147</td>
      <td>0.017</td>
      <td>-0.415</td>
      <td>1.163</td>
      <td>-0.043</td>
      <td>-0.095</td>
      <td>0.132</td>
      <td>-0.106</td>
      <td>0.269</td>
      <td>0.092</td>
      <td>-0.085</td>
      <td>0.584</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In <a class="reference internal" href="#equation-eq-yolov1-loss-for-single-grid-cell">(8)</a>, we first see the first equation <span class="math notranslate nohighlight">\((a)\)</span>. Let us forget <span class="math notranslate nohighlight">\(\lambda_\textbf{coord}\)</span> for now and just focus on the summand.</p>
<ol class="arabic simple">
<li><p>Inside the summand, there is the indicator function <span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^{obj}\)</span>, and by definition, we first check
if there is a ground truth object in the grid cell <span class="math notranslate nohighlight">\(i=30\)</span>.</p></li>
<li><p>We do know that there is a ground truth object in the grid cell <span class="math notranslate nohighlight">\(i=30\)</span> as the dog is located in this grid cell, as a priori knowledge. But
the code does not. It suffices to check the 5th element in the ground truth bounding box coordinates, corresponding
to the confidence score <span class="math notranslate nohighlight">\(\conf_{30}\)</span> in the dataframe. By construction in <a class="reference internal" href="#yolo_gt_matrix">Definition 6</a>, the confidence
score is 1 if there is a ground truth object in the grid cell, and 0 otherwise. So a quick look at the dataframe
tells us that <span class="math notranslate nohighlight">\(\conf_{30}=1\)</span>. So we can proceed.</p></li>
<li><p>Next, the matching algorithm happens by means of looping through the <span class="math notranslate nohighlight">\(B=2\)</span> bounding box predictors in the grid cell <span class="math notranslate nohighlight">\(i=30\)</span>.
However, we soon realize this is not very easily done in code because at each iteration <span class="math notranslate nohighlight">\(j\)</span> over <span class="math notranslate nohighlight">\(B=2\)</span>, we are only
able to compute the <code class="docutils literal notranslate"><span class="pre">iou(b,</span> <span class="pre">bhat)</span></code> for the <span class="math notranslate nohighlight">\(j\)</span>th bounding box predictor in the grid cell <span class="math notranslate nohighlight">\(i=30\)</span>, but unable
to deduce if the current iou is the highest. We can of course find a way to store the highest iou,
but to make explanation easier, we will remove this loop!</p></li>
</ol>
</section>
<section id="the-formula-modified">
<h4>The Formula Modified<a class="headerlink" href="#the-formula-modified" title="Permalink to this headline">#</a></h4>
<p>As mentioned in point 3, the observant reader would have realized that the summation <span class="math notranslate nohighlight">\(\sum_{j=1}^{B=2}\)</span> can be refactored out of the equation.
This is because for each grid cell <span class="math notranslate nohighlight">\(i\)</span>, there is only <strong>one unique</strong> bounding box predictor that is matched to the ground truth object.
The other bounding box predictor(s) in that same grid cell <span class="math notranslate nohighlight">\(i\)</span> will get the cold shoulder since <span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^{obj}=0\)</span>,
resulting the summand to be <span class="math notranslate nohighlight">\(0\)</span>. Further, it is easier to illustrate the loss function with the summation refactored out in code.
Let’s again take the liberty to modify slightly the notation to make it more explicit.</p>
<p>Let <span class="math notranslate nohighlight">\(\jmax\)</span> be the index of the bounding box with the IOU score with the ground truth <span class="math notranslate nohighlight">\(\y_i\)</span> in grid cell <span class="math notranslate nohighlight">\(i\)</span>.
More concretely, <span class="math notranslate nohighlight">\(\jmax\)</span> is our survivor out of the <span class="math notranslate nohighlight">\(B=2\)</span> bounding box predictors in the grid cell <span class="math notranslate nohighlight">\(i\)</span>, the one
that got successfully matched to the ground truth object in the grid cell <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="proof definition admonition" id="def_jmax">
<p class="admonition-title"><span class="caption-number">Definition 8 </span> (<span class="math notranslate nohighlight">\(\jmax\)</span>)</p>
<section class="definition-content" id="proof-content">
<p><span class="math notranslate nohighlight">\(\jmax\)</span> is the index in the <span class="math notranslate nohighlight">\(B\)</span> predicted bounding boxes which has the highest IOU score with the
ground truth <span class="math notranslate nohighlight">\(\y_i\)</span> in grid cell <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>To be more concise, the IOU score is computed between <span class="math notranslate nohighlight">\(\b_i\)</span> and <span class="math notranslate nohighlight">\(\bhat_i^j\)</span> for <span class="math notranslate nohighlight">\(j=1,2\)</span> as IOU
score is a function of the 4 bounding box coordinates.</p>
<div class="math notranslate nohighlight">
\[
\jmax = \underset{j \in \{1,2\}}{\operatorname{argmax}} \textbf{IOU}(\b_i, \bhat_i^j)
\]</div>
</section>
</div><p>Then we define the new loss function <span class="math notranslate nohighlight">\(\L_i\)</span> for each grid cell <span class="math notranslate nohighlight">\(i\)</span> as follows:</p>
<div class="proof definition admonition" id="def_loss_i_modified">
<p class="admonition-title"><span class="caption-number">Definition 9 </span> (<span class="math notranslate nohighlight">\(\L_i\)</span> Modified!)</p>
<section class="definition-content" id="proof-content">
<p>The modified loss function <span class="math notranslate nohighlight">\(\L_i\)</span> for each grid cell <span class="math notranslate nohighlight">\(i\)</span> is defined as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-yolov1-loss-for-single-grid-cell-modified">
<span class="eqno">(9)<a class="headerlink" href="#equation-eq-yolov1-loss-for-single-grid-cell-modified" title="Permalink to this equation">#</a></span>\[\begin{split}
  \begin{align}
      \L_i(\y_i, \yhat_i) &amp; \overset{(a)}{=}  \color{blue}{\lambda_\textbf{coord} \cdot \obji \lsq \lpar x_i - \hat{x}_i^{\jmax} \rpar^2 + \lpar y_i - \hat{y}_i^{\jmax}  \rpar^2 \rsq}                             \\
                          &amp; \overset{(b)}{+}  \color{blue}{\lambda_\textbf{coord} \cdot \obji \lsq \lpar \sqrt{w_i} - \sqrt{\hat{w}_i^{\jmax} } \rpar^2 + \lpar \sqrt{h_i} - \sqrt{\hat{h}_i^{\jmax} } \rpar^2 \rsq} \\
                          &amp; \overset{(c)}{+}  \color{green}{\obji \lpar \conf_i - \confhat_i^{\jmax} \rpar^2}                                                                                          \\
                          &amp; \overset{(d)}{+} \color{green}{\lambda_\textbf{noobj} \cdot \nobji \lpar \conf_i - \confhat_i^{\jmax} \rpar^2}                                                                  \\
                          &amp; \overset{(e)}{+}  \color{red}{\obji \sum_{c \in \cc} \lpar \p_i(c) - \phat_i(c) \rpar^2}                                                                                               \\
  \end{align}
\end{split}\]</div>
<p>thereby collapsing the equation to checking only two conditions:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\obji\)</span> is <span class="math notranslate nohighlight">\(1\)</span> when there is an object in cell <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(0\)</span> elsewhere</p></li>
<li><p><span class="math notranslate nohighlight">\(\1_{i}^{\text{noobj}}\)</span> is <span class="math notranslate nohighlight">\(1\)</span> when there is no object in cell <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(0\)</span> elsewhere</p></li>
<li><p><span class="math notranslate nohighlight">\(\y_i\)</span> is exactly as defined in <a class="reference internal" href="#yolo_gt_matrix">Definition 6</a>’s equation <a class="reference internal" href="#equation-eq-gt-yi">(3)</a>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\yhat_i\)</span> is exactly as defined in <a class="reference internal" href="#yolo_pred_matrix">Definition 7</a>’s equation <a class="reference internal" href="#equation-eq-gt-yhati">(4)</a>.</p></li>
</ul>
<p>The most significant change is that we are going to pre-compute the IOU score between the ground truth <span class="math notranslate nohighlight">\(\y_i\)</span> (<span class="math notranslate nohighlight">\(\b_i\)</span>)
with each of the <span class="math notranslate nohighlight">\(B=2\)</span> bounding box predictors <span class="math notranslate nohighlight">\(\bhat_i^j\)</span> (<span class="math notranslate nohighlight">\(j=1,2\)</span>) in the grid cell <span class="math notranslate nohighlight">\(i\)</span>, and then pick the
bounding box predictor <span class="math notranslate nohighlight">\(\bhat_i^j\)</span> with the highest IOU score and denote the index to be <span class="math notranslate nohighlight">\(\jmax\)</span>.</p>
</section>
</div></section>
<section id="recap-on-the-modified-loss-function">
<h4>Recap on the Modified Loss Function<a class="headerlink" href="#recap-on-the-modified-loss-function" title="Permalink to this headline">#</a></h4>
<p>Before we dive into numbers, let’s first recap the loss function for a single grid cell <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Let’s briefly go through this term by term, bearing in mind that we are talking about 1 single grid cell and not the whole image.</p>
<ul class="simple">
<li><p>The first part of the equation in <a class="reference internal" href="#equation-eq-original-yolo-loss">(5)</a> computes the loss between the predicted bounding box <span class="math notranslate nohighlight">\(\xx-\yy\)</span>
offsets <span class="math notranslate nohighlight">\((\xxhat_i, \yyhat_i)\)</span> and the ground-truth bounding box <span class="math notranslate nohighlight">\(\xx-\yy\)</span> offsets <span class="math notranslate nohighlight">\((\xx_i, \yy_i)\)</span>.</p>
<ul>
<li><p>However, we are referring to the <strong>original yolo loss</strong> function here, which sums up the total loss for a single image.
In code however, we want to first compute the loss for each grid cell in a single image, and then sum up the losses for all grid cells in the image.</p></li>
<li><p>Therefore, we will use the our <a class="reference internal" href="#def_loss_i_modified">Definition 9</a>, a modified version to find the loss for each grid cell in a single image.</p></li>
<li><p>In the modified version, we see that the first part of the equation <a class="reference internal" href="#equation-eq-yolov1-loss-for-single-grid-cell-modified">(9)</a>
computes the loss between the ground truth bounding box <span class="math notranslate nohighlight">\(\xx-\yy\)</span> offsets <span class="math notranslate nohighlight">\((\xx_i, \yy_i)\)</span>
and the matched predicted bounding box <span class="math notranslate nohighlight">\(\xx-\yy\)</span> offsets <span class="math notranslate nohighlight">\((\xxhat_i^{\jmax}, \yyhat_i^{\jmax})\)</span>.</p></li>
<li><p>Remember carefully that the model predicts <strong>two</strong> bounding boxes for each grid cell,
and we need to match the ground truth bounding box with the predicted bounding box that has the highest IoU
with the ground truth bounding box.</p></li>
<li><p>We multiply the loss by a constant <span class="math notranslate nohighlight">\(\lambda_\text{coord}=5\)</span> to ensure that this equation does not get drowned out by
the fact that most grid cells do not contain an object. In our example, out of 49 grid cells, only 2 grid cells contains ground truth objects.</p></li>
<li><p><strong>Most importantly, this equation only gets computed for grid cells that contain a ground truth object,
as indicated by <span class="math notranslate nohighlight">\(\obji\)</span></strong>.</p></li>
</ul>
</li>
<li><p>The second part of the equation in <a class="reference internal" href="#equation-eq-original-yolo-loss">(5)</a> computes the loss between the predicted bounding box <span class="math notranslate nohighlight">\(\ww-\hh\)</span>
dimensions <span class="math notranslate nohighlight">\((\wwhat_i, \hhhat_i)\)</span> and the ground-truth bounding box <span class="math notranslate nohighlight">\(\ww-\hh\)</span> dimensions <span class="math notranslate nohighlight">\((\ww_i, \hh_i)\)</span>.</p>
<ul>
<li><p>Again, we are referring to the <strong>original yolo loss</strong> function here, which sums up the total loss for a single image.
In code however, we want to first compute the loss for each grid cell in a single image, and then sum up the losses for all grid cells in the image.</p></li>
<li><p>Therefore, we will use the our <a class="reference internal" href="#def_loss_i_modified">Definition 9</a>, a modified version to find the loss for each grid cell in a single image.</p></li>
<li><p>In the modified version, we see that the second part of the equation <a class="reference internal" href="#equation-eq-yolov1-loss-for-single-grid-cell-modified">(9)</a>
computes the loss between the ground truth bounding box <span class="math notranslate nohighlight">\(\ww-\hh\)</span> dimensions <span class="math notranslate nohighlight">\((\ww_i, \hh_i)\)</span>
and the matched predicted bounding box <span class="math notranslate nohighlight">\(\ww-\hh\)</span> dimensions <span class="math notranslate nohighlight">\((\wwhat_i^{\jmax}, \hhhat_i^{\jmax})\)</span>.</p></li>
<li><p>There is a catch, however. We are actually computing the loss between the square root of the ground truth bounding box <span class="math notranslate nohighlight">\(\ww-\hh\)</span> dimensions <span class="math notranslate nohighlight">\(\sqrt{\ww_i}, \sqrt{\hh_i}\)</span>
and the square root of the matched predicted bounding box <span class="math notranslate nohighlight">\(\ww-\hh\)</span> dimensions <span class="math notranslate nohighlight">\(\sqrt{\wwhat_i^{\jmax}}, \sqrt{\hhhat_i^{\jmax}}\)</span>.
The reason can be found in my <strong>Intuition: Parametrization of Bounding Box</strong>.</p></li>
<li><p>Remember carefully that the model predicts <strong>two</strong> bounding boxes for each grid cell,
and we need to match the ground truth bounding box with the predicted bounding box that has the highest IoU
with the ground truth bounding box.</p></li>
<li><p>We multiply the loss by a constant <span class="math notranslate nohighlight">\(\lambda_\text{coord}=5\)</span> to ensure that this equation does not get drowned out by
the fact that most grid cells do not contain an object. In our example, out of 49 grid cells, only 2 grid cells contains ground truth objects.</p></li>
<li><p><strong>Most importantly, this equation only gets computed for grid cells that contain a ground truth object,
as indicated by <span class="math notranslate nohighlight">\(\obji\)</span></strong>.</p></li>
</ul>
</li>
<li><p>The third part of the equation in <a class="reference internal" href="#equation-eq-original-yolo-loss">(5)</a> computes the loss between the predicted object confidence <span class="math notranslate nohighlight">\(\confhat_i\)</span>
and the ground-truth object confidence <span class="math notranslate nohighlight">\(\conf_i\)</span>.</p>
<ul>
<li><p>We see that the third part of the equation <a class="reference internal" href="#equation-eq-yolov1-loss-for-single-grid-cell-modified">(9)</a> computes the loss between the ground truth object confidence <span class="math notranslate nohighlight">\(\conf_i\)</span>
and the matched predicted object confidence <span class="math notranslate nohighlight">\(\confhat_i^{\jmax}\)</span>.</p></li>
<li><p>Remember carefully that the model predicts <strong>two</strong> object confidences for each grid cell,
and we need to match the ground truth object confidence with the predicted object confidence that has the highest IoU
with the ground truth bounding box.</p></li>
<li><p>Recall that the ground truth <span class="math notranslate nohighlight">\(\conf_i\)</span> is defined as 1 if the grid cell contains an object, and 0 otherwise initially,
but during loss computation, <span class="math notranslate nohighlight">\(\conf_i\)</span> is defined to be the IoU between the ground truth bounding box and the predicted bounding box that has the highest IoU,
as seen in <a class="reference internal" href="#gt-confidence">Definition 5</a>. But for our code, we will just use <code class="docutils literal notranslate"><span class="pre">conf_i</span> <span class="pre">=</span> <span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">conf_i</span> <span class="pre">=</span> <span class="pre">0</span></code> for simplicity.</p></li>
<li><p><strong>Most importantly, this equation only gets computed for grid cells that contain a ground truth object,
as indicated by <span class="math notranslate nohighlight">\(\obji\)</span></strong>.</p></li>
</ul>
</li>
<li><p>The fourth part of the equation in <a class="reference internal" href="#equation-eq-original-yolo-loss">(5)</a> computes the loss between the predicted object confidence <span class="math notranslate nohighlight">\(\confhat_i\)</span>
and the ground-truth object confidence <span class="math notranslate nohighlight">\(\conf_i\)</span> when the <em><strong>grid cell does not contain an object</strong></em>.</p>
<ul>
<li><p>Even though in our fourth part of the equation <a class="reference internal" href="#equation-eq-yolov1-loss-for-single-grid-cell-modified">(9)</a>,
we put a <span class="math notranslate nohighlight">\(\jmax\)</span> subscript, but in code we actually compute the loss for all predicted bounding boxes in the grid cell.
This is logical as well because if the grid cell does not contain an object, then the predicted bounding boxes
should both have a low object confidence and be penalized to tell the model that the grid cell does not contain an object.</p></li>
<li><p>We multiply the loss by a constant <span class="math notranslate nohighlight">\(\lambda_\text{noobj}=0.5\)</span> to ensure that this equation does not get drowned out by
the fact that most grid cells do not contain an object.
We do not want this term to overpower the gradients for the cells containing objects
as this could lead to model instability and be harder to optimize <span id="id10">[<a class="reference internal" href="#id13" title="Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: unified, real-time object detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. doi:10.1109/cvpr.2016.91.">Redmon <em>et al.</em>, 2016</a>]</span>.</p></li>
</ul>
</li>
<li><p>The fifth part of the equation in <a class="reference internal" href="#equation-eq-original-yolo-loss">(5)</a> computes the loss between the predicted class probabilities <span class="math notranslate nohighlight">\(\phat_i\)</span>
and the ground-truth class probabilities <span class="math notranslate nohighlight">\(\p_i\)</span>.</p>
<ul>
<li><p>We see that the fifth part of the equation <a class="reference internal" href="#equation-eq-yolov1-loss-for-single-grid-cell-modified">(9)</a> computes the loss between the ground truth class probabilities <span class="math notranslate nohighlight">\(\p_i\)</span>
and the matched predicted class probabilities <span class="math notranslate nohighlight">\(\phat_i\)</span>.</p></li>
<li><p>Note that <span class="math notranslate nohighlight">\(\p_i\)</span> and <span class="math notranslate nohighlight">\(\phat_i\)</span> are both vectors of length <span class="math notranslate nohighlight">\(C\)</span>, where <span class="math notranslate nohighlight">\(C\)</span> is the number of classes. They are not scalars unlike the previous equations.</p></li>
<li><p>Note that we do not have <span class="math notranslate nohighlight">\(\jmax\)</span> here because both predicted bounding boxes share the same class probabilities.</p></li>
<li><p><strong>Most importantly, this equation only gets computed for grid cells that contain a ground truth object,
as indicated by <span class="math notranslate nohighlight">\(\obji\)</span>. This is why the author call them <em><strong>conditional probabilities</strong></em>,
<span class="math notranslate nohighlight">\(\mathcal{P} \left( \p_i \mid \obji = 1 \right)\)</span></strong>. It is only computed conditionally on the fact that the grid cell contains an object.</p></li>
</ul>
</li>
</ul>
<p>In summary:</p>
<p>After the forward pass, the model will have encoded the ground truth <span class="math notranslate nohighlight">\(\b_30\)</span> and
the two predicted bounding boxes <span class="math notranslate nohighlight">\(\bhat_i^1\)</span> and <span class="math notranslate nohighlight">\(\bhat_i^2\)</span>. Our first step is to
determine which of the two predicted bounding boxes is the best match for the ground truth,
the filtering process is done by the IOU metric.</p>
<p>Assuming that the matching is done, for the predicted boxes that are matched to ground truth boxes
the loss function is minimizing the error between those boxes,
maximising the objectness confidence, and maximising the liklihood of the correct class
(which is shared between two boxes).
For all predicted boxes that are not matched with a ground truth box,
it is minimising the objectness confidence, but ignoring the box coordinates
and class probabilities <span id="id11">[<a class="reference internal" href="#id14" title="Harry Turner. Yolo v1. 2021. URL: https://www.harrysprojects.com/articles/yolov1.html.">Turner, 2021</a>]</span>.</p>
</section>
<section id="examples-with-numbers-part-ii">
<h4>Examples with Numbers Part II<a class="headerlink" href="#examples-with-numbers-part-ii" title="Permalink to this headline">#</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">  2</span><span class="sd">Implementation of Yolo Loss Function from the original yolo paper</span>
<span class="linenos">  3</span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">  4</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="linenos">  5</span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="linenos">  6</span><span class="c1"># from utils import intersection_over_union, bmatrix</span>
<span class="linenos">  7</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos">  8</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="linenos">  9</span>
<span class="linenos"> 10</span><span class="c1"># reshape to [49, 30] from [7, 7, 30]</span>
<span class="linenos"> 11</span><span class="k">class</span> <span class="nc">YOLOv1Loss2D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="linenos"> 12</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<span class="linenos"> 13</span>        <span class="bp">self</span><span class="p">,</span>
<span class="linenos"> 14</span>        <span class="n">S</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="linenos"> 15</span>        <span class="n">B</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="linenos"> 16</span>        <span class="n">C</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="linenos"> 17</span>        <span class="n">lambda_coord</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
<span class="linenos"> 18</span>        <span class="n">lambda_noobj</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
<span class="linenos"> 19</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 20</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="linenos"> 21</span>        <span class="bp">self</span><span class="o">.</span><span class="n">S</span> <span class="o">=</span> <span class="n">S</span>
<span class="linenos"> 22</span>        <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span>
<span class="linenos"> 23</span>        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
<span class="linenos"> 24</span>        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_coord</span> <span class="o">=</span> <span class="n">lambda_coord</span>
<span class="linenos"> 25</span>        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_noobj</span> <span class="o">=</span> <span class="n">lambda_noobj</span>
<span class="linenos"> 26</span>        <span class="c1"># mse = (y_pred - y_true)^2</span>
<span class="linenos"> 27</span>        <span class="c1"># FIXME: still unclean cause by right reduction is not sum since we are</span>
<span class="linenos"> 28</span>        <span class="c1"># adding scalars, but class_loss uses vector sum reduction so need to use for all?</span>
<span class="linenos"> 29</span>        <span class="bp">self</span><span class="o">.</span><span class="n">mse</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
<span class="linenos"> 30</span>        
<span class="linenos"> 31</span>        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="linenos"> 32</span>
<span class="linenos"> 33</span>    <span class="k">def</span> <span class="nf">_initiate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 34</span>        <span class="sd">&quot;&quot;&quot;Initializes all the loss values to 0.</span>
<span class="linenos"> 35</span><span class="sd">        </span>
<span class="linenos"> 36</span><span class="sd">        Note:</span>
<span class="linenos"> 37</span><span class="sd">            This is an important step if not the current batch loss will be added to the</span>
<span class="linenos"> 38</span><span class="sd">            next batch loss which causes error in `loss.backward()`.</span>
<span class="linenos"> 39</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos"> 40</span>        <span class="c1"># bbox loss</span>
<span class="linenos"> 41</span>        <span class="bp">self</span><span class="o">.</span><span class="n">bbox_xy_offset_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos"> 42</span>        <span class="bp">self</span><span class="o">.</span><span class="n">bbox_wh_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos"> 43</span>
<span class="linenos"> 44</span>        <span class="c1"># objectness loss</span>
<span class="linenos"> 45</span>        <span class="bp">self</span><span class="o">.</span><span class="n">object_conf_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos"> 46</span>        <span class="bp">self</span><span class="o">.</span><span class="n">no_object_conf_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span>        <span class="c1"># class loss</span>
<span class="linenos"> 49</span>        <span class="bp">self</span><span class="o">.</span><span class="n">class_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos"> 50</span>
<span class="linenos"> 51</span>    <span class="k">def</span> <span class="nf">compute_xy_offset_loss</span><span class="p">(</span>
<span class="linenos"> 52</span>        <span class="bp">self</span><span class="p">,</span>
<span class="linenos"> 53</span>        <span class="n">x_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="linenos"> 54</span>        <span class="n">xhat_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="linenos"> 55</span>        <span class="n">y_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="linenos"> 56</span>        <span class="n">yhat_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="linenos"> 57</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos"> 58</span>        <span class="sd">&quot;&quot;&quot;Computes the loss for the x and y offset of the bounding box.&quot;&quot;&quot;</span>
<span class="linenos"> 59</span>        <span class="n">x_offset_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">xhat_i</span><span class="p">)</span>
<span class="linenos"> 60</span>        <span class="n">y_offset_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">yhat_i</span><span class="p">)</span>
<span class="linenos"> 61</span>        <span class="n">xy_offset_loss</span> <span class="o">=</span> <span class="n">x_offset_loss</span> <span class="o">+</span> <span class="n">y_offset_loss</span>
<span class="linenos"> 62</span>        <span class="k">return</span> <span class="n">xy_offset_loss</span>
<span class="linenos"> 63</span>
<span class="linenos"> 64</span>    <span class="k">def</span> <span class="nf">compute_wh_loss</span><span class="p">(</span>
<span class="linenos"> 65</span>        <span class="bp">self</span><span class="p">,</span>
<span class="linenos"> 66</span>        <span class="n">w_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="linenos"> 67</span>        <span class="n">what_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="linenos"> 68</span>        <span class="n">h_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="linenos"> 69</span>        <span class="n">hhat_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="linenos"> 70</span>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
<span class="linenos"> 71</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos"> 72</span>        <span class="sd">&quot;&quot;&quot;Computes the loss for the width and height of the bounding box.</span>
<span class="linenos"> 73</span><span class="sd">        </span>
<span class="linenos"> 74</span><span class="sd">        Note:</span>
<span class="linenos"> 75</span><span class="sd">            The width and height are predicted as the square root of the width and height</span>
<span class="linenos"> 76</span><span class="sd">            and absolute values are applied to the predictions as they are unbounded</span>
<span class="linenos"> 77</span><span class="sd">            below and can be numerically unstable.</span>
<span class="linenos"> 78</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos"> 79</span>        <span class="n">w_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">w_i</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">what_i</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)))</span>
<span class="linenos"> 80</span>        <span class="n">h_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">h_i</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">hhat_i</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)))</span>
<span class="linenos"> 81</span>        <span class="n">wh_loss</span> <span class="o">=</span> <span class="n">w_loss</span> <span class="o">+</span> <span class="n">h_loss</span>
<span class="linenos"> 82</span>        <span class="k">return</span> <span class="n">wh_loss</span>
<span class="linenos"> 83</span>
<span class="linenos"> 84</span>    <span class="k">def</span> <span class="nf">compute_object_conf_loss</span><span class="p">(</span>
<span class="linenos"> 85</span>        <span class="bp">self</span><span class="p">,</span> <span class="n">conf_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">confhat_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="linenos"> 86</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos"> 87</span>        <span class="sd">&quot;&quot;&quot;Computes the loss for the object confidence when there is really an object.&quot;&quot;&quot;</span>
<span class="linenos"> 88</span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">conf_i</span><span class="p">,</span> <span class="n">confhat_i</span><span class="p">)</span>
<span class="linenos"> 89</span>
<span class="linenos"> 90</span>    <span class="k">def</span> <span class="nf">compute_no_object_conf_loss</span><span class="p">(</span>
<span class="linenos"> 91</span>        <span class="bp">self</span><span class="p">,</span> <span class="n">conf_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">confhat_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="linenos"> 92</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos"> 93</span>        <span class="sd">&quot;&quot;&quot;Computes the loss for the object confidence when there is no object.&quot;&quot;&quot;</span>
<span class="linenos"> 94</span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">conf_i</span><span class="p">,</span> <span class="n">confhat_i</span><span class="p">)</span>
<span class="linenos"> 95</span>
<span class="linenos"> 96</span>    <span class="k">def</span> <span class="nf">compute_class_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">phat_i</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos"> 97</span>        <span class="sd">&quot;&quot;&quot;Computes the loss for the class prediction.</span>
<span class="linenos"> 98</span>
<span class="linenos"> 99</span><span class="sd">        Note:</span>
<span class="linenos">100</span><span class="sd">            Instead of looping C number of classes, we can use self.mse(p_i, phat_i)</span>
<span class="linenos">101</span><span class="sd">            as the vectorized version.</span>
<span class="linenos">102</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">103</span>        <span class="n">total_class_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos">104</span>        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">):</span>
<span class="linenos">105</span>            <span class="n">total_class_loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">p_i</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="n">phat_i</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>
<span class="linenos">106</span>        <span class="k">return</span> <span class="n">total_class_loss</span>
<span class="linenos">107</span>
<span class="linenos">108</span>    <span class="c1"># fmt: off</span>
<span class="linenos">109</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_trues</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos">110</span>        <span class="sd">&quot;&quot;&quot;Forward pass of the loss function.</span>
<span class="linenos">111</span>
<span class="linenos">112</span><span class="sd">        Args:</span>
<span class="linenos">113</span><span class="sd">            y_trues (torch.Tensor): The ground truth tensor of shape (bs, S, S, 5B + C).</span>
<span class="linenos">114</span><span class="sd">            y_preds (torch.Tensor): The predicted tensor of shape (bs, S, S, 5B + C).</span>
<span class="linenos">115</span>
<span class="linenos">116</span><span class="sd">        Returns:</span>
<span class="linenos">117</span><span class="sd">            total_loss_averaged_over_batch (torch.Tensor): The total loss averaged over the batch.</span>
<span class="linenos">118</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">119</span>        <span class="bp">self</span><span class="o">.</span><span class="n">_initiate_loss</span><span class="p">()</span>                                                                       <span class="c1"># reset loss values</span>
<span class="linenos">120</span>        
<span class="linenos">121</span>        <span class="n">y_trues</span> <span class="o">=</span> <span class="n">y_trues</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>                         <span class="c1"># (4, 49, 30)</span>
<span class="linenos">122</span>        <span class="n">y_preds</span> <span class="o">=</span> <span class="n">y_preds</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>                         <span class="c1"># (4, 49, 30)</span>
<span class="linenos">123</span>
<span class="linenos">124</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y_preds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>                                                               <span class="c1"># 4</span>
<span class="linenos">125</span>
<span class="linenos">126</span>        <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>                                                       <span class="c1"># for each image in batch </span>
<span class="linenos">127</span>            <span class="n">y_true</span> <span class="o">=</span> <span class="n">y_trues</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span>                                                           <span class="c1"># y:    (49, 30)</span>
<span class="linenos">128</span>            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span>                                                           <span class="c1"># yhat: (49, 30)</span>
<span class="linenos">129</span>
<span class="linenos">130</span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">):</span>                                                        <span class="c1"># i is the grid cell index [0, 48] </span>
<span class="linenos">131</span>                <span class="n">y_true_i</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>                                                                <span class="c1"># y_i:    (30,) or (1, 30)</span>
<span class="linenos">132</span>                <span class="n">y_pred_i</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>                                                                <span class="c1"># yhat_i: (30,) or (1, 30)</span>
<span class="linenos">133</span>                
<span class="linenos">134</span>                <span class="n">indicator_obj_i</span> <span class="o">=</span> <span class="n">y_true_i</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>                                                  <span class="c1"># this is $\obji$ and checking y_true[i, 4] is sufficient since y_true[i, 9] is repeated</span>
<span class="linenos">135</span>
<span class="linenos">136</span>                <span class="k">if</span> <span class="n">indicator_obj_i</span><span class="p">:</span>                                                                 <span class="c1"># here equation (a), (b), (c) and (e) of the loss equation on a single grid cell.</span>
<span class="linenos">137</span>                    <span class="n">b</span> <span class="o">=</span> <span class="n">y_true_i</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>                                                               <span class="c1"># b:    (4,) or (1, 4)</span>
<span class="linenos">138</span>                    <span class="n">bhat_1</span> <span class="o">=</span> <span class="n">y_pred_i</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>                                                          <span class="c1"># bhat1: (4,) or (1, 4)</span>
<span class="linenos">139</span>                    <span class="n">bhat_2</span> <span class="o">=</span> <span class="n">y_pred_i</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="mi">9</span><span class="p">]</span>                                                          <span class="c1"># bhat2: (4,) or (1, 4)</span>
<span class="linenos">140</span>                    
<span class="linenos">141</span>                    <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">w_i</span><span class="p">,</span> <span class="n">h_i</span> <span class="o">=</span> <span class="n">b</span>                                                          <span class="c1"># x_i, y_i, w_i, h_i: (1,)</span>
<span class="linenos">142</span>                    <span class="c1"># at this stage jmax is not known yet.</span>
<span class="linenos">143</span>                    <span class="n">xhat_i1</span><span class="p">,</span> <span class="n">yhat_i1</span><span class="p">,</span> <span class="n">what_i1</span><span class="p">,</span> <span class="n">hhat_i1</span> <span class="o">=</span> <span class="n">bhat_1</span>                                     <span class="c1"># xhat_i1, yhat_i1, what_i1, hhat_i1: (1,)</span>
<span class="linenos">144</span>                    <span class="n">xhat_i2</span><span class="p">,</span> <span class="n">yhat_i2</span><span class="p">,</span> <span class="n">what_i2</span><span class="p">,</span> <span class="n">hhat_i2</span> <span class="o">=</span> <span class="n">bhat_2</span>                                     <span class="c1"># xhat_i2, yhat_i2, what_i2, hhat_i2: (1,)</span>
<span class="linenos">145</span>                    
<span class="linenos">146</span>                    <span class="n">conf_i</span><span class="p">,</span> <span class="n">confhat_i1</span><span class="p">,</span> <span class="n">confhat_i2</span> <span class="o">=</span> <span class="n">y_true_i</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">y_pred_i</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">y_pred_i</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span>          <span class="c1"># conf_i, confhat_i1, confhat_i2: (1,)</span>
<span class="linenos">147</span>                    
<span class="linenos">148</span>                    <span class="n">p_i</span><span class="p">,</span> <span class="n">phat_i</span> <span class="o">=</span> <span class="n">y_true_i</span><span class="p">[</span><span class="mi">10</span><span class="p">:],</span> <span class="n">y_pred_i</span><span class="p">[</span><span class="mi">10</span><span class="p">:]</span>                                      <span class="c1"># p_i, phat_i: (20,) or (1, 20)</span>
<span class="linenos">149</span>
<span class="linenos">150</span>                    <span class="c1"># area of overlap</span>
<span class="linenos">151</span>                    <span class="n">iou_b1</span> <span class="o">=</span> <span class="n">intersection_over_union</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">bhat_1</span><span class="p">,</span> <span class="n">bbox_format</span><span class="o">=</span><span class="s2">&quot;yolo&quot;</span><span class="p">)</span>                 <span class="c1"># iou of b and bhat1</span>
<span class="linenos">152</span>                    <span class="n">iou_b2</span> <span class="o">=</span> <span class="n">intersection_over_union</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">bhat_2</span><span class="p">,</span> <span class="n">bbox_format</span><span class="o">=</span><span class="s2">&quot;yolo&quot;</span><span class="p">)</span>                 <span class="c1"># iou of b and bhat2</span>
<span class="linenos">153</span>
<span class="linenos">154</span>                    <span class="k">if</span> <span class="n">iou_b1</span> <span class="o">&gt;</span> <span class="n">iou_b2</span><span class="p">:</span>
<span class="linenos">155</span>                        <span class="c1"># conf_i = max_{bhat \in {bhat_1, bhat_2}} IoU(b, bhat)</span>
<span class="linenos">156</span>                        <span class="c1"># however I set conf_i = y_true_i[4] = 1 here as it gives better results for our case</span>
<span class="linenos">157</span>                        <span class="n">xhat_i_jmax</span><span class="p">,</span> <span class="n">yhat_i_jmax</span><span class="p">,</span> <span class="n">what_i_jmax</span><span class="p">,</span> <span class="n">hhat_i_jmax</span><span class="p">,</span> <span class="n">confhat_i_jmax</span> <span class="o">=</span> <span class="n">xhat_i1</span><span class="p">,</span> <span class="n">yhat_i1</span><span class="p">,</span> <span class="n">what_i1</span><span class="p">,</span> <span class="n">hhat_i1</span><span class="p">,</span> <span class="n">confhat_i1</span>
<span class="linenos">158</span>                        <span class="n">confhat_i_complement</span> <span class="o">=</span> <span class="n">confhat_i2</span>
<span class="linenos">159</span>                    <span class="k">else</span><span class="p">:</span>
<span class="linenos">160</span>                        <span class="n">xhat_i_jmax</span><span class="p">,</span> <span class="n">yhat_i_jmax</span><span class="p">,</span> <span class="n">what_i_jmax</span><span class="p">,</span> <span class="n">hhat_i_jmax</span><span class="p">,</span> <span class="n">confhat_i_jmax</span> <span class="o">=</span> <span class="n">xhat_i2</span><span class="p">,</span> <span class="n">yhat_i2</span><span class="p">,</span> <span class="n">what_i2</span><span class="p">,</span> <span class="n">hhat_i2</span><span class="p">,</span> <span class="n">confhat_i2</span>
<span class="linenos">161</span>                        <span class="n">confhat_i_complement</span> <span class="o">=</span> <span class="n">confhat_i1</span>
<span class="linenos">162</span>                        
<span class="linenos">163</span>                    
<span class="linenos">164</span>                    <span class="bp">self</span><span class="o">.</span><span class="n">bbox_xy_offset_loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_coord</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_xy_offset_loss</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">xhat_i_jmax</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">yhat_i_jmax</span><span class="p">)</span>                 <span class="c1"># equation 1</span>
<span class="linenos">165</span>                    <span class="bp">self</span><span class="o">.</span><span class="n">bbox_wh_loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_coord</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_wh_loss</span><span class="p">(</span><span class="n">w_i</span><span class="p">,</span> <span class="n">what_i_jmax</span><span class="p">,</span> <span class="n">h_i</span><span class="p">,</span> <span class="n">hhat_i_jmax</span><span class="p">)</span>                               <span class="c1"># equation 2</span>
<span class="linenos">166</span>                    <span class="bp">self</span><span class="o">.</span><span class="n">object_conf_loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_object_conf_loss</span><span class="p">(</span><span class="n">conf_i</span><span class="p">,</span> <span class="n">confhat_i_jmax</span><span class="p">)</span>                                                  <span class="c1"># equation 3</span>
<span class="linenos">167</span>
<span class="linenos">168</span>                    <span class="c1"># mention 2 other ways</span>
<span class="linenos">169</span>                    <span class="c1"># iou比较小的bbox不负责预测物体，因此confidence loss算在noobj中，注意，对于标签的置信度应该是iou2</span>
<span class="linenos">170</span>                    <span class="c1"># we can set conf_i = iou_b2 as well as the smaller of the two should be optimized to say there exist no object instead of proposing something.</span>
<span class="linenos">171</span>                    <span class="c1"># we can set conf_i = 0 as well and it will work.</span>
<span class="linenos">172</span>                    <span class="c1"># TODO: comment for blog if uncomment it performs a bit better early.</span>
<span class="linenos">173</span>                    <span class="c1"># self.no_object_conf_loss += self.lambda_noobj * self.compute_no_object_conf_loss(conf_i=torch.tensor(0., device=&quot;cuda&quot;), confhat_i=confhat_i_complement)</span>
<span class="linenos">174</span>                    <span class="bp">self</span><span class="o">.</span><span class="n">class_loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_class_loss</span><span class="p">(</span><span class="n">p_i</span><span class="p">,</span> <span class="n">phat_i</span><span class="p">)</span>                                                                               <span class="c1"># equation 5</span>
<span class="linenos">175</span>                <span class="k">else</span><span class="p">:</span>
<span class="linenos">176</span>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">):</span>
<span class="linenos">177</span>                        <span class="bp">self</span><span class="o">.</span><span class="n">no_object_conf_loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_noobj</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_no_object_conf_loss</span><span class="p">(</span><span class="n">conf_i</span><span class="o">=</span><span class="n">y_true_i</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">confhat_i</span><span class="o">=</span><span class="n">y_pred_i</span><span class="p">[</span><span class="mi">4</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="mi">5</span><span class="p">])</span> <span class="c1"># equation 4</span>
<span class="linenos">178</span>
<span class="linenos">179</span>        <span class="n">total_loss</span> <span class="o">=</span> <span class="p">(</span>
<span class="linenos">180</span>            <span class="bp">self</span><span class="o">.</span><span class="n">bbox_xy_offset_loss</span>
<span class="linenos">181</span>            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bbox_wh_loss</span>
<span class="linenos">182</span>            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">object_conf_loss</span>
<span class="linenos">183</span>            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_object_conf_loss</span>
<span class="linenos">184</span>            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_loss</span>
<span class="linenos">185</span>        <span class="p">)</span>
<span class="linenos">186</span>
<span class="linenos">187</span>        <span class="c1"># if dataloader has a batch size of 2, then our total loss is the average of the two losses.</span>
<span class="linenos">188</span>        <span class="c1"># i.e. total_loss = (loss1 + loss2) / 2 where loss1 is the loss for the first image in the</span>
<span class="linenos">189</span>        <span class="c1"># batch and loss2 is the loss for the second image in the batch.</span>
<span class="linenos">190</span>        <span class="n">total_loss_averaged_over_batch</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">batch_size</span>
<span class="linenos">191</span>        <span class="c1"># print(f&quot;total_loss_averaged_over_batch {total_loss_averaged_over_batch}&quot;)</span>
<span class="linenos">192</span>
<span class="linenos">193</span>        <span class="k">return</span> <span class="n">total_loss_averaged_over_batch</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_output_scroll tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>x_i^1</th>
      <th>y_i^1</th>
      <th>w_i^1</th>
      <th>h_i^1</th>
      <th>conf_i^1</th>
      <th>x_i^2</th>
      <th>y_i^2</th>
      <th>w_i^2</th>
      <th>h_i^2</th>
      <th>conf_i^2</th>
      <th>p_1</th>
      <th>p_2</th>
      <th>p_3</th>
      <th>p_4</th>
      <th>p_5</th>
      <th>p_6</th>
      <th>p_7</th>
      <th>p_8</th>
      <th>p_9</th>
      <th>p_10</th>
      <th>p_11</th>
      <th>p_12</th>
      <th>p_13</th>
      <th>p_14</th>
      <th>p_15</th>
      <th>p_16</th>
      <th>p_17</th>
      <th>p_18</th>
      <th>p_19</th>
      <th>p_20</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>0.409</td>
      <td>0.277</td>
      <td>0.416</td>
      <td>0.262</td>
      <td>1.0</td>
      <td>0.409</td>
      <td>0.277</td>
      <td>0.416</td>
      <td>0.262</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell tag_output_scroll tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>xhat_i^1</th>
      <th>yhat_i^1</th>
      <th>what_i^1</th>
      <th>hhat_i^1</th>
      <th>confhat_i^1</th>
      <th>xhat_i^2</th>
      <th>yhat_i^2</th>
      <th>what_i^2</th>
      <th>hhat_i^2</th>
      <th>confhat_i^2</th>
      <th>phat_1</th>
      <th>phat_2</th>
      <th>phat_3</th>
      <th>phat_4</th>
      <th>phat_5</th>
      <th>phat_6</th>
      <th>phat_7</th>
      <th>phat_8</th>
      <th>phat_9</th>
      <th>phat_10</th>
      <th>phat_11</th>
      <th>phat_12</th>
      <th>phat_13</th>
      <th>phat_14</th>
      <th>phat_15</th>
      <th>phat_16</th>
      <th>phat_17</th>
      <th>phat_18</th>
      <th>phat_19</th>
      <th>phat_20</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>-0.483</td>
      <td>-0.351</td>
      <td>-0.595</td>
      <td>0.018</td>
      <td>0.162</td>
      <td>2.688</td>
      <td>2.002</td>
      <td>-1.169</td>
      <td>-4.592</td>
      <td>0.758</td>
      <td>-0.232</td>
      <td>0.527</td>
      <td>-0.256</td>
      <td>-0.283</td>
      <td>0.382</td>
      <td>-0.166</td>
      <td>-0.071</td>
      <td>-0.175</td>
      <td>0.147</td>
      <td>0.017</td>
      <td>-0.415</td>
      <td>1.163</td>
      <td>-0.043</td>
      <td>-0.095</td>
      <td>0.132</td>
      <td>-0.106</td>
      <td>0.269</td>
      <td>0.092</td>
      <td>-0.085</td>
      <td>0.584</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>1st image in the batch has objects in row 24 and 30 (human, dog at 3,3 and 2,4)</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">x-x</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">y_trues</span></code> consists of <span class="math notranslate nohighlight">\(4\)</span> ground truth bounding boxes of shape <code class="docutils literal notranslate"><span class="pre">(4,</span> <span class="pre">49,</span> <span class="pre">30)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_preds</span></code> consists of <span class="math notranslate nohighlight">\(4\)</span> predicted bounding boxes of shape <code class="docutils literal notranslate"><span class="pre">(4,</span> <span class="pre">49,</span> <span class="pre">30)</span></code>.</p></li>
<li><p>These two variables are zipped into batches of <span class="math notranslate nohighlight">\(N\)</span> for deep learning computation but we will
only focus on the first image of the batch.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">x</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is the number of images in the batch, in this case <span class="math notranslate nohighlight">\(N = 4\)</span>.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">x</span></code> says <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">batch_index</span> <span class="pre">in</span> <span class="pre">range(batch_size):</span></code> will loop over the first image in the batch.</p>
<ul>
<li><p>when <code class="docutils literal notranslate"><span class="pre">batch_index=0</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">y_true</span></code> is the ground truth matrix of shape <code class="docutils literal notranslate"><span class="pre">(49,</span> <span class="pre">30)</span></code>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_pred</span></code> is the predicted matrix of shape <code class="docutils literal notranslate"><span class="pre">(49,</span> <span class="pre">30)</span></code>;</p></li>
<li><p>loop over grid cells from <code class="docutils literal notranslate"><span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">48</span></code>:</p>
<ul>
<li><p>when <code class="docutils literal notranslate"><span class="pre">i=0</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">y_true_i</span></code> is the ground truth matrix of shape <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">30)</span></code> for the cell <code class="docutils literal notranslate"><span class="pre">i=0</span></code>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_true_i</span> <span class="pre">=</span> <span class="pre">[0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">...,</span> <span class="pre">0]</span></code> a zero vector because there is no ground truth object in the cell <code class="docutils literal notranslate"><span class="pre">i=0</span></code>
by <a class="reference internal" href="#yolo_gt_matrix">Definition 6</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_pred_i</span></code> is the predicted matrix of shape <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">30)</span></code> for the cell <code class="docutils literal notranslate"><span class="pre">i=0</span></code>;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="mf">0.9248</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1671</span><span class="p">,</span>  <span class="mf">0.1906</span><span class="p">,</span>  <span class="mf">0.7233</span><span class="p">,</span>  <span class="mf">0.2136</span><span class="p">,</span>  <span class="mf">1.0013</span><span class="p">,</span>  <span class="mf">0.1228</span><span class="p">,</span>  <span class="mf">0.0577</span><span class="p">,</span> <span class="mf">0.5615</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1186</span><span class="p">,</span>
<span class="o">-</span><span class="mf">0.5857</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0233</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1024</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1603</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4082</span><span class="p">,</span>  <span class="mf">0.5643</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1626</span><span class="p">,</span>  <span class="mf">0.2941</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6844</span><span class="p">,</span>
<span class="o">-</span><span class="mf">0.2154</span><span class="p">,</span>  <span class="mf">0.3397</span><span class="p">,</span>  <span class="mf">0.4247</span><span class="p">,</span>  <span class="mf">0.1387</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4165</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2329</span><span class="p">,</span>  <span class="mf">0.2822</span><span class="p">,</span>  <span class="mf">0.5175</span><span class="p">,</span>  <span class="mf">0.3936</span><span class="p">,</span>  <span class="mf">0.0671</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">indicator_obj_i</span></code> corresponds to <span class="math notranslate nohighlight">\(\obji\)</span> in the equation above and is
equal to <span class="math notranslate nohighlight">\(0\)</span> when there is no object in cell <span class="math notranslate nohighlight">\(i=0\)</span>. In this grid cell <code class="docutils literal notranslate"><span class="pre">i=0</span></code>,
this will be equal to <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">x</span></code> says <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">indicator_obj_i</span> <span class="pre">==</span> <span class="pre">1:</span></code> will be <code class="docutils literal notranslate"><span class="pre">False</span></code> because there is no object in cell <code class="docutils literal notranslate"><span class="pre">i=0</span></code>.</p></li>
</ul>
<p>We do not explicitly define <code class="docutils literal notranslate"><span class="pre">indicator_noobj_i</span></code> to correspond to <span class="math notranslate nohighlight">\(\nobji\)</span>
but it is equal to <span class="math notranslate nohighlight">\(1\)</span> since there is no object in cell <span class="math notranslate nohighlight">\(i=0\)</span>. This means
that it will not go through the <code class="docutils literal notranslate"><span class="pre">if</span></code> clause in <code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">??</span></code> and will go through
<code class="docutils literal notranslate"><span class="pre">else</span></code> clause in <code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">??</span></code> and will compute the loss for the no object equation
at <span class="math notranslate nohighlight">\(d\)</span> equation above.</p>
<ul class="simple">
<li><p>So we will skip over all lines in the <code class="docutils literal notranslate"><span class="pre">if</span></code> clause and go to <code class="docutils literal notranslate"><span class="pre">else</span></code> clause.</p></li>
<li><p><strong>No object loss</strong>: <code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">xx-xx</span></code> means we are looping over the 2 bounding boxes in cell <span class="math notranslate nohighlight">\(i=0\)</span>.</p>
<ul>
<li><p>Loop over <span class="math notranslate nohighlight">\(j=0\)</span></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">y_true[i,</span> <span class="pre">4]</span></code> is the confidence score of the 1st bounding box in cell <span class="math notranslate nohighlight">\(i=0\)</span>.</p></li>
<li><p>It is equal to <span class="math notranslate nohighlight">\(0\)</span> since there is no object in cell <span class="math notranslate nohighlight">\(i=0\)</span> by <a class="reference internal" href="#gt-confidence">Definition 5</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_pred[i,</span> <span class="pre">4]</span></code> is the confidence score of the 1st bounding box in cell <span class="math notranslate nohighlight">\(i=0\)</span>.</p></li>
<li><p>We see that the model predicted <code class="docutils literal notranslate"><span class="pre">0.2136</span></code> for the first bounding box <code class="docutils literal notranslate"><span class="pre">j=0</span></code>.</p></li>
<li><p>We will compute the mean squared error between the ground truth confidence
score and the predicted confidence score of the 1st predicted bounding box.</p></li>
<li><p><span class="math notranslate nohighlight">\(0.5 \times (0.2136 - 0)^2 = 0.5 \times 0.04562496 = 0.02281248\)</span></p></li>
</ul>
</li>
<li><p>Loop over <span class="math notranslate nohighlight">\(j=1\)</span></p>
<ul>
<li><p>We still use <code class="docutils literal notranslate"><span class="pre">y_true[i,</span> <span class="pre">4]</span></code> because the <code class="docutils literal notranslate"><span class="pre">y_true[i,</span> <span class="pre">9]</span></code> is same as <code class="docutils literal notranslate"><span class="pre">y_true[i,</span> <span class="pre">4]</span></code> by
construction in <a class="reference internal" href="#yolo_gt_matrix">Definition 6</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_pred[i,</span> <span class="pre">9]</span></code> is the confidence score of the 2nd bounding box in cell <span class="math notranslate nohighlight">\(i=0\)</span>.</p></li>
<li><p>We see that the model predicted <code class="docutils literal notranslate"><span class="pre">-0.1186</span></code> for the second bounding box <code class="docutils literal notranslate"><span class="pre">j=1</span></code>.</p></li>
<li><p>We will compute the mean squared error between the ground truth confidence score and
the predicted confidence score of the 2nd predicted bounding box.</p></li>
<li><p><span class="math notranslate nohighlight">\(0.5 \times (-0.1186 - 0)^2 = 0.5 \times 0.01406596 = 0.00703298\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Finally we have <span class="math notranslate nohighlight">\(0.02281248 + 0.00703298 = 0.02984546\)</span> for the no object loss in cell <span class="math notranslate nohighlight">\(i=0\)</span>.</p></li>
<li><p>At this point the total loss for grid cell <code class="docutils literal notranslate"><span class="pre">i=0</span></code> is done:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">self.bbox_xy_offset_loss</span> <span class="pre">=</span> <span class="pre">0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.bbox_wh_loss</span> <span class="pre">=</span> <span class="pre">0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.object_conf_loss=0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.no_object_conf_loss=0.02984546</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.class_loss=0</span></code></p></li>
</ul>
</li>
</ul>
</li>
<li><p>when <code class="docutils literal notranslate"><span class="pre">i=30</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">y_true_i</span> <span class="pre">=</span> <span class="pre">[0.4093,</span> <span class="pre">0.2770,</span> <span class="pre">0.4164,</span> <span class="pre">0.2620,</span> <span class="pre">1,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">0.4093,</span> <span class="pre">0.2770,</span> <span class="pre">0.4164,</span> <span class="pre">0.2620,</span> <span class="pre">1,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_pred_i</span> <span class="pre">=</span> <span class="pre">[-0.4827,</span> <span class="pre">-0.3511,</span> <span class="pre">-0.5949,</span> <span class="pre">0.0183,</span> <span class="pre">0.1622,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">2.6878,</span>&#160; <span class="pre">2.0020,</span> <span class="pre">-1.1686,</span> <span class="pre">-4.5915,</span> <span class="pre">0.7578,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">-0.2317,</span>&#160; <span class="pre">0.5274,</span> <span class="pre">-0.2556,</span> <span class="pre">-0.2830,</span>&#160; <span class="pre">0.3817,</span> <span class="pre">-0.1655,</span> <span class="pre">-0.0712,</span> <span class="pre">-0.1746,</span>&#160; <span class="pre">0.1467,</span>&#160; <span class="pre">0.0166,</span> <span class="pre">-0.4146,</span>&#160; <span class="pre">1.1629,</span> <span class="pre">-0.0429,</span> <span class="pre">-0.0952,</span> <span class="pre">0.1323,</span> <span class="pre">-0.1055,</span>&#160; <span class="pre">0.2692,</span>&#160; <span class="pre">0.0923,</span> <span class="pre">-0.0845,</span>&#160; <span class="pre">0.5842]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">indicator_obj_i</span></code> corresponds to <span class="math notranslate nohighlight">\(\obji\)</span> in the equation above and is equal to <span class="math notranslate nohighlight">\(1\)</span> since there is an object in cell <span class="math notranslate nohighlight">\(i=30\)</span>.</p></li>
<li><p>So equations a, b, c and e goes into the <code class="docutils literal notranslate"><span class="pre">if</span></code> clause and equation d goes into the <code class="docutils literal notranslate"><span class="pre">else</span></code> clause.</p></li>
<li><p>Some variables below:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b_gt</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.4093485</span>  <span class="mf">0.27699995</span> <span class="mf">0.4164306</span>  <span class="mf">0.262</span>     <span class="p">]</span>
<span class="n">b_pred_1</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.48271456</span> <span class="o">-</span><span class="mf">0.3510531</span>  <span class="o">-</span><span class="mf">0.5948943</span>   <span class="mf">0.01832254</span><span class="p">]</span>
<span class="n">b_pred_2</span><span class="p">:</span> <span class="p">[</span> <span class="mf">2.6877854</span>  <span class="mf">2.0019963</span> <span class="o">-</span><span class="mf">1.1685951</span> <span class="o">-</span><span class="mf">4.591512</span> <span class="p">]</span>
<span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">w_i</span><span class="p">,</span> <span class="n">h_i</span> <span class="o">=</span> <span class="mf">0.4093485</span><span class="p">,</span> <span class="mf">0.27699995</span><span class="p">,</span> <span class="mf">0.4164306</span><span class="p">,</span> <span class="mf">0.262</span>
<span class="n">xhat_i1</span><span class="p">,</span> <span class="n">yhat_i1</span><span class="p">,</span> <span class="n">what_i1</span><span class="p">,</span> <span class="n">hhat_i1</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.48271456</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3510531</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5948943</span><span class="p">,</span> <span class="mf">0.01832254</span>
<span class="n">xhat_i2</span><span class="p">,</span> <span class="n">yhat_i2</span><span class="p">,</span> <span class="n">what_i2</span><span class="p">,</span> <span class="n">hhat_i2</span> <span class="o">=</span> <span class="mf">2.6877854</span><span class="p">,</span> <span class="mf">2.0019963</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1685951</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.591512</span>
<span class="n">conf_1</span><span class="p">,</span> <span class="n">confhat_i1</span><span class="p">,</span> <span class="n">confhat_i2</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.1622</span><span class="p">,</span> <span class="mf">0.7578</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Then in <code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">x-x</span></code>, we compute <code class="docutils literal notranslate"><span class="pre">iou_b1</span></code> and <code class="docutils literal notranslate"><span class="pre">iou_b2</span></code> which are the intersection over union between the ground truth bounding box and the predicted bounding boxes. This is
for the Bipartite matching part when we choose which predictor <span class="math notranslate nohighlight">\(\jmax\)</span>
to use to compute the loss with the ground truth.</p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">x-x</span></code>, we do a simple inequality check (note this is not scalable if you have
more than 2 bounding box predictors per grid cell) to see which bounding box predictor
has the highest intersection over union with the ground truth bounding box. In this case
it is the 2nd bounding box predictor <code class="docutils literal notranslate"><span class="pre">j=1</span></code> since <code class="docutils literal notranslate"><span class="pre">iou_b2</span> <span class="pre">&gt;</span> <span class="pre">iou_b1</span></code>.</p></li>
</ul>
<p>So:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xhat_i_jmax</span><span class="p">,</span> <span class="n">yhat_i_jmax</span><span class="p">,</span> <span class="n">what_i_jmax</span><span class="p">,</span> <span class="n">hhat_i_jmax</span> <span class="o">=</span> <span class="n">xhat_i2</span><span class="p">,</span> <span class="n">yhat_i2</span><span class="p">,</span> <span class="n">what_i2</span><span class="p">,</span> <span class="n">hhat_i2</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.bbox_xy_offset_loss</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">x_offset_loss</span></code> -&gt; <span class="math notranslate nohighlight">\((0.4093484878540039 - 2.6877853870391846)^2 = 5.19127470357\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_offset_loss</span></code> -&gt; <span class="math notranslate nohighlight">\((0.27699995040893555 - 2.0019962787628174)^2 = 2.97561233283\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.bbox_xy_offset_loss</span></code> -&gt; <span class="math notranslate nohighlight">\(5 \times (5.19127470357 + 2.97561233283) = 5 \times 8.1668870364 = 40.8344\)</span></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.bbox_wh_loss</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">w_loss</span></code> -&gt; <span class="math notranslate nohighlight">\((\sqrt{0.41643059253692627} - \sqrt{\abs{-1.1685951}})^2 = 0.1898357414137847\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">h_loss</span></code> -&gt; <span class="math notranslate nohighlight">\((\sqrt{0.262} - \sqrt{\abs{-4.591512}})^2 = 2.659906617074438\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.bbox_wh_loss</span></code> -&gt; <span class="math notranslate nohighlight">\(5 \times (0.1898357414137847 + 2.659906617074438) = 14.2487\)</span></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.object_conf_loss</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">self.object_conf_loss</span></code> -&gt; <span class="math notranslate nohighlight">\((1 - 0.7578)^2 = 0.05864378437399864\)</span></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.no_object_conf_loss</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">self.no_object_conf_loss</span></code> -&gt; <span class="math notranslate nohighlight">\(0\)</span></p></li>
<li><p>Note that in my code we have <code class="docutils literal notranslate"><span class="pre">self.no_object_conf_loss</span> <span class="pre">+=</span> <span class="pre">self.lambda_noobj</span> <span class="pre">*</span> <span class="pre">self.compute_no_object_conf_loss(conf_i=torch.tensor(0.,</span> <span class="pre">device=&quot;cuda&quot;),</span> <span class="pre">confhat_i=confhat_i_complement)</span></code> which also computes the no object loss
for the other bounding box predictor in the cell. But we won’t calculate it here.
The reason some people do that is they want to penalize the network for predicting
something for the other bounding box predictor even though there is an object in that grid cell. In other words, they want it to specialize for predictor <span class="math notranslate nohighlight">\(\jmax\)</span>.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.class_loss</span></code>:</p>
<ul>
<li><p>This part is simple, we just compute the mean squared error between the ground truth class and the predicted class for each element.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation/287497">https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation/287497</a><em>=</em></p></li>
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation/287497#287497">https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation/287497#287497</a></p></li>
<li><p><a class="reference external" href="https://hackernoon.com/understanding-yolo-f5a74bbc7967">https://hackernoon.com/understanding-yolo-f5a74bbc7967</a></p></li>
</ul>
</section>
<section id="sphinx">
<h2>Sphinx<a class="headerlink" href="#sphinx" title="Permalink to this headline">#</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ pip uninstall sphinx-proof
$ pip install -U git+https://github.com/executablebooks/sphinx-proof.git@master
</pre></div>
</div>
<p>For the newest assumptions directives.</p>
</section>
<section id="citations">
<h2>Citations<a class="headerlink" href="#citations" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id12">
<dl class="citation">
<dt class="label" id="id13"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>,<a href="#id7">3</a>,<a href="#id9">4</a>,<a href="#id10">5</a>)</span></dt>
<dd><p>Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: unified, real-time object detection. <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016. <a class="reference external" href="https://doi.org/10.1109/cvpr.2016.91">doi:10.1109/cvpr.2016.91</a>.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">2</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id8">2</a>,<a href="#id11">3</a>)</span></dt>
<dd><p>Harry Turner. Yolo v1. 2021. URL: <a class="reference external" href="https://www.harrysprojects.com/articles/yolov1.html">https://www.harrysprojects.com/articles/yolov1.html</a>.</p>
</dd>
</dl>
</div>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id5">1</a></span></dt>
<dd><p><a class="reference external" href="https://www.harrysprojects.com/articles/yolov1.html">https://www.harrysprojects.com/articles/yolov1.html</a></p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Introduction</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Hongnan Gao<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>